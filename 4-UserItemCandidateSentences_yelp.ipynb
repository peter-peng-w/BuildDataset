{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
                "\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "# Create a Tokenizer with the default settings for English\n",
                "# including punctuation rules and exceptions\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset_name = \"yelp\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_review_filtered_clean.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 10000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train_review_filtered.json\n",
                        "100000 lines loaded.\n",
                        "200000 lines loaded.\n",
                        "300000 lines loaded.\n",
                        "400000 lines loaded.\n",
                        "500000 lines loaded.\n",
                        "600000 lines loaded.\n",
                        "Finish loading train dataset, totally 668245 lines.\n",
                        "Load file: ../Dataset/yelp/test_review_filtered_clean.json\n",
                        "10000 lines loaded.\n",
                        "20000 lines loaded.\n",
                        "30000 lines loaded.\n",
                        "40000 lines loaded.\n",
                        "50000 lines loaded.\n",
                        "60000 lines loaded.\n",
                        "70000 lines loaded.\n",
                        "80000 lines loaded.\n",
                        "90000 lines loaded.\n",
                        "100000 lines loaded.\n",
                        "110000 lines loaded.\n",
                        "120000 lines loaded.\n",
                        "130000 lines loaded.\n",
                        "140000 lines loaded.\n",
                        "150000 lines loaded.\n",
                        "160000 lines loaded.\n",
                        "170000 lines loaded.\n",
                        "180000 lines loaded.\n",
                        "Finish loading test dataset, totally 183650 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0       10000     0       4  recommendations are right on and always fun to...\n",
                            "1       10019     0       3  just the same pricing but with less item selec...\n",
                            "2        1002     0       3  taste much better than ayce sushi ! the sushi ...\n",
                            "3       10020     0       3  this is a brand new location of ruelo that rec...\n",
                            "4       10039     0       4  medium rare is a little dry ... since the stea...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "668240   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "668241    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "668242   7379  9999       5                             everything was great .\n",
                            "668243   8530  9999       4  our last experience here went pretty well and ...\n",
                            "668244   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[668245 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>recommendations are right on and always fun to...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10019</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>just the same pricing but with less item selec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1002</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>taste much better than ayce sushi ! the sushi ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10020</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>this is a brand new location of ruelo that rec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>10039</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>medium rare is a little dry ... since the stea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668240</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668241</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668242</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668243</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668244</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>668245 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# groupby multiple columns\n",
                "groupby_user_item = df_train_data.groupby(['user', 'item'])\n",
                "cnt = 0\n",
                "for key, item in groupby_user_item:\n",
                "    cur_df_user_item = groupby_user_item.get_group(key)\n",
                "    if len(cur_df_user_item) > 1:\n",
                "        if cnt <= 10:\n",
                "            print(cur_df_user_item)\n",
                "        cnt += 1\n",
                "print(\"{} data instance are the same\".format(cnt))\n",
                "# make sure that there are no duplicated reviews"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0 data instance are the same\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2ID and ID2Sentence Mapping"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    sent_to_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "type(sent_to_id['good service .'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "str"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "sent_to_id['good service .']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'10'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    id_to_sent = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/id2sentence.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "id_to_sent['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service was good .'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 12
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "assert len(sent_to_id) == len(id_to_sent)\n",
                "print(\"Number of sentence (with feature) on train set: {}\".format(len(sent_to_id)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of sentence (with feature) on train set: 1617208\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "feature2id_filepath = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature2id_filepath))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "len(feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "498"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "feature_vocab['wifi']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'29'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "sentence2feature_filepath = '../Dataset/{}/train/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2feature_filepath))\n",
                "    sentence_to_feature = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/sentence2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "sentence_to_feature['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "{'10': 1.0}"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 18
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "assert len(sentence_to_feature) == len(sent_to_id)\n",
                "len(sentence_to_feature)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "1617208"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 19
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct User-Item Pair"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GrounpBy User"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "group_by_user = df_train_data.groupby('user')\n",
                "group_by_user_dict = dict(tuple(group_by_user))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "group_by_user_dict['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item user  rating                                             review\n",
                            "0     10000    0       4  recommendations are right on and always fun to...\n",
                            "1     10019    0       3  just the same pricing but with less item selec...\n",
                            "2      1002    0       3  taste much better than ayce sushi ! the sushi ...\n",
                            "3     10020    0       3  this is a brand new location of ruelo that rec...\n",
                            "4     10039    0       4  medium rare is a little dry ... since the stea...\n",
                            "...     ...  ...     ...                                                ...\n",
                            "1432    994    0       3  whole sea bream ceviche ( $ 26 ) basque cake &...\n",
                            "1433   9944    0       3  tenon vegetarian cuisine is not a bad choice i...\n",
                            "1434   9969    0       4  sam james coffee bar is awesome ! i have to sa...\n",
                            "1435   9979    0       4  a sophisticated but laid back atmosphere if yo...\n",
                            "1436   9985    0       4  la boheme croissant is so devilish good ! the ...\n",
                            "\n",
                            "[1437 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>recommendations are right on and always fun to...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10019</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>just the same pricing but with less item selec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1002</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>taste much better than ayce sushi ! the sushi ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10020</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>this is a brand new location of ruelo that rec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>10039</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>medium rare is a little dry ... since the stea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1432</th>\n",
                            "      <td>994</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>whole sea bream ceviche ( $ 26 ) basque cake &amp;...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1433</th>\n",
                            "      <td>9944</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>tenon vegetarian cuisine is not a bad choice i...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1434</th>\n",
                            "      <td>9969</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>sam james coffee bar is awesome ! i have to sa...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1435</th>\n",
                            "      <td>9979</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>a sophisticated but laid back atmosphere if yo...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1436</th>\n",
                            "      <td>9985</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>la boheme croissant is so devilish good ! the ...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>1437 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 21
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "len(group_by_user_dict)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "15639"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 22
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "user_id_list = list(df_train_data['user'].unique())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "print(len(user_id_list))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "15639\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "user_to_sent = dict()\n",
                "cnt_sentence_with_no_feature = 0\n",
                "\n",
                "for user_id in user_id_list:\n",
                "    # note this user_id is a str\n",
                "    # get the dataframe for this user\n",
                "    user_df = group_by_user_dict[user_id]\n",
                "    user_reviews = list(user_df['review'])\n",
                "    user_sent_ids = set()\n",
                "    for review in user_reviews:\n",
                "        # tokenize this review (i.e. split into sentences)\n",
                "        review_sents = sent_tokenize(review)\n",
                "        # check whether the sentence is in the sentence2id dictionary\n",
                "        for sent in review_sents:\n",
                "            if sent in sent_to_id:\n",
                "                cur_sent_id = sent_to_id[sent]\n",
                "                assert isinstance(cur_sent_id, str)\n",
                "                # make sure that this sentence has feature\n",
                "                # assert cur_sent_id in sentence_to_feature\n",
                "                # user_sent_ids.add(cur_sent_id)\n",
                "                if cur_sent_id in sentence_to_feature:\n",
                "                    # add this sentence into the set of this user\n",
                "                    user_sent_ids.add(cur_sent_id)\n",
                "                else:\n",
                "                    cnt_sentence_with_no_feature += 1\n",
                "        # TODO: what should we do if there are sentence that appears in multiple reivews?\n",
                "    if len(user_sent_ids) == 0:\n",
                "        print(\"User: {} has no effective sentences, skip it.\".format(user_id))\n",
                "    else:\n",
                "        user_to_sent[user_id] = user_sent_ids\n",
                "print('{} sentence with no feature'.format(cnt_sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0 sentence with no feature\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "len(user_to_sent)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "15639"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 26
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "user_to_sentids = dict()\n",
                "for user_id, user_sents in user_to_sent.items():\n",
                "    assert len(user_sents) > 0\n",
                "    assert isinstance(user_id, str)\n",
                "    assert isinstance(list(user_sents)[0], str)\n",
                "    user_to_sentids[user_id] = list(user_sents)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "len(user_to_sentids)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "15639"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 28
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "user2sentids_filepath = '../Dataset/{}/train/user/user2sentids.json'.format(dataset_name)\n",
                "with open(user2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(user2sentids_filepath))\n",
                "    json.dump(user_to_sentids, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/user/user2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "user_side_cdd_sents_num = list()\n",
                "for key, value in user_to_sentids.items():\n",
                "    user_side_cdd_sents_num.append(len(value))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "print(\"Mean number of sentence per user: {}\".format(\n",
                "    np.mean(user_side_cdd_sents_num)\n",
                "))\n",
                "print(\"Min number of sentence per user: {}\".format(\n",
                "    np.min(user_side_cdd_sents_num)\n",
                "))\n",
                "print(\"Max number of sentence per user: {}\".format(\n",
                "    np.max(user_side_cdd_sents_num)\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of sentence per user: 109.21433595498434\n",
                        "Min number of sentence per user: 1\n",
                        "Max number of sentence per user: 4854\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "print(\"Top-10 least numbber of sentences per user: {}\".format(\n",
                "    sorted(user_side_cdd_sents_num)[:10]\n",
                "))\n",
                "print(\"Top-10 most numbber of sentences per user: {}\".format(\n",
                "    sorted(user_side_cdd_sents_num)[-10:]\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Top-10 least numbber of sentences per user: [1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
                        "Top-10 most numbber of sentences per user: [1986, 2022, 2088, 2376, 2482, 2492, 2913, 3267, 3593, 4854]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy Item"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "group_by_item = df_train_data.groupby('item')\n",
                "group_by_item_dict = dict(tuple(group_by_item))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "group_by_item_dict['1267']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "        item   user  rating                                             review\n",
                            "5804    1267   1007       3  their latte art is also quite cute :) the spac...\n",
                            "7932    1267   1013       4  the decor is nice inside but somewhat busy com...\n",
                            "9847    1267  10186       4                            coffee wise - amazing !\n",
                            "16766   1267  10396       3  it was sweet and pleasant tasting . the staff ...\n",
                            "21302   1267  10523       4  the seating is nicely laid out and many people...\n",
                            "...      ...    ...     ...                                                ...\n",
                            "647766  1267   9408       4  i do find that their pastries are a bit pricey...\n",
                            "653869  1267    958       4                          excellent latte and tea .\n",
                            "661991  1267   9806       3  i had an latte , decent latte art but the latt...\n",
                            "662376  1267   9818       5  the staff has always been extra pleasant and h...\n",
                            "665131  1267    990       4  the drink was quite pricey at $ 4.75 ( $ 5.05 ...\n",
                            "\n",
                            "[81 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>5804</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>1007</td>\n",
                            "      <td>3</td>\n",
                            "      <td>their latte art is also quite cute :) the spac...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7932</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>1013</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the decor is nice inside but somewhat busy com...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9847</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>10186</td>\n",
                            "      <td>4</td>\n",
                            "      <td>coffee wise - amazing !</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>16766</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>10396</td>\n",
                            "      <td>3</td>\n",
                            "      <td>it was sweet and pleasant tasting . the staff ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>21302</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>10523</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the seating is nicely laid out and many people...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>647766</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>9408</td>\n",
                            "      <td>4</td>\n",
                            "      <td>i do find that their pastries are a bit pricey...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>653869</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>958</td>\n",
                            "      <td>4</td>\n",
                            "      <td>excellent latte and tea .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>661991</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>9806</td>\n",
                            "      <td>3</td>\n",
                            "      <td>i had an latte , decent latte art but the latt...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>662376</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>9818</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the staff has always been extra pleasant and h...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>665131</th>\n",
                            "      <td>1267</td>\n",
                            "      <td>990</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the drink was quite pricey at $ 4.75 ( $ 5.05 ...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>81 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 35
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "len(group_by_item_dict)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "21515"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 36
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "item_id_list = list(df_train_data['item'].unique())\n",
                "item_to_sent = dict()\n",
                "cnt_sentence_with_no_feature = 0\n",
                "for item_id in item_id_list:\n",
                "    # note this item_id is a str\n",
                "    # get the dataframe for this item\n",
                "    assert isinstance(item_id, str)\n",
                "    item_df = group_by_item_dict[item_id]\n",
                "    item_reviews = list(item_df['review'])\n",
                "    item_sent_ids = set()\n",
                "    for review in item_reviews:\n",
                "        # tokenize this review (i.e. split into sentences)\n",
                "        review_sents = sent_tokenize(review)\n",
                "        # check whether the sentence is in the sentence2id dictionary\n",
                "        for sent in review_sents:\n",
                "            if sent in sent_to_id:\n",
                "                cur_sent_id = sent_to_id[sent]\n",
                "                assert isinstance(cur_sent_id, str)\n",
                "                # make sure that this sentence has feature\n",
                "                if cur_sent_id in sentence_to_feature:\n",
                "                    # add this sentence into the set of this user\n",
                "                    item_sent_ids.add(cur_sent_id)\n",
                "                else:\n",
                "                    cnt_sentence_with_no_feature += 1\n",
                "        # TODO: what should we do if there are sentence that appears in multiple reivews?\n",
                "    if len(item_sent_ids) == 0:\n",
                "        print(\"Item {} has no effective sentence, skip it.\".format(item_id))\n",
                "    else:\n",
                "        item_to_sent[item_id] = item_sent_ids\n",
                "\n",
                "print('{} sentence with no feature'.format(cnt_sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0 sentence with no feature\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "len(item_to_sent)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "21515"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 38
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "item_to_sentids = dict()\n",
                "for item_id, item_sents in item_to_sent.items():\n",
                "    assert len(item_sents) > 0\n",
                "    assert isinstance(list(item_sents)[0], str)\n",
                "    item_to_sentids[item_id] = list(item_sents)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "len(item_to_sentids)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "21515"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 40
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "item2sentids_filepath = '../Dataset/{}/train/item/item2sentids.json'.format(dataset_name)\n",
                "with open(item2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(item2sentids_filepath))\n",
                "    json.dump(item_to_sentids, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/item/item2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "item_side_cdd_sents_num = list()\n",
                "for key, value in item_to_sentids.items():\n",
                "    item_side_cdd_sents_num.append(len(value))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "print(\"Mean number of sentence per item: {}\".format(\n",
                "    np.mean(item_side_cdd_sents_num)\n",
                "))\n",
                "print(\"Min number of sentence per item: {}\".format(\n",
                "    np.min(item_side_cdd_sents_num)\n",
                "))\n",
                "print(\"Max number of sentence per item: {}\".format(\n",
                "    np.max(item_side_cdd_sents_num)\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of sentence per item: 79.67683011852196\n",
                        "Min number of sentence per item: 1\n",
                        "Max number of sentence per item: 2336\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "print(\"Top-10 least numbber of sentence per item: {}\".format(\n",
                "    sorted(item_side_cdd_sents_num)[:10]\n",
                "))\n",
                "print(\"Top-10 most numbber of sentence per item: {}\".format(\n",
                "    sorted(item_side_cdd_sents_num)[-10:]\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Top-10 least numbber of sentence per item: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
                        "Top-10 most numbber of sentence per item: [1254, 1285, 1300, 1354, 1502, 1508, 1608, 1947, 2061, 2336]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# For Each Data Instance in TrainSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "import random\n",
                "sample_sent_num = 500\n",
                "user_item_candidate_sent_ids = dict()\n",
                "# Loop over all User\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "review_with_no_selectd_label_sentence = 0\n",
                "useable_review_cnt = 0\n",
                "sentence_with_no_feature_cnt = 0\n",
                "sentence_not_tracked = set()\n",
                "for user_df_chunk in list(group_by_user):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    user_df = user_df_chunk[1]\n",
                "    # get user sents\n",
                "    cur_user_sent_ids = user_to_sent[user_id_str]\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sents\n",
                "        cur_item_sent_ids = item_to_sent[item_id_str]\n",
                "        # get review_text's sent ids\n",
                "        cur_review_sent_ids = set()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the sentence2id dictionary\n",
                "        for sent in review_sents:\n",
                "            if sent in sent_to_id:\n",
                "                cur_sent_id = sent_to_id[sent]\n",
                "                assert isinstance(cur_sent_id, str)\n",
                "                # make sure that this sentence has feature\n",
                "                if cur_sent_id in sentence_to_feature:\n",
                "                    # add this sentence into the set of current review\n",
                "                    cur_review_sent_ids.add(cur_sent_id)\n",
                "                else:\n",
                "                    sentence_with_no_feature_cnt += 1\n",
                "            else:\n",
                "                # if this sentence is not being tracked by the sentence-id mapping\n",
                "                # we add it into this set to see how many sentences are being ignored\n",
                "                sentence_not_tracked.add(sent)\n",
                "        ## check whether the true label of the sentence is an empty list of sent_ids\n",
                "        if len(cur_review_sent_ids) == 0:\n",
                "            review_with_no_selectd_label_sentence += 1\n",
                "        else:\n",
                "            # construct the candidate set which is an union of user sentences and item sentences\n",
                "            cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "            # sample some sentences\n",
                "            if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "                sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            else:\n",
                "                sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "            # union sampled sentences with true labeled sentences\n",
                "            final_useritem_sent_ids = sample_useritem_sent_ids | cur_review_sent_ids\n",
                "            # add this into the dict\n",
                "            item_candidate_sent_ids[item_id_str] = [list(final_useritem_sent_ids), list(cur_review_sent_ids)]\n",
                "            # add useable review cnt\n",
                "            useable_review_cnt += 1\n",
                "    if len(item_candidate_sent_ids) == 0:\n",
                "        print(\"User: {} has no useful item, skip this user ...\".format(user_id_str))\n",
                "    else:\n",
                "        # add the item_candidate_sent_ids dict into the user-level dict\n",
                "        user_item_candidate_sent_ids[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 1000 == 0:\n",
                "        print(\"{} user processed\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {} users'.format(user_cnt))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, review_with_no_selectd_label_sentence))\n",
                "print(\"{} sentences has 0 feature\".format(sentence_with_no_feature_cnt))\n",
                "print(\"{} sentences are not being tracked in the sent2id mapping\".format(len(sentence_not_tracked)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed\n",
                        "2000 user processed\n",
                        "3000 user processed\n",
                        "4000 user processed\n",
                        "5000 user processed\n",
                        "6000 user processed\n",
                        "7000 user processed\n",
                        "8000 user processed\n",
                        "9000 user processed\n",
                        "10000 user processed\n",
                        "11000 user processed\n",
                        "12000 user processed\n",
                        "13000 user processed\n",
                        "14000 user processed\n",
                        "15000 user processed\n",
                        "Finish.\n",
                        "Totally 15639 users\n",
                        "Totally 668245 reviews. Among them 1 reviews has empty true label sentence\n",
                        "0 sentences has 0 feature\n",
                        "3205 sentences are not being tracked in the sent2id mapping\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "len(user_item_candidate_sent_ids)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "15639"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 46
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "# let's check how many unique reviews are there\n",
                "\n",
                "cnt_unique_reviews = 0\n",
                "cnt_empty_true_sent = 0\n",
                "sentence_per_review = []\n",
                "candidate_sentence_num_cnt_per_review = []\n",
                "# [user-level] Loop for each user\n",
                "for user_chunk in list(user_item_candidate_sent_ids.items()):\n",
                "    user_id_str = str(user_chunk[0])\n",
                "    # assert isinstance(user_chunk[0], str)\n",
                "    # [item-level] Loop for each user-item pair\n",
                "    user_item_chunks = list(user_chunk[1].items())\n",
                "    for item_chunk in user_item_chunks:\n",
                "        item_id_str = str(item_chunk[0])\n",
                "        # assert isinstance(item_chunk[0], str)\n",
                "        candidate_sent_ids = item_chunk[1][0]\n",
                "        true_sent_ids = item_chunk[1][1]\n",
                "        if len(true_sent_ids) == 0:\n",
                "            cnt_empty_true_sent += 1\n",
                "        else:\n",
                "            assert isinstance(candidate_sent_ids[0], str)\n",
                "            assert isinstance(true_sent_ids[0], str)\n",
                "            # make sure that all true label sent_ids appears in the corresponding candidate set\n",
                "            for true_sent_id in true_sent_ids:\n",
                "                assert true_sent_id in candidate_sent_ids\n",
                "            sentence_per_review.append(len(true_sent_ids))\n",
                "            candidate_sentence_num_cnt_per_review.append(len(candidate_sent_ids))\n",
                "        cnt_unique_reviews += 1\n",
                "\n",
                "print(\"Total number of unique selected reviews: {}\".format(cnt_unique_reviews))\n",
                "print(\"Total number of review with empty true sentences: {}\".format(cnt_empty_true_sent))\n",
                "print(\"Total number of unique review with non-empty true sentences: {}\".format(\n",
                "    cnt_unique_reviews - cnt_empty_true_sent))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of unique selected reviews: 668244\n",
                        "Total number of review with empty true sentences: 0\n",
                        "Total number of unique review with non-empty true sentences: 668244\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "print(\"Totally {} user-item pairs in the trainset\".format(len(sentence_per_review)))\n",
                "print(\"max number of true sentence per review: {}\".format(np.max(sentence_per_review)))\n",
                "print(\"min number of true sentence per review: {}\".format(np.min(sentence_per_review)))\n",
                "print(\"mean number of true sentence per review: {}\".format(np.mean(sentence_per_review)))\n",
                "print(\"max number of candidate sentence per review: {}\".format(np.max(candidate_sentence_num_cnt_per_review)))\n",
                "print(\"min number of candidate sentence per review: {}\".format(np.min(candidate_sentence_num_cnt_per_review)))\n",
                "print(\"mean number of candidate sentence per review: {}\".format(np.mean(candidate_sentence_num_cnt_per_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 668244 user-item pairs in the trainset\n",
                        "max number of true sentence per review: 24\n",
                        "min number of true sentence per review: 1\n",
                        "mean number of true sentence per review: 2.5671027349291577\n",
                        "max number of candidate sentence per review: 523\n",
                        "min number of candidate sentence per review: 7\n",
                        "mean number of candidate sentence per review: 333.09391779050765\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "useritem2sentids_filepath = '../Dataset/{}/train/useritem2sentids.json'.format(dataset_name)\n",
                "with open(useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/useritem2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "example_user_id = '1007'\n",
                "example_item_id = '1267'\n",
                "print(\"User: {0}\\tItem: {1}\".format(example_user_id, example_item_id))\n",
                "print(\n",
                "    \"Number of (sampled) cdd sents: \", \n",
                "    len(user_item_candidate_sent_ids[example_user_id][example_item_id][0]))\n",
                "print(\"Number of user side cdd sents:\", len(user_to_sent[example_user_id]))\n",
                "print(\"Number of item side cdd sents:\", len(item_to_sent[example_item_id]))\n",
                "print(\"Number of GT sentences: \", \n",
                "    len(user_item_candidate_sent_ids[example_user_id][example_item_id][1]))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "User: 1007\tItem: 1267\n",
                        "Number of (sampled) cdd sents:  501\n",
                        "Number of user side cdd sents: 585\n",
                        "Number of item side cdd sents: 172\n",
                        "Number of GT sentences:  2\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "for sent_id in user_item_candidate_sent_ids[example_user_id][example_item_id][1]:\n",
                "    assert sent_id in user_item_candidate_sent_ids[example_user_id][example_item_id][0]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Write train useritem_cdd in a line-by-line format"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "# Write useritem2sentids into a line-by-line format\n",
                "train_useritem2sentid_multiline_file = '../Dataset/{}/train/useritem2sentids_multilines.json'.format(dataset_name)\n",
                "if os.path.exists(train_useritem2sentid_multiline_file):\n",
                "    print(\"File: {} exists, remove it.\".format(train_useritem2sentid_multiline_file))\n",
                "    os.remove(train_useritem2sentid_multiline_file)\n",
                "else:\n",
                "    print(\"File: {} doesn't exist, creat it.\".format(train_useritem2sentid_multiline_file))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "File: ../Dataset/yelp/train/useritem2sentids_multilines.json doesn't exist, creat it.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "with open(train_useritem2sentid_multiline_file, 'a') as f1:\n",
                "    print(\"Write file: {}\".format(train_useritem2sentid_multiline_file))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids.items()):\n",
                "        assert isinstance(trainset_user_chunk[0], str)\n",
                "        user_id_str = trainset_user_chunk[0]\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            assert isinstance(item_chunk[0], str)\n",
                "            item_id_str = item_chunk[0]\n",
                "            item_id = int(item_chunk[0])\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            gold_revw_sent_ids = item_chunk[1][1]\n",
                "            assert isinstance(candidate_sent_ids, list)\n",
                "            assert isinstance(gold_revw_sent_ids, list)\n",
                "            assert isinstance(candidate_sent_ids[0], str)\n",
                "            assert isinstance(gold_revw_sent_ids[0], str)\n",
                "            for cur_id in gold_revw_sent_ids:\n",
                "                assert cur_id in candidate_sent_ids\n",
                "            cur_data_dict = {'user_id': user_id_str, 'item_id': item_id_str, 'candidate': candidate_sent_ids, \"review\": gold_revw_sent_ids}\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "            item_set.add(item_id_str)\n",
                "            if cnt_review % 50000 == 0:\n",
                "                print(\"{} lines of train data written.\".format(cnt_review))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert cnt_user == len(user_set)\n",
                "assert cnt_review == len(useritem_set)\n",
                "print(\"Totally {} users\".format(cnt_user))\n",
                "print(\"Totally {} items\".format(len(item_set)))\n",
                "print(\"Totally {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/useritem2sentids_multilines.json\n",
                        "50000 lines of train data written.\n",
                        "100000 lines of train data written.\n",
                        "150000 lines of train data written.\n",
                        "200000 lines of train data written.\n",
                        "250000 lines of train data written.\n",
                        "300000 lines of train data written.\n",
                        "350000 lines of train data written.\n",
                        "400000 lines of train data written.\n",
                        "450000 lines of train data written.\n",
                        "500000 lines of train data written.\n",
                        "550000 lines of train data written.\n",
                        "600000 lines of train data written.\n",
                        "650000 lines of train data written.\n",
                        "Totally 15639 users\n",
                        "Totally 21515 items\n",
                        "Totally 668244 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "source": [
                "!head -n 1 '../Dataset/yelp/train/useritem2sentids_multilines.json'"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{\"user_id\": \"0\", \"item_id\": \"10000\", \"candidate\": [\"25821\", \"25864\", \"25998\", \"26986\", \"24500\", \"25543\", \"25037\", \"25089\", \"27514\", \"26054\", \"25610\", \"25935\", \"25181\", \"26668\", \"26487\", \"26735\", \"25852\", \"25466\", \"24799\", \"25150\", \"24638\", \"27529\", \"27348\", \"24489\", \"27177\", \"26280\", \"27473\", \"26388\", \"26062\", \"25478\", \"24811\", \"27402\", \"26638\", \"25720\", \"26665\", \"25303\", \"10420\", \"25225\", \"25091\", \"25561\", \"25730\", \"26086\", \"25019\", \"25708\", \"24174\", \"27301\", \"25134\", \"27085\", \"599485\", \"26168\", \"10418\", \"26194\", \"24255\", \"26501\", \"24939\", \"26696\", \"24792\", \"26637\", \"24594\", \"26375\", \"26261\", \"25305\", \"27392\", \"25679\", \"27276\", \"27278\", \"25169\", \"10396\", \"24117\", \"27148\", \"27210\", \"25000\", \"1132950\", \"24786\", \"24910\", \"26113\", \"27272\", \"103\", \"25439\", \"26035\", \"26460\", \"25911\", \"27252\", \"26876\", \"1656\", \"24584\", \"24664\", \"25392\", \"27031\", \"24363\", \"24604\", \"27222\", \"26747\", \"25404\", \"27156\", \"24964\", \"26848\", \"24992\", \"24710\", \"25675\", \"24775\", \"25090\", \"24251\", \"10422\", \"24129\", \"26234\", \"26878\", \"26250\", \"26269\", \"24411\", \"25597\", \"25517\", \"25361\", \"24870\", \"24825\", \"25293\", \"24375\", \"27350\", \"26651\", \"24252\", \"27093\", \"25190\", \"27129\", \"24765\", \"2758\", \"25060\", \"26576\", \"25958\", \"10419\", \"26622\", \"25253\", \"24134\", \"25641\", \"24629\", \"25234\", \"26208\", \"25845\", \"26534\", \"24669\", \"25052\", \"27215\", \"27214\", \"27443\", \"25433\", \"26728\", \"24478\", \"25457\", \"27260\", \"1132951\", \"1132953\", \"25506\", \"26315\", \"24678\", \"25622\", \"26605\", \"26336\", \"26003\", \"26374\", \"24701\", \"27144\", \"26403\", \"25094\", \"25814\", \"24566\", \"25058\", \"26593\", \"27151\", \"27499\", \"26245\", \"26647\", \"25764\", \"25609\", \"27097\", \"26079\", \"24747\", \"26796\", \"24403\", \"27258\", \"27310\", \"26970\", \"10421\", \"25746\", \"24503\", \"27540\", \"24230\", \"26826\", \"26493\", \"25995\", \"24973\", \"1354663\", \"26727\", \"24099\", \"24529\", \"10427\", \"25001\", \"26428\", \"26553\", \"25831\", \"27359\", \"25289\", \"24866\", \"25119\", \"25215\", \"24267\", \"26494\", \"24243\", \"25701\", \"24430\", \"26992\", \"24293\", \"25306\", \"24455\", \"25883\", \"26692\", \"25835\", \"26323\", \"24305\", \"26715\", \"24695\", \"25430\", \"25555\", \"26810\", \"25274\", \"25443\", \"27267\", \"26811\", \"25756\", \"26418\", \"26530\", \"24697\", \"27092\", \"24325\", \"24680\", \"25461\", \"26263\", \"27385\", \"17052\", \"25282\", \"2755\", \"24605\", \"26772\", \"25977\", \"26606\", \"25515\", \"26151\", \"26737\", \"24437\", \"25071\", \"24304\", \"25320\", \"24223\", \"25886\", \"26152\", \"26118\", \"433101\", \"24568\", \"27397\", \"25216\", \"25477\", \"26242\", \"24906\", \"25615\", \"26108\", \"24556\", \"25721\", \"25781\", \"27040\", \"25465\", \"25972\", \"27332\", \"25201\", \"27086\", \"26348\", \"25438\", \"24326\", \"25184\", \"25943\", \"27247\", \"27462\", \"24345\", \"27536\", \"25333\", \"26858\", \"25984\", \"26107\", \"26676\", \"25712\", \"27113\", \"25063\", \"24891\", \"26061\", \"25932\", \"25613\", \"27187\", \"1039515\", \"25642\", \"24896\", \"27311\", \"10428\", \"27142\", \"24573\", \"24530\", \"24100\", \"24352\", \"25112\", \"25527\", \"26453\", \"26924\", \"25263\", \"24731\", \"26948\", \"26069\", \"26803\", \"24782\", \"25259\", \"24468\", \"27112\", \"26853\", \"26625\", \"24737\", \"25415\", \"25308\", \"26609\", \"26904\", \"25261\", \"25231\", \"24382\", \"27223\", \"24475\", \"25960\", \"25106\", \"25355\", \"27027\", \"25646\", \"27253\", \"26913\", \"24822\", \"26309\", \"24213\", \"24791\", \"25803\", \"1039516\", \"25377\", \"24646\", \"24600\", \"25286\", \"25115\", \"26172\", \"27455\", \"24452\", \"27347\", \"27224\", \"24284\", \"24441\", \"25700\", \"24958\", \"26260\", \"27545\", \"26220\", \"27476\", \"26488\", \"25504\", \"25192\", \"24709\", \"25710\", \"24124\", \"25809\", \"25133\", \"6448\", \"26448\", \"26672\", \"25078\", \"24828\", \"27228\", \"27001\", \"25769\", \"2756\", \"26669\", \"27431\", \"27090\", \"26262\", \"24861\", \"27009\", \"25656\", \"24390\", \"24358\", \"24832\", \"25822\", \"27329\", \"24715\", \"27237\", \"26702\", \"25499\", \"25436\", \"24693\", \"27531\", \"26057\", \"1005320\", \"26674\", \"24768\", \"24618\", \"1572512\", \"25476\", \"26739\", \"24661\", \"25747\", \"25780\", \"25365\", \"24692\", \"24687\", \"27537\", \"26200\", \"26664\", \"25672\", \"24975\", \"24875\", \"25399\", \"95374\", \"25830\", \"24651\", \"13\", \"25863\", \"25689\", \"24586\", \"24372\", \"27475\", \"24331\", \"26613\", \"4549\", \"24300\", \"26898\", \"27150\", \"24565\", \"26815\", \"25498\", \"26947\", \"24971\", \"24364\", \"24186\", \"25204\", \"26311\", \"24827\", \"25865\", \"26410\", \"25262\", \"25010\", \"26115\", \"27160\", \"25016\", \"24126\", \"24878\", \"27147\", \"27255\", \"26577\", \"25298\", \"26246\", \"25915\", \"25774\", \"25080\", \"26024\", \"2284\", \"24299\", \"24589\", \"26316\", \"25496\", \"26642\", \"25653\", \"24579\", \"26750\", \"26379\", \"25879\", \"24406\", \"25957\", \"25630\", \"27425\", \"26516\", \"27094\", \"24632\", \"25894\", \"24960\", \"25857\", \"26999\", \"25146\", \"10400\", \"26630\", \"25959\", \"27122\", \"10430\", \"25723\", \"26950\", \"6451\", \"26983\", \"26539\", \"26318\", \"25936\", \"26310\", \"25193\", \"25891\", \"26942\", \"24481\", \"24207\", \"26937\"], \"review\": [\"24099\", \"24100\"]}\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save User-Item Pairs (Train Set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "user_item_pairs = dict()\n",
                "cnt_user_item_pairs = 0\n",
                "for trainset_user_chunk in list(user_item_candidate_sent_ids.items()):\n",
                "    assert isinstance(trainset_user_chunk[0], str)\n",
                "    user_id_str = trainset_user_chunk[0]\n",
                "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "    assert user_id_str not in user_item_pairs\n",
                "    user_item_pairs[user_id_str] = list()\n",
                "    for item_chunk in user_item_chunks:\n",
                "        assert isinstance(item_chunk[0], str)\n",
                "        item_id_str = item_chunk[0]\n",
                "        assert item_id_str not in user_item_pairs[user_id_str]\n",
                "        user_item_pairs[user_id_str].append(item_id_str)\n",
                "        cnt_user_item_pairs += 1\n",
                "print(\"Total number of user-item pair on trainset: {}\".format(cnt_user_item_pairs))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of user-item pair on trainset: 668244\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "source": [
                "train_useritempairs_file = '../Dataset/{}/train/useritem_pairs.json'.format(dataset_name)\n",
                "with open(train_useritempairs_file, 'w') as f:\n",
                "    print(\"write file: {}\".format(train_useritempairs_file))\n",
                "    json.dump(user_item_pairs, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "write file: ../Dataset/yelp/train/useritem_pairs.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}