{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
                "\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "# Create a Tokenizer with the default settings for English\n",
                "# including punctuation rules and exceptions\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset_name = 'wine'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Read Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 10000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/test_filtered.json\n",
                        "10000 lines loaded.\n",
                        "20000 lines loaded.\n",
                        "30000 lines loaded.\n",
                        "40000 lines loaded.\n",
                        "50000 lines loaded.\n",
                        "Finish loading test dataset, totally 59294 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train_filtered.json\n",
                        "100000 lines loaded.\n",
                        "200000 lines loaded.\n",
                        "Finish loading train dataset, totally 248452 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "          item    user  rating  \\\n",
                            "0      1176422  131074      91   \n",
                            "1       892706  131074      92   \n",
                            "2       598605  131074      93   \n",
                            "3       684257  131074      90   \n",
                            "4       544877  131074      97   \n",
                            "...        ...     ...     ...   \n",
                            "59289   224276  130851      89   \n",
                            "59290     4058  130971      94   \n",
                            "59291   260890  130971      91   \n",
                            "59292     1760  152917      93   \n",
                            "59293     2705  152917      89   \n",
                            "\n",
                            "                                                  review  \n",
                            "0      ripe red currant jam , cedar box , spices and ...  \n",
                            "1      this wine is an absolute beauty . quite burgun...  \n",
                            "2      wonderful complex nose . youthful fruit of str...  \n",
                            "3      sensual cabernet with velvety tannins and coco...  \n",
                            "4      explosion of aromas on the nose , from spice t...  \n",
                            "...                                                  ...  \n",
                            "59289  purple red color . blackberries , currants , c...  \n",
                            "59290  ooof , this wine was incredible ! insomnia due...  \n",
                            "59291  decanted 1/2 hour and this nice . lots of choc...  \n",
                            "59292  interesting bottle , but let me start by sayin...  \n",
                            "59293  this much maligned vintage actually is drinkin...  \n",
                            "\n",
                            "[59294 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1176422</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>91</td>\n",
                            "      <td>ripe red currant jam , cedar box , spices and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>892706</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>92</td>\n",
                            "      <td>this wine is an absolute beauty . quite burgun...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>598605</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>93</td>\n",
                            "      <td>wonderful complex nose . youthful fruit of str...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>684257</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>90</td>\n",
                            "      <td>sensual cabernet with velvety tannins and coco...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>544877</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>97</td>\n",
                            "      <td>explosion of aromas on the nose , from spice t...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59289</th>\n",
                            "      <td>224276</td>\n",
                            "      <td>130851</td>\n",
                            "      <td>89</td>\n",
                            "      <td>purple red color . blackberries , currants , c...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59290</th>\n",
                            "      <td>4058</td>\n",
                            "      <td>130971</td>\n",
                            "      <td>94</td>\n",
                            "      <td>ooof , this wine was incredible ! insomnia due...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59291</th>\n",
                            "      <td>260890</td>\n",
                            "      <td>130971</td>\n",
                            "      <td>91</td>\n",
                            "      <td>decanted 1/2 hour and this nice . lots of choc...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59292</th>\n",
                            "      <td>1760</td>\n",
                            "      <td>152917</td>\n",
                            "      <td>93</td>\n",
                            "      <td>interesting bottle , but let me start by sayin...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59293</th>\n",
                            "      <td>2705</td>\n",
                            "      <td>152917</td>\n",
                            "      <td>89</td>\n",
                            "      <td>this much maligned vintage actually is drinkin...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>59294 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "train_groupby_item_user = df_train_data.groupby(['item', 'user'])\n",
                "train_groupby_item_user_dict = dict(tuple(train_groupby_item_user))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "for idx, row in df_test_data.iterrows():\n",
                "    user_id_str = row['user']\n",
                "    item_id_str = row['item']\n",
                "    assert (item_id_str, user_id_str) not in train_groupby_item_user_dict"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2ID and ID2Sentence Mapping From Training Set"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    trainset_sent_to_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    trainset_id_to_sent = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/sentence/id2sentence.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "assert len(trainset_sent_to_id) == len(trainset_id_to_sent)\n",
                "print(\"There are {} sentences in the training set.\".format(len(trainset_id_to_sent)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "There are 554564 sentences in the training set.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "# Feature words are the same between training and testing\n",
                "# since we can only know the review text from training set\n",
                "feature2id_filepath = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature2id_filepath))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "feature_word_list = list(feature_vocab.keys())\n",
                "assert len(feature_word_list) == len(feature_vocab)\n",
                "print('Number of feature words: {}'.format(len(feature_word_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of feature words: 215\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "id2feature_filepath = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(id2feature_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2feature_filepath))\n",
                "    id2feature_train = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Build Sentence Vocab on TestSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Check Whether there are reviews with no sentence"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "invalid_data = 0\n",
                "for idx, row in df_test_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    if len(review_sents) == 0:\n",
                "        print(row)\n",
                "        invalid_data += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "print(invalid_data)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "def get_tf_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    return word_count.toarray()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "# sentence vocab\n",
                "sentence_count = dict()\n",
                "sentence_with_no_feature = 0\n",
                "# Loop for each review\n",
                "# TODO: Do we need to filter out sentences with less than 3 tokens same as during training?\n",
                "for idx, row in df_test_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
                "    tf_sum_sents = np.sum(tf_score, axis=1)\n",
                "    for i in range(len(review_sents)):\n",
                "        if tf_sum_sents[i] != 0.0:\n",
                "            cur_sent = review_sents[i]\n",
                "            sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
                "        else:\n",
                "            sentence_with_no_feature += 1\n",
                "    if (idx+1) % 2000 == 0:\n",
                "        print(\"Processed {} lines\".format(idx+1))\n",
                "print(\"Totally {} tracked sentences\".format(len(sentence_count)))\n",
                "print(\"There are {} sentences with no feature words\".format(sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 2000 lines\n",
                        "Processed 4000 lines\n",
                        "Processed 6000 lines\n",
                        "Processed 8000 lines\n",
                        "Processed 10000 lines\n",
                        "Processed 12000 lines\n",
                        "Processed 14000 lines\n",
                        "Processed 16000 lines\n",
                        "Processed 18000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 22000 lines\n",
                        "Processed 24000 lines\n",
                        "Processed 26000 lines\n",
                        "Processed 28000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 32000 lines\n",
                        "Processed 34000 lines\n",
                        "Processed 36000 lines\n",
                        "Processed 38000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 42000 lines\n",
                        "Processed 44000 lines\n",
                        "Processed 46000 lines\n",
                        "Processed 48000 lines\n",
                        "Processed 50000 lines\n",
                        "Processed 52000 lines\n",
                        "Processed 54000 lines\n",
                        "Processed 56000 lines\n",
                        "Processed 58000 lines\n",
                        "Totally 136421 tracked sentences\n",
                        "There are 115424 sentences with no feature words\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "len(sentence_count)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "136421"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 20
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "# sort sentence based on counts (the majority should be 1)\n",
                "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "# sentence_vocab_list = list(sentence_count.keys())\n",
                "# Building mappings from sentences to ids and ids to sentences\n",
                "testset_sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
                "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
                "testset_id_to_sent = {str(id): sent for (sent, id) in testset_sent_to_id.items()}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save Sentence2ID into Json File (Test / Valid set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "with open('../Dataset/{}/test/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_id_to_sent, f)\n",
                "\n",
                "with open('../Dataset/{}/test/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_sent_to_id, f)\n",
                "\n",
                "with open('../Dataset/{}/valid/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_id_to_sent, f)\n",
                "\n",
                "with open('../Dataset/{}/valid/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_sent_to_id, f)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Get Sentence Feature (Test/Valid Set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "          item    user  rating  \\\n",
                            "0      1176422  131074      91   \n",
                            "1       892706  131074      92   \n",
                            "2       598605  131074      93   \n",
                            "3       684257  131074      90   \n",
                            "4       544877  131074      97   \n",
                            "...        ...     ...     ...   \n",
                            "59289   224276  130851      89   \n",
                            "59290     4058  130971      94   \n",
                            "59291   260890  130971      91   \n",
                            "59292     1760  152917      93   \n",
                            "59293     2705  152917      89   \n",
                            "\n",
                            "                                                  review  \n",
                            "0      ripe red currant jam , cedar box , spices and ...  \n",
                            "1      this wine is an absolute beauty . quite burgun...  \n",
                            "2      wonderful complex nose . youthful fruit of str...  \n",
                            "3      sensual cabernet with velvety tannins and coco...  \n",
                            "4      explosion of aromas on the nose , from spice t...  \n",
                            "...                                                  ...  \n",
                            "59289  purple red color . blackberries , currants , c...  \n",
                            "59290  ooof , this wine was incredible ! insomnia due...  \n",
                            "59291  decanted 1/2 hour and this nice . lots of choc...  \n",
                            "59292  interesting bottle , but let me start by sayin...  \n",
                            "59293  this much maligned vintage actually is drinkin...  \n",
                            "\n",
                            "[59294 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1176422</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>91</td>\n",
                            "      <td>ripe red currant jam , cedar box , spices and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>892706</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>92</td>\n",
                            "      <td>this wine is an absolute beauty . quite burgun...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>598605</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>93</td>\n",
                            "      <td>wonderful complex nose . youthful fruit of str...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>684257</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>90</td>\n",
                            "      <td>sensual cabernet with velvety tannins and coco...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>544877</td>\n",
                            "      <td>131074</td>\n",
                            "      <td>97</td>\n",
                            "      <td>explosion of aromas on the nose , from spice t...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59289</th>\n",
                            "      <td>224276</td>\n",
                            "      <td>130851</td>\n",
                            "      <td>89</td>\n",
                            "      <td>purple red color . blackberries , currants , c...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59290</th>\n",
                            "      <td>4058</td>\n",
                            "      <td>130971</td>\n",
                            "      <td>94</td>\n",
                            "      <td>ooof , this wine was incredible ! insomnia due...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59291</th>\n",
                            "      <td>260890</td>\n",
                            "      <td>130971</td>\n",
                            "      <td>91</td>\n",
                            "      <td>decanted 1/2 hour and this nice . lots of choc...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59292</th>\n",
                            "      <td>1760</td>\n",
                            "      <td>152917</td>\n",
                            "      <td>93</td>\n",
                            "      <td>interesting bottle , but let me start by sayin...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>59293</th>\n",
                            "      <td>2705</td>\n",
                            "      <td>152917</td>\n",
                            "      <td>89</td>\n",
                            "      <td>this much maligned vintage actually is drinkin...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>59294 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 24
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "print(\"Number of users on test set: {}\".format(len(df_test_data['user'].unique())))\n",
                "print(\"Number of items on test set: {}\".format(len(df_test_data['item'].unique())))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on test set: 6080\n",
                        "Number of items on test set: 14529\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "# groupby item\n",
                "group_by_item_test = df_test_data.groupby('item')\n",
                "group_by_item_dict = dict(tuple(group_by_item_test))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "assert len(group_by_item_dict) == len(df_test_data['item'].unique())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "group_by_item_dict['246']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "      item    user  rating                                             review\n",
                            "32     246       5      89  there is nothing similar to a leonetti nose . ...\n",
                            "3231   246     808      90  still young in colour , dark cherry center , c...\n",
                            "31295  246  163680      87  i agree with my previous posting . not a bad w...\n",
                            "49005  246   78074      88  nose was very weak , barely perceptible . howe..."
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>32</th>\n",
                            "      <td>246</td>\n",
                            "      <td>5</td>\n",
                            "      <td>89</td>\n",
                            "      <td>there is nothing similar to a leonetti nose . ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3231</th>\n",
                            "      <td>246</td>\n",
                            "      <td>808</td>\n",
                            "      <td>90</td>\n",
                            "      <td>still young in colour , dark cherry center , c...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>31295</th>\n",
                            "      <td>246</td>\n",
                            "      <td>163680</td>\n",
                            "      <td>87</td>\n",
                            "      <td>i agree with my previous posting . not a bad w...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>49005</th>\n",
                            "      <td>246</td>\n",
                            "      <td>78074</td>\n",
                            "      <td>88</td>\n",
                            "      <td>nose was very weak , barely perceptible . howe...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 28
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "peritem_num_sent_testset = dict()\n",
                "peritemreview_num_sent_testset = list()\n",
                "for key, item_df_test in group_by_item_dict.items():\n",
                "    # print(key)\n",
                "    # print(item_df_test)\n",
                "    reviews_list = item_df_test['review']\n",
                "    sentence_count = 0\n",
                "    for review in reviews_list:\n",
                "        review_sent_count = 0\n",
                "        sentences_review = sent_tokenize(review)\n",
                "        for sent in sentences_review:\n",
                "            if sent in testset_sent_to_id:\n",
                "                sentence_count += 1\n",
                "                review_sent_count += 1\n",
                "        peritemreview_num_sent_testset.append(review_sent_count)\n",
                "    peritem_num_sent_testset[key] = sentence_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "assert len(peritemreview_num_sent_testset) == len(df_test_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "print(\"Number of review in testset: {}\".format(\n",
                "    len(peritemreview_num_sent_testset)))\n",
                "print(\"Mean number of sentence per review in testset: {}\".format(\n",
                "    np.mean(peritemreview_num_sent_testset)))\n",
                "print(\"Min number of sentence per review in testset {}\".format(\n",
                "    np.min(peritemreview_num_sent_testset)))\n",
                "print(\"Max number of sentence per review in testset {}\".format(\n",
                "    np.max(peritemreview_num_sent_testset)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of review in testset: 59294\n",
                        "Mean number of sentence per review in testset: 2.398556346341957\n",
                        "Min number of sentence per review in testset 1\n",
                        "Max number of sentence per review in testset 16\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "print(\"Number of items in testset: {}\".format(\n",
                "    len(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Mean number of sentence per item in testset: {}\".format(\n",
                "    np.mean(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Min number of sentence per item in testset: {}\".format(\n",
                "    np.min(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Max number of sentence per item in testset: {}\".format(\n",
                "    np.max(list(peritem_num_sent_testset.values()))\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of items in testset: 14529\n",
                        "Mean number of sentence per item in testset: 9.788698465138689\n",
                        "Min number of sentence per item in testset: 1\n",
                        "Max number of sentence per item in testset: 128\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "# get the list of sentence text on testset\n",
                "testset_sent_text_list = list(testset_sent_to_id.keys())\n",
                "testset_sent_text_list[:10]\n",
                "# NOTE: Based on the examples, \\\n",
                "# should we set the short sentence threshold to be 2 instead of 3 on train?"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['great wine .',\n",
                            " 'nice wine .',\n",
                            " 'very nice wine .',\n",
                            " 'nose',\n",
                            " 'good acidity .',\n",
                            " 'dark purple color .',\n",
                            " 'dark ruby color .',\n",
                            " 'dark purple .',\n",
                            " 'nice acidity .',\n",
                            " 'excellent wine .']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 34
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "def get_tfidf_embedding(text, feature_word_list):\n",
                "    \"\"\"\n",
                "    :param: text: list, sent_number * word\n",
                "    :return: \n",
                "        vectorizer: \n",
                "            vocabulary_: word2id\n",
                "            get_feature_names(): id2word\n",
                "        tfidf: array [sent_number, max_word_number]\n",
                "    \"\"\"\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    tfidf_transformer = TfidfTransformer()\n",
                "    tfidf = tfidf_transformer.fit_transform(word_count)\n",
                "    tfidf_weight = tfidf.toarray()\n",
                "    return vectorizer, tfidf_weight"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "cntvector, tfidf_weight = get_tfidf_embedding(testset_sent_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "tfidf_weight.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(136421, 215)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 37
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "def check_vocab_is_same(sklearn_vocab, feature_vocab):\n",
                "    if len(sklearn_vocab) == len(feature_vocab):\n",
                "        for key, value in sklearn_vocab.items():\n",
                "            sklearn_vocab_id = value\n",
                "            feature_vocab_id = feature_vocab[key]\n",
                "            if int(feature_vocab_id) == sklearn_vocab_id:\n",
                "                continue\n",
                "            else:\n",
                "                return False\n",
                "    else:\n",
                "        return False\n",
                "    return True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "check_vocab_is_same(cntvector.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 39
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "testset_sentence_to_feature = dict()\n",
                "sentence_with_no_feature = 0\n",
                "tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
                "for i in range(len(testset_sent_text_list)):\n",
                "    cur_sent = testset_sent_text_list[i]\n",
                "    # if this sentence is in the sent_to_id vocabulary\n",
                "    assert cur_sent in testset_sent_to_id\n",
                "    # get the sentence_id (str)\n",
                "    cur_sent_id = testset_sent_to_id[cur_sent]\n",
                "    assert int(cur_sent_id) == i\n",
                "    # find all the feature that has non-zero tf-idf weight\n",
                "    feature_dict = dict()\n",
                "    for j in range(len(tfidf_weight[i])):\n",
                "        if tfidf_weight[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            feature_tfidf = tfidf_weight[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    if len(feature_dict) > 0:\n",
                "        testset_sentence_to_feature[cur_sent_id] = feature_dict\n",
                "    else:\n",
                "        sentence_with_no_feature += 1\n",
                "    if (i+1) % 10000 == 0:\n",
                "        print(\"Processed {} lines\".format(i+1))\n",
                "print(\"Finish. Totally {} lines\".format(i+1))\n",
                "print(\"Totally {} sentences has at least 1 feature and {} sentences don't have feature.\".format(\n",
                "    len(testset_sentence_to_feature), sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 10000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 50000 lines\n",
                        "Processed 60000 lines\n",
                        "Processed 70000 lines\n",
                        "Processed 80000 lines\n",
                        "Processed 90000 lines\n",
                        "Processed 100000 lines\n",
                        "Processed 110000 lines\n",
                        "Processed 120000 lines\n",
                        "Processed 130000 lines\n",
                        "Finish. Totally 136421 lines\n",
                        "Totally 136421 sentences has at least 1 feature and 0 sentences don't have feature.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "# save testset_sentence_to_feature into json file\n",
                "# testset sent_to_id is same as validset, also save this to validset\n",
                "sentence2feature_filepath = '../Dataset/{}/test/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(testset_sentence_to_feature, f)\n",
                "sentence2feature_filepath = '../Dataset/{}/valid/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(testset_sentence_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/wine/test/sentence/sentence2feature.json\n",
                        "Write file: ../Dataset/wine/valid/sentence/sentence2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "print(testset_sentence_to_feature['0'])\n",
                "print(testset_id_to_sent['0'])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'0': 1.0}\n",
                        "great wine .\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "print(testset_sentence_to_feature['12310'])\n",
                "print(testset_id_to_sent['12310'])\n",
                "for fea_id in testset_sentence_to_feature['12310'].keys():\n",
                "    print(id2feature_train[fea_id])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'2': 0.3526602148196162, '3': 0.7377476845216935, '13': 0.5756380172176452}\n",
                        "on the palate , similar notes - lots of citrus , dark chocolate , dark fruits .\n",
                        "palate\n",
                        "dark\n",
                        "citrus\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "# get some statistics of sentence_to_feature on testset/validset\n",
                "num_feature_per_sentence = []\n",
                "for key, value in testset_sentence_to_feature.items():\n",
                "    num_feature_per_sentence.append(len(value))\n",
                "    assert len(value) > 0       # every sentence should have at least 1 feature"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "print(\"Mean number of features per sentence: {}\".format(np.mean(num_feature_per_sentence)))\n",
                "print(\"Max number of features per sentence: {}\".format(np.max(num_feature_per_sentence)))\n",
                "print(\"Min number of features per sentence: {}\".format(np.min(num_feature_per_sentence)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per sentence: 1.7446727410002858\n",
                        "Max number of features per sentence: 13\n",
                        "Min number of features per sentence: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load User to SentenceID"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "train_user2sentids_filepath = '../Dataset/{}/train/user/user2sentids.json'.format(dataset_name)\n",
                "with open(train_user2sentids_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_user2sentids_filepath))\n",
                "    trainset_user_to_sent_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/user/user2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Item to SentenceID"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "train_item2sentids_filepath = '../Dataset/{}/train/item/item2sentids.json'.format(dataset_name)\n",
                "with open(train_item2sentids_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_item2sentids_filepath))\n",
                "    trainset_item_to_sent_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/item/item2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load User-Item Pairs on TrainSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "train_useritem_pairs_filepath = '../Dataset/{}/train/useritem_pairs.json'.format(dataset_name)\n",
                "with open(train_useritem_pairs_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_useritem_pairs_filepath))\n",
                "    trainset_useritem_pairs = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/useritem_pairs.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "# Get the user set and item set on train-set\n",
                "train_user_set = set()\n",
                "train_item_set = set()\n",
                "for key,value in trainset_useritem_pairs.items():\n",
                "    uid = key\n",
                "    assert uid not in train_user_set\n",
                "    train_user_set.add(uid)\n",
                "    for iid in value:\n",
                "        train_item_set.add(iid)\n",
                "print(\"Number of users on the constructed train set: {}\".format(len(train_user_set)))\n",
                "print(\"Number of items on the constructed train set: {}\".format(len(train_item_set)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on the constructed train set: 6080\n",
                        "Number of items on the constructed train set: 15253\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# For Each Data Instance in TestSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy User"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "group_by_user_test = df_test_data.groupby('user')\n",
                "print(\"Number of users on test-set: {}\".format(len(group_by_user_test)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on test-set: 6080\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Construct Valid Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Remember in validset we are doing samping as what we did on the train set"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "import random\n",
                "sample_sent_num = 500           # this should be among 30, 200 and 500\n",
                "user_item_candidate_sent_ids_validset = dict()\n",
                "cnt_empty_true_sentence = 0\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "user_item_candidate_sentence_num = list()\n",
                "user_item_candidate_sentence_num_sampled = list()\n",
                "cnt_being_cut_useritem = 0\n",
                "# Loop over all users\n",
                "user_cnt = 0\n",
                "for user_df_chunk in list(group_by_user_test):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    user_df = user_df_chunk[1]\n",
                "    if user_id_str not in train_user_set:\n",
                "        continue\n",
                "    # get user sentences, these sentences are on TRAIN set\n",
                "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        if item_id_str not in train_item_set:\n",
                "            continue\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sentences, they are on TRAIN set\n",
                "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
                "        # get review_text's sent ids, they are on TEST set\n",
                "        cur_review_sent_ids = set()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the testset_sent_to_id dict\n",
                "        for sent in review_sents:\n",
                "            if sent in testset_sent_to_id:\n",
                "                cur_sent_id = testset_sent_to_id[sent]\n",
                "                # add this sentence into the set of current review\n",
                "                cur_review_sent_ids.add(cur_sent_id)\n",
                "        # construct the candidate set which is an union of user sentence and item sentence\n",
                "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "        # sample some sentences (they are on TRAIN set)\n",
                "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            cnt_being_cut_useritem += 1\n",
                "        else:\n",
                "            # FIXED!!\n",
                "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "        # add this into the dict\n",
                "        if len(cur_review_sent_ids) != 0:\n",
                "            # only add the ones that contain at least 1 true label sentence (on valid/test set)\n",
                "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), list(cur_review_sent_ids)]\n",
                "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
                "            user_item_candidate_sentence_num_sampled.append(len(sample_useritem_sent_ids))\n",
                "        else:\n",
                "            cnt_empty_true_sentence += 1\n",
                "\n",
                "    # add this item-level dict into the user-level dict\n",
                "    if len(item_candidate_sent_ids) == 0:\n",
                "        print(\"User: {} has no useful items, skip it.\".format(user_id_str))\n",
                "    else:\n",
                "        user_item_candidate_sent_ids_validset[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 500 == 0:\n",
                "        print(\"{} user processed.\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {} users'.format(user_cnt))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, cnt_empty_true_sentence))\n",
                "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "500 user processed.\n",
                        "1000 user processed.\n",
                        "1500 user processed.\n",
                        "2000 user processed.\n",
                        "2500 user processed.\n",
                        "3000 user processed.\n",
                        "3500 user processed.\n",
                        "4000 user processed.\n",
                        "4500 user processed.\n",
                        "5000 user processed.\n",
                        "5500 user processed.\n",
                        "6000 user processed.\n",
                        "Finish.\n",
                        "Totally 6080 users\n",
                        "Totally 59294 reviews. Among them 0 reviews has empty true label sentence\n",
                        "During constructing, 9853 user-item pair are being cutted due to their length\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "print(\"Totally {} user item pairs in the testset\".format(len(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"mean number of candidate sentence: {}\".format(np.median(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"max number of candidate sentence: {}\".format(np.max(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"min number of candidate sentence: {}\".format(np.min(user_item_candidate_sentence_num_sampled)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 59294 user item pairs in the testset\n",
                        "mean number of candidate sentence: 224.0\n",
                        "max number of candidate sentence: 500\n",
                        "min number of candidate sentence: 12\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "source": [
                "print(sorted(user_item_candidate_sentence_num)[-200:-100])\n",
                "\n",
                "\"\"\" This shows that if we restrict the candidate sentences to have a maximum number of 1500,\n",
                "we will cut-off about 200 reviews. This will be applied on test set to avoid user-item paris\n",
                "with too many candidate sentences.\n",
                "\"\"\""
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1320, 1323, 1324, 1326, 1326, 1329, 1329, 1330, 1330, 1330, 1335, 1337, 1338, 1340, 1341, 1341, 1345, 1348, 1349, 1350, 1350, 1354, 1354, 1355, 1357, 1359, 1359, 1360, 1362, 1372, 1372, 1373, 1374, 1399, 1419, 1453, 1454, 1457, 1483, 1486, 1488, 1489, 1489, 1490, 1490, 1490, 1491, 1492, 1492, 1493, 1493, 1493, 1494, 1494, 1494, 1497, 1497, 1498, 1498, 1498, 1498, 1498, 1498, 1499, 1500, 1500, 1501, 1501, 1502, 1502, 1503, 1503, 1503, 1505, 1507, 1507, 1508, 1508, 1508, 1509, 1510, 1512, 1512, 1512, 1513, 1514, 1515, 1515, 1519, 1520, 1521, 1523, 1523, 1524, 1525, 1527, 1527, 1531, 1532, 1532]\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "' This shows that if we restrict the candidate sentences to have a maximum number of 1000,\\nwe will cut-off about 1500 reviews. This will be applied on test set to avoid user-item paris\\nwith too many candidate sentences.\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 72
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "source": [
                "len(user_item_candidate_sent_ids_validset)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "6080"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 62
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "source": [
                "# save this into json file\n",
                "valid_useritem2sentids_filepath = '../Dataset/{}/valid/useritem2sentids_test.json'.format(dataset_name)\n",
                "with open(valid_useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(valid_useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids_validset, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/wine/valid/useritem2sentids_test.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "source": [
                "check_user_id = \"17048\"\n",
                "check_item_id = \"17185\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 17048 \t item: 17185\n",
                        "number of sentence in candidate set: 286\n",
                        "number of sentence in true review set: 2\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "source": [
                "for sentid in user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1]:\n",
                "    print(testset_id_to_sent[sentid])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "wow after 3 hours of air this has really opened up , smooth coffee , leather and fruit flavors .\n",
                        "decent nose of ccocoa , black fruit and graphite .\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### However, the GT text is actually: \"first bottle of a dozen . needs at least 2 hours of air before it opens . decent nose of ccocoa , black fruit and graphite . exceptioanlly dry but smoothes out with some air . hope it improves with time . wow after 3 hours of air this has really opened up , smooth coffee , leather and fruit flavors .\". Should think more about how to select features."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "source": [
                "# Checking How Many User/Item/Review are in the valid set\n",
                "cnt_user = 0\n",
                "cnt_review = 0\n",
                "cnt_item_set = set()\n",
                "for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
                "    user_id_str = str(trainset_user_chunk[0])\n",
                "    user_id = int(trainset_user_chunk[0])\n",
                "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "    for item_chunk in user_item_chunks:\n",
                "        item_id_str = str(item_chunk[0])\n",
                "        item_id = int(item_chunk[0])\n",
                "        # candidate_true_sent_ids = item_chunk[1]\n",
                "        # cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'sent_id': candidate_true_sent_ids}\n",
                "        # write this into the json file\n",
                "        # json.dump(cur_data_dict, f1)\n",
                "        # f1.write(\"\\n\")\n",
                "        # assert user_id_str in train_user_id_set\n",
                "        # assert item_id_str in train_item_id_set\n",
                "        cnt_item_set.add(item_id_str)\n",
                "        cnt_review += 1\n",
                "    cnt_user += 1\n",
                "\n",
                "print(\"Total number of reviews: {}\".format(cnt_review))\n",
                "print(\"Total number of user: {}\".format(cnt_user))\n",
                "print(\"Total number of item: {}\".format(len(cnt_item_set)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of reviews: 59294\n",
                        "Total number of user: 6080\n",
                        "Total number of item: 14529\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "source": [
                "# Write useritem2sentids_test into a line-by-line format\n",
                "valid_useritem2sentids_multiline_filepath = '../Dataset/{}/valid/useritem2sentids_test_multilines.json'.format(dataset_name)\n",
                "with open(valid_useritem2sentids_multiline_filepath, 'w') as f1:\n",
                "    print(\"Write file: {}\".format(valid_useritem2sentids_multiline_filepath))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
                "        user_id_str = str(trainset_user_chunk[0])\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            item_id_str = str(item_chunk[0])\n",
                "            item_id = int(item_chunk[0])\n",
                "            item_set.add(item_id_str)\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            true_revw_sent_ids = item_chunk[1][1]\n",
                "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert len(user_set) == cnt_user\n",
                "assert len(useritem_set) == cnt_review\n",
                "print(\"Total {} users\".format(cnt_user))\n",
                "print(\"Total {} items\".format(len(item_set)))\n",
                "print(\"Totat {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/wine/valid/useritem2sentids_test_multilines.json\n",
                        "Total 6080 users\n",
                        "Total 14529 items\n",
                        "Totat 59294 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct Test Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "source": [
                "sample_sent_num = 1500\n",
                "user_item_candidate_sent_ids_testset = dict()\n",
                "cnt_empty_true_sentence = 0\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "user_item_candidate_sentence_num = list()\n",
                "user_item_candidate_sentence_num_sampled = list()\n",
                "cnt_being_cut_useritem = 0\n",
                "# Loop over all users\n",
                "user_cnt = 0\n",
                "for user_df_chunk in list(group_by_user_test):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    if user_id_str not in train_user_set:\n",
                "        continue\n",
                "    user_df = user_df_chunk[1]\n",
                "    # get user sentences, these sentences are on TRAIN set\n",
                "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        if item_id_str not in train_item_set:\n",
                "            continue\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sentences, they are on TRAIN set\n",
                "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
                "        # get review_text's sent ids, they are on TEST set\n",
                "        cur_review_sent_ids = set()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the testset_sent_to_id dict\n",
                "        for sent in review_sents:\n",
                "            if sent in testset_sent_to_id:\n",
                "                cur_sent_id = testset_sent_to_id[sent]\n",
                "                # add this sentence into the set of current review\n",
                "                cur_review_sent_ids.add(cur_sent_id)\n",
                "        # set union\n",
                "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "        # sample some sentences (they are on TRAIN set)\n",
                "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            cnt_being_cut_useritem += 1\n",
                "        else:\n",
                "            # FIXED!!\n",
                "            # sample_useritem_sent_ids = cur_user_sent_ids\n",
                "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "        # add this into the dict\n",
                "        if len(cur_review_sent_ids) != 0:\n",
                "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), list(cur_review_sent_ids)]\n",
                "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
                "            user_item_candidate_sentence_num_sampled.append(len(sample_useritem_sent_ids))\n",
                "        else:\n",
                "            cnt_empty_true_sentence += 1\n",
                "\n",
                "    # add this item-level dict into the user-level dict\n",
                "    user_item_candidate_sent_ids_testset[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 500 == 0:\n",
                "        print(\"{} user processed.\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {} users'.format(user_cnt))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, cnt_empty_true_sentence))\n",
                "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "500 user processed.\n",
                        "1000 user processed.\n",
                        "1500 user processed.\n",
                        "2000 user processed.\n",
                        "2500 user processed.\n",
                        "3000 user processed.\n",
                        "3500 user processed.\n",
                        "4000 user processed.\n",
                        "4500 user processed.\n",
                        "5000 user processed.\n",
                        "5500 user processed.\n",
                        "6000 user processed.\n",
                        "Finish.\n",
                        "Totally 6080 users\n",
                        "Totally 59294 reviews. Among them 0 reviews has empty true label sentence\n",
                        "During constructing, 134 user-item pair are being cutted due to their length\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "source": [
                "print(\"Totally {} user item pairs in the testset\".format(\n",
                "    len(user_item_candidate_sentence_num)))\n",
                "print(\"mean number of candidate sentence: {}\".format(\n",
                "    np.mean(user_item_candidate_sentence_num)))\n",
                "print(\"max number of candidate sentence: {}\".format(\n",
                "    np.max(user_item_candidate_sentence_num)))\n",
                "print(\"min number of candidate sentence: {}\".format(\n",
                "    np.min(user_item_candidate_sentence_num)))\n",
                "print(\"mean number of sampled candidate sentence: {}\".format(\n",
                "    np.mean(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"max number of sampled candidate sentence: {}\".format(\n",
                "    np.max(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"min number of sampled candidate sentence: {}\".format(\n",
                "    np.min(user_item_candidate_sentence_num_sampled)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 59294 user item pairs in the testset\n",
                        "mean number of candidate sentence: 301.1362363814214\n",
                        "max number of candidate sentence: 1981\n",
                        "min number of candidate sentence: 12\n",
                        "mean number of sampled candidate sentence: 300.9716497453368\n",
                        "max number of sampled candidate sentence: 1500\n",
                        "min number of sampled candidate sentence: 12\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "source": [
                "print(sorted(user_item_candidate_sentence_num)[-40:])\n",
                "print(sorted(user_item_candidate_sentence_num_sampled)[-40:])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1585, 1586, 1590, 1592, 1596, 1597, 1598, 1599, 1601, 1604, 1605, 1606, 1607, 1608, 1610, 1612, 1613, 1613, 1616, 1616, 1618, 1620, 1626, 1626, 1628, 1628, 1633, 1634, 1638, 1640, 1641, 1650, 1659, 1680, 1692, 1734, 1739, 1831, 1849, 1981]\n",
                        "[1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "source": [
                "len(user_item_candidate_sent_ids_testset)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "6080"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 76
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "source": [
                "# save this into json file\n",
                "test_useritem2sentids_filepath = '../Dataset/{}/test/useritem2sentids_test.json'.format(dataset_name)\n",
                "with open(test_useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(test_useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids_testset, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/wine/test/useritem2sentids_test.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "source": [
                "review_test_cnt = 0\n",
                "for user_chunk in user_item_candidate_sent_ids_testset.items():\n",
                "    user_id = user_chunk[0]\n",
                "    user_dict = user_chunk[1]\n",
                "    for user_item_chunk in user_dict.items():\n",
                "        item_id = user_item_chunk[0]\n",
                "        candidate_sents = user_item_chunk[0]\n",
                "        true_label_sents = user_item_chunk[1]\n",
                "        review_test_cnt += 1\n",
                "print(review_test_cnt)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "59294\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "source": [
                "check_user_id = \"17048\"\n",
                "check_item_id = \"17185\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 17048 \t item: 17185\n",
                        "number of sentence in candidate set: 286\n",
                        "number of sentence in true review set: 2\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "# Write useritem2sentids_test into a line-by-line format\n",
                "test_useritem2sentids_multiline_filepath = '../Dataset/{}/test/useritem2sentids_test_multilines.json'.format(dataset_name)\n",
                "with open(test_useritem2sentids_multiline_filepath, 'w') as f1:\n",
                "    print(\"Write file: {}\".format(test_useritem2sentids_multiline_filepath))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids_testset.items()):\n",
                "        user_id_str = str(trainset_user_chunk[0])\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            item_id_str = str(item_chunk[0])\n",
                "            item_id = int(item_chunk[0])\n",
                "            item_set.add(item_id_str)\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            true_revw_sent_ids = item_chunk[1][1]\n",
                "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert len(user_set) == cnt_user\n",
                "assert len(useritem_set) == cnt_review\n",
                "print(\"Total {} users\".format(cnt_user))\n",
                "print(\"Total {} items\".format(len(item_set)))\n",
                "print(\"Totat {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/wine/test/useritem2sentids_test_multilines.json\n",
                        "Total 6080 users\n",
                        "Total 14529 items\n",
                        "Totat 59294 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "source": [
                "check_user_id = \"2124\"\n",
                "check_item_id = \"1372\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"[VALID] number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"[VALID] number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))\n",
                "print(\"[TEST]  number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"[TEST]  number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 2124 \t item: 1372\n",
                        "[VALID] number of sentence in candidate set: 500\n",
                        "[VALID] number of sentence in true review set: 1\n",
                        "[TEST]  number of sentence in candidate set: 603\n",
                        "[TEST]  number of sentence in true review set: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}