{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
                "\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "# Create a Tokenizer with the default settings for English\n",
                "# including punctuation rules and exceptions\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset_name = \"yelp\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Read Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 50000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_review_filtered_clean.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 10000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train_review_filtered.json\n",
                        "50000 lines loaded.\n",
                        "100000 lines loaded.\n",
                        "150000 lines loaded.\n",
                        "200000 lines loaded.\n",
                        "250000 lines loaded.\n",
                        "300000 lines loaded.\n",
                        "350000 lines loaded.\n",
                        "400000 lines loaded.\n",
                        "450000 lines loaded.\n",
                        "500000 lines loaded.\n",
                        "550000 lines loaded.\n",
                        "600000 lines loaded.\n",
                        "650000 lines loaded.\n",
                        "Finish loading train dataset, totally 668245 lines.\n",
                        "Load file: ../Dataset/yelp/test_review_filtered_clean.json\n",
                        "10000 lines loaded.\n",
                        "20000 lines loaded.\n",
                        "30000 lines loaded.\n",
                        "40000 lines loaded.\n",
                        "50000 lines loaded.\n",
                        "60000 lines loaded.\n",
                        "70000 lines loaded.\n",
                        "80000 lines loaded.\n",
                        "90000 lines loaded.\n",
                        "100000 lines loaded.\n",
                        "110000 lines loaded.\n",
                        "120000 lines loaded.\n",
                        "130000 lines loaded.\n",
                        "140000 lines loaded.\n",
                        "150000 lines loaded.\n",
                        "160000 lines loaded.\n",
                        "170000 lines loaded.\n",
                        "180000 lines loaded.\n",
                        "Finish loading test dataset, totally 183650 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Convert List Data to Pandas Dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0       10000     0       4  recommendations are right on and always fun to...\n",
                            "1       10019     0       3  just the same pricing but with less item selec...\n",
                            "2        1002     0       3  taste much better than ayce sushi ! the sushi ...\n",
                            "3       10020     0       3  this is a brand new location of ruelo that rec...\n",
                            "4       10039     0       4  medium rare is a little dry ... since the stea...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "668240   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "668241    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "668242   7379  9999       5                             everything was great .\n",
                            "668243   8530  9999       4  our last experience here went pretty well and ...\n",
                            "668244   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[668245 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>recommendations are right on and always fun to...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10019</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>just the same pricing but with less item selec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1002</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>taste much better than ayce sushi ! the sushi ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10020</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>this is a brand new location of ruelo that rec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>10039</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>medium rare is a little dry ... since the stea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668240</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668241</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668242</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668243</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668244</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>668245 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "print(\"Number of users on train: {}\\tNumber of items on train: {}\".format(\n",
                "    len(df_train_data['user'].unique()), len(df_train_data['item'].unique())\n",
                "))\n",
                "print(\"Number of users on test: {}\\tNumber of items on test: {}\".format(\n",
                "    len(df_test_data['user'].unique()), len(df_test_data['item'].unique())\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on train: 15639\tNumber of items on train: 21515\n",
                        "Number of users on test: 15633\tNumber of items on test: 20840\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Compute Sentence Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def catDoc(textlist):\n",
                "    res = []\n",
                "    for tlist in textlist:\n",
                "        res.extend(tlist)\n",
                "    return res"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "def get_tfidf_embedding(text, feature_word_list):\n",
                "    \"\"\"\n",
                "    :param: text: list, sent_number * word\n",
                "    :return: \n",
                "        vectorizer: \n",
                "            vocabulary_: word2id\n",
                "            get_feature_names(): id2word\n",
                "        tfidf: array [sent_number, max_word_number]\n",
                "    \"\"\"\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    tfidf_transformer = TfidfTransformer()\n",
                "    tfidf = tfidf_transformer.fit_transform(word_count)\n",
                "    tfidf_weight = tfidf.toarray()\n",
                "    return vectorizer, tfidf_weight"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def get_tf_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    return word_count.toarray()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "def get_df_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    # from word count (i.e. tf) get document frequency (i.e. df)\n",
                "    df_count = np.sum(word_count.toarray()>0, axis=0)\n",
                "    return df_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def compress_array(a, id2word, vocab):\n",
                "    \"\"\"\n",
                "    :param a: matrix, [N, M], N is document number, M is word number\n",
                "    :param id2word: word id to word\n",
                "    :return: \n",
                "    \"\"\"\n",
                "    d = {}\n",
                "    # Loop over documents\n",
                "    for i in range(len(a)):\n",
                "        d[i] = {}\n",
                "        # Loop over words\n",
                "        for j in range(len(a[i])):\n",
                "            if a[i][j] != 0:\n",
                "                wid_voc = vocab[id2word[j]]\n",
                "                d[i][wid_voc] = a[i][j]\n",
                "    return d"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "feature_2_id_file = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature_2_id_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature_2_id_file))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "len(feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "498"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 14
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "feature_vocab['wifi']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'29'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "feature_word_list = list(feature_vocab.keys())\n",
                "print('Number of feature words: {}'.format(len(feature_word_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of feature words: 498\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "id2feature_dict = dict()\n",
                "for key,value in feature_vocab.items():\n",
                "    id2feature_dict[value] = key"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "id_2_feature_file = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(id_2_feature_file, 'w') as f:\n",
                "    print(\"Write file: {}\".format(id_2_feature_file))\n",
                "    json.dump(id2feature_dict, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Check Whether there are reviews with no sentences"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "invalid_data = 0\n",
                "for idx, row in df_train_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    if len(review_sents) == 0:\n",
                "        print(row)\n",
                "        invalid_data += 1\n",
                "print(\"Invalid reviews: {}\".format(invalid_data))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Invalid reviews: 0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct Sentence Vocab"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "# sentence vocab\n",
                "sentence_count = dict()\n",
                "sentence_with_no_feature = 0\n",
                "# Loop for each review\n",
                "for idx, row in df_train_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
                "    # _, tf_score = get_tfidf_embedding(review_sents, feature_word_list)\n",
                "    # Sum up the tf-value for each sentence so that if this sum is 0, this sentence should be removed\n",
                "    tfidf_sum_sents = np.sum(tf_score, axis=1)\n",
                "    for i in range(len(review_sents)):\n",
                "        if tfidf_sum_sents[i] != 0.0:\n",
                "            cur_sent = review_sents[i]\n",
                "            # check whether this sentence has more than 3 tokens\n",
                "            tokens = word_tokenize(cur_sent)\n",
                "            cnt_tokens = 0\n",
                "            for token in tokens:\n",
                "                if token.isdigit() or (token in punct):\n",
                "                    pass\n",
                "                else:\n",
                "                    cnt_tokens += 1\n",
                "            # only sentence with more than (or equal to) 2 effective tokens \n",
                "            # can be added into the sentence vocab\n",
                "            if cnt_tokens < 2:\n",
                "                pass\n",
                "            else:\n",
                "                sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
                "        else:\n",
                "            sentence_with_no_feature += 1\n",
                "    if (idx+1) % 10000 == 0:\n",
                "        print(\"Processed {} lines\".format(idx+1))\n",
                "print('Finish.')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 10000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 50000 lines\n",
                        "Processed 60000 lines\n",
                        "Processed 70000 lines\n",
                        "Processed 80000 lines\n",
                        "Processed 90000 lines\n",
                        "Processed 100000 lines\n",
                        "Processed 110000 lines\n",
                        "Processed 120000 lines\n",
                        "Processed 130000 lines\n",
                        "Processed 140000 lines\n",
                        "Processed 150000 lines\n",
                        "Processed 160000 lines\n",
                        "Processed 170000 lines\n",
                        "Processed 180000 lines\n",
                        "Processed 190000 lines\n",
                        "Processed 200000 lines\n",
                        "Processed 210000 lines\n",
                        "Processed 220000 lines\n",
                        "Processed 230000 lines\n",
                        "Processed 240000 lines\n",
                        "Processed 250000 lines\n",
                        "Processed 260000 lines\n",
                        "Processed 270000 lines\n",
                        "Processed 280000 lines\n",
                        "Processed 290000 lines\n",
                        "Processed 300000 lines\n",
                        "Processed 310000 lines\n",
                        "Processed 320000 lines\n",
                        "Processed 330000 lines\n",
                        "Processed 340000 lines\n",
                        "Processed 350000 lines\n",
                        "Processed 360000 lines\n",
                        "Processed 370000 lines\n",
                        "Processed 380000 lines\n",
                        "Processed 390000 lines\n",
                        "Processed 400000 lines\n",
                        "Processed 410000 lines\n",
                        "Processed 420000 lines\n",
                        "Processed 430000 lines\n",
                        "Processed 440000 lines\n",
                        "Processed 450000 lines\n",
                        "Processed 460000 lines\n",
                        "Processed 470000 lines\n",
                        "Processed 480000 lines\n",
                        "Processed 490000 lines\n",
                        "Processed 500000 lines\n",
                        "Processed 510000 lines\n",
                        "Processed 520000 lines\n",
                        "Processed 530000 lines\n",
                        "Processed 540000 lines\n",
                        "Processed 550000 lines\n",
                        "Processed 560000 lines\n",
                        "Processed 570000 lines\n",
                        "Processed 580000 lines\n",
                        "Processed 590000 lines\n",
                        "Processed 600000 lines\n",
                        "Processed 610000 lines\n",
                        "Processed 620000 lines\n",
                        "Processed 630000 lines\n",
                        "Processed 640000 lines\n",
                        "Processed 650000 lines\n",
                        "Processed 660000 lines\n",
                        "Finish.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "print(\"Number of sentences with feature word(s): {0}\\nNumber of sentences w/o feature word: {1}\".format(\n",
                "    len(sentence_count), sentence_with_no_feature\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of sentences with feature word(s): 1617208\n",
                        "Number of sentences w/o feature word: 14947\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "# sort sentence based on counts (the majority should be 1)\n",
                "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "# sentence_vocab_list = list(sentence_count.keys())\n",
                "# Building mappings from sentences to ids and ids to sentences\n",
                "sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
                "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
                "id_to_sent = {str(id): sent for (sent, id) in sent_to_id.items()}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "assert len(sent_to_id) == len(id_to_sent)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "id_to_sent['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service was good .'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 25
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(id2sentence_filepath))\n",
                "    json.dump(id_to_sent, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/sentence/id2sentence.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2id_filepath))\n",
                "    json.dump(sent_to_id, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "# Load id2sentence and sentence2id, check whether they are the same as the newly processed mappings\n",
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    trainset_id2sent = json.load(f)\n",
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    trainset_sent2id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/id2sentence.json\n",
                        "Load file: ../Dataset/yelp/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "assert trainset_id2sent == id_to_sent\n",
                "assert trainset_sent2id == sent_to_id"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Get Sentence Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0       10000     0       4  recommendations are right on and always fun to...\n",
                            "1       10019     0       3  just the same pricing but with less item selec...\n",
                            "2        1002     0       3  taste much better than ayce sushi ! the sushi ...\n",
                            "3       10020     0       3  this is a brand new location of ruelo that rec...\n",
                            "4       10039     0       4  medium rare is a little dry ... since the stea...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "668240   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "668241    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "668242   7379  9999       5                             everything was great .\n",
                            "668243   8530  9999       4  our last experience here went pretty well and ...\n",
                            "668244   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[668245 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>recommendations are right on and always fun to...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10019</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>just the same pricing but with less item selec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1002</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>taste much better than ayce sushi ! the sushi ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10020</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>this is a brand new location of ruelo that rec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>10039</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>medium rare is a little dry ... since the stea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668240</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668241</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668242</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668243</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668244</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>668245 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 30
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "def check_vocab_is_same(sklearn_vocab, feature_vocab):\n",
                "    if len(sklearn_vocab) == len(feature_vocab):\n",
                "        for key, value in sklearn_vocab.items():\n",
                "            sklearn_vocab_id = value\n",
                "            feature_vocab_id = feature_vocab[key]\n",
                "            if int(feature_vocab_id) == sklearn_vocab_id:\n",
                "                continue\n",
                "            else:\n",
                "                return False\n",
                "    else:\n",
                "        return False\n",
                "    return True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "sentence_text_list = list(sent_to_id.keys())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "len(sentence_text_list)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "1617208"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 33
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "sentence_text_list[:10]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['service was good .',\n",
                            " 'great service .',\n",
                            " 'service was great .',\n",
                            " 'service was excellent .',\n",
                            " 'the service was great .',\n",
                            " 'friendly staff .',\n",
                            " 'the food was good .',\n",
                            " 'great food .',\n",
                            " 'the service was good .',\n",
                            " 'good food .']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 34
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "cntvector, tfidf_weight = get_tfidf_embedding(sentence_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "df_count = get_df_score(sentence_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "df_count.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(498,)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 37
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "trainset_feature_df = dict()\n",
                "trainset_feature_df_norm = dict()\n",
                "for i in range(len(feature_word_list)):\n",
                "    trainset_feature_df[feature_word_list[i]] = df_count[i]\n",
                "    trainset_feature_df_norm[feature_word_list[i]] = df_count[i]/len(sentence_text_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "type(trainset_feature_df['wifi'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "numpy.int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 40
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "for key, value in trainset_feature_df.items():\n",
                "    if isinstance(value, np.int64):\n",
                "        trainset_feature_df[key] = int(value)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "type(trainset_feature_df['wifi'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "int"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 42
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "trainset_feat_df_file = '../Dataset/{}/train/feature/feature2df.json'.format(dataset_name)\n",
                "\n",
                "with open(trainset_feat_df_file, 'w') as f:\n",
                "    print(\"Write file: {}\".format(trainset_feat_df_file))\n",
                "    json.dump(trainset_feature_df, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/feature/feature2df.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "trainset_feature_df_sort = dict(sorted(trainset_feature_df.items(), key = lambda x: -x[1]))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "trainset_feature_df_sort_list = list(trainset_feature_df_sort.keys())\n",
                "trainset_feature_df_sort_rank = dict()\n",
                "for i in range(len(trainset_feature_df_sort_list)):\n",
                "    trainset_feature_df_sort_rank[trainset_feature_df_sort_list[i]] = i+1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "this_word = 'wifi'\n",
                "print(\"df value: {}\".format(trainset_feature_df[this_word]))\n",
                "print(\"rank of the feature: {}\".format(trainset_feature_df_sort_rank[this_word]))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "df value: 1527\n",
                        "rank of the feature: 356\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "tfidf_weight.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(1617208, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 47
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "check_vocab_is_same(cntvector.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 48
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "sentence_to_feature = dict()\n",
                "sentence_with_no_feature = 0\n",
                "tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
                "print(\"Shape of tf-idf sum: {}\".format(tfidf_sum_sents.shape))\n",
                "for i in range(len(sentence_text_list)):\n",
                "    cur_sent = sentence_text_list[i]\n",
                "    # if this sentence is in the sent_to_id vocabulary\n",
                "    assert cur_sent in sent_to_id\n",
                "    # get the sentence_id (str)\n",
                "    cur_sent_id = sent_to_id[cur_sent]\n",
                "    assert int(cur_sent_id) == i\n",
                "    # find all the feature that has non-zero tf-idf weight\n",
                "    feature_dict = dict()\n",
                "    for j in range(len(tfidf_weight[i])):\n",
                "        if tfidf_weight[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            feature_tfidf = tfidf_weight[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    if len(feature_dict) > 0:\n",
                "        sentence_to_feature[cur_sent_id] = feature_dict\n",
                "    else:\n",
                "        sentence_with_no_feature += 1\n",
                "    if (i+1) % 50000 == 0:\n",
                "        print(\"Processed {} lines\".format(i+1))\n",
                "print(\"Finish. Totally {} lines\".format(i+1))\n",
                "print(\"Totally {} sentences has at least 1 feature and {} sentences don't have feature.\".format(\n",
                "    len(sentence_to_feature), sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Shape of tf-idf sum: (1617208,)\n",
                        "Processed 50000 lines\n",
                        "Processed 100000 lines\n",
                        "Processed 150000 lines\n",
                        "Processed 200000 lines\n",
                        "Processed 250000 lines\n",
                        "Processed 300000 lines\n",
                        "Processed 350000 lines\n",
                        "Processed 400000 lines\n",
                        "Processed 450000 lines\n",
                        "Processed 500000 lines\n",
                        "Processed 550000 lines\n",
                        "Processed 600000 lines\n",
                        "Processed 650000 lines\n",
                        "Processed 700000 lines\n",
                        "Processed 750000 lines\n",
                        "Processed 800000 lines\n",
                        "Processed 850000 lines\n",
                        "Processed 900000 lines\n",
                        "Processed 950000 lines\n",
                        "Processed 1000000 lines\n",
                        "Processed 1050000 lines\n",
                        "Processed 1100000 lines\n",
                        "Processed 1150000 lines\n",
                        "Processed 1200000 lines\n",
                        "Processed 1250000 lines\n",
                        "Processed 1300000 lines\n",
                        "Processed 1350000 lines\n",
                        "Processed 1400000 lines\n",
                        "Processed 1450000 lines\n",
                        "Processed 1500000 lines\n",
                        "Processed 1550000 lines\n",
                        "Processed 1600000 lines\n",
                        "Finish. Totally 1617208 lines\n",
                        "Totally 1617208 sentences has at least 1 feature and 0 sentences don't have feature.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "sentence2feature_filepath = '../Dataset/{}/train/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(sentence_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/sentence/sentence2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "sentence_to_feature['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "{'10': 1.0}"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "id_to_sent['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service was good .'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 52
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "id2feature_dict['10']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 53
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "num_feature_per_sentence = []\n",
                "for key, value in sentence_to_feature.items():\n",
                "    num_feature_per_sentence.append(len(value))\n",
                "    assert len(value) > 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "source": [
                "print(\"Mean number of features per sentence: {}\".format(np.mean(num_feature_per_sentence)))\n",
                "print(\"Max number of features per sentence: {}\".format(np.max(num_feature_per_sentence)))\n",
                "print(\"Min number of features per sentence: {}\".format(np.min(num_feature_per_sentence)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per sentence: 2.0742854351450153\n",
                        "Max number of features per sentence: 38\n",
                        "Min number of features per sentence: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Get User to Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0       10000     0       4  recommendations are right on and always fun to...\n",
                            "1       10019     0       3  just the same pricing but with less item selec...\n",
                            "2        1002     0       3  taste much better than ayce sushi ! the sushi ...\n",
                            "3       10020     0       3  this is a brand new location of ruelo that rec...\n",
                            "4       10039     0       4  medium rare is a little dry ... since the stea...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "668240   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "668241    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "668242   7379  9999       5                             everything was great .\n",
                            "668243   8530  9999       4  our last experience here went pretty well and ...\n",
                            "668244   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[668245 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>10000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>recommendations are right on and always fun to...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10019</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>just the same pricing but with less item selec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1002</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>taste much better than ayce sushi ! the sushi ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>10020</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>this is a brand new location of ruelo that rec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>10039</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>medium rare is a little dry ... since the stea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668240</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668241</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668242</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668243</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>668244</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>668245 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 56
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy User"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "group_by_user = df_train_data.groupby('user')\n",
                "user_id_list = []\n",
                "user_reviews = []\n",
                "# Loop over all user\n",
                "for user_df_chunk in list(group_by_user):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_df = user_df_chunk[1]\n",
                "    user_text = \" \".join(list(user_df['review']))\n",
                "    user_id_list.append(user_id)\n",
                "    user_reviews.append(user_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "source": [
                "print(\"Number of users: {}\".format(len(user_id_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users: 15639\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "source": [
                "assert len(user_id_list) == len(user_reviews)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute User Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "source": [
                "cntvector_user, tfidf_weight_user = get_tfidf_embedding(user_reviews, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "source": [
                "check_vocab_is_same(cntvector_user.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 61
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "source": [
                "tfidf_weight_user.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(15639, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 62
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "source": [
                "print(feature_word_list[:20])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['beer', 'recommendations', 'beers', 'drink', 'food', 'selections', 'pricing', 'sushi', 'taste', 'bite', 'service', 'location', 'plaza', 'customer', 'tea', 'tables', 'sesame', 'filling', 'steak', 'medium']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "source": [
                "user_to_feature = dict()\n",
                "for i in range(len(user_id_list)):\n",
                "    feature_dict = dict()\n",
                "    cur_user_id = user_id_list[i]\n",
                "    assert len(tfidf_weight_user[i]) == len(feature_vocab)\n",
                "    for j in range(len(tfidf_weight_user[i])):\n",
                "        if tfidf_weight_user[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            # NOTE: make sure that the feature_id is str format\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            assert feature_vocab[feature] == feature_id\n",
                "            feature_tfidf = tfidf_weight_user[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    assert len(feature_dict) > 0\n",
                "    user_to_feature[str(cur_user_id)] = feature_dict\n",
                "    if (i+1) % 1000 == 0:\n",
                "        print(\"{} user processed.\".format(i+1))\n",
                "print(\"Totally {} users\".format(i+1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed.\n",
                        "2000 user processed.\n",
                        "3000 user processed.\n",
                        "4000 user processed.\n",
                        "5000 user processed.\n",
                        "6000 user processed.\n",
                        "7000 user processed.\n",
                        "8000 user processed.\n",
                        "9000 user processed.\n",
                        "10000 user processed.\n",
                        "11000 user processed.\n",
                        "12000 user processed.\n",
                        "13000 user processed.\n",
                        "14000 user processed.\n",
                        "15000 user processed.\n",
                        "Totally 15639 users\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "source": [
                "len(user_to_feature)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "15639"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 65
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "source": [
                "num_feature_per_user = []\n",
                "for key,value in user_to_feature.items():\n",
                "    num_feature_per_user.append(len(value))\n",
                "    assert len(value) > 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "source": [
                "print(\"Mean number of features per user: {}\".format(np.mean(num_feature_per_user)))\n",
                "print(\"Max number of features per user: {}\".format(np.max(num_feature_per_user)))\n",
                "print(\"Min number of features per user: {}\".format(np.min(num_feature_per_user)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per user: 86.91674659505084\n",
                        "Max number of features per user: 477\n",
                        "Min number of features per user: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "source": [
                "len(user_to_feature['3'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "449"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 68
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save User to Feature Mapping into Json File"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "source": [
                "user2feature_filepath = '../Dataset/{}/train/user/user2feature.json'.format(dataset_name)\n",
                "with open(user2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(user2feature_filepath))\n",
                "    json.dump(user_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/user/user2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Get Item to Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy Item"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "source": [
                "group_by_item = df_train_data.groupby('item')\n",
                "item_id_list = []\n",
                "item_reviews = []\n",
                "# Loop over all user\n",
                "for item_df_chunk in list(group_by_item):\n",
                "    item_id = str(item_df_chunk[0])\n",
                "    item_df = item_df_chunk[1]\n",
                "    item_text = \" \".join(list(item_df['review']))\n",
                "    item_id_list.append(item_id)\n",
                "    item_reviews.append(item_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "source": [
                "print(\"Number of items: {}\".format(len(item_id_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of items: 21515\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "source": [
                "assert len(item_id_list) == len(item_reviews)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute Item Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "source": [
                "cntvector_item, tfidf_weight_item = get_tfidf_embedding(item_reviews, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "source": [
                "check_vocab_is_same(cntvector_item.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 74
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "source": [
                "tfidf_weight_item.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(21515, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 75
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "source": [
                "item_to_feature = dict()\n",
                "for i in range(len(item_id_list)):\n",
                "    feature_dict = dict()\n",
                "    cur_item_id = item_id_list[i]\n",
                "    assert len(tfidf_weight_item[i]) == len(feature_vocab)\n",
                "    for j in range(len(tfidf_weight_item[i])):\n",
                "        if tfidf_weight_item[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            assert feature_id == feature_vocab[feature]\n",
                "            feature_tfidf = tfidf_weight_item[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    assert len(feature_dict) > 0\n",
                "    item_to_feature[cur_item_id] = feature_dict\n",
                "    if (i+1) % 1000 == 0:\n",
                "        print(\"{} items processed.\".format(i+1))\n",
                "print('Finish. Totally {} items'.format(i+1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 items processed.\n",
                        "2000 items processed.\n",
                        "3000 items processed.\n",
                        "4000 items processed.\n",
                        "5000 items processed.\n",
                        "6000 items processed.\n",
                        "7000 items processed.\n",
                        "8000 items processed.\n",
                        "9000 items processed.\n",
                        "10000 items processed.\n",
                        "11000 items processed.\n",
                        "12000 items processed.\n",
                        "13000 items processed.\n",
                        "14000 items processed.\n",
                        "15000 items processed.\n",
                        "16000 items processed.\n",
                        "17000 items processed.\n",
                        "18000 items processed.\n",
                        "19000 items processed.\n",
                        "20000 items processed.\n",
                        "21000 items processed.\n",
                        "Finish. Totally 21515 items\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "source": [
                "len(item_to_feature)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "21515"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 77
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "source": [
                "num_feature_per_item = []\n",
                "for key,value in item_to_feature.items():\n",
                "    num_feature_per_item.append(len(value))\n",
                "    assert len(value) > 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "source": [
                "print(\"Mean number of features per item: {}\".format(np.mean(num_feature_per_item)))\n",
                "print(\"Max number of features per item: {}\".format(np.max(num_feature_per_item)))\n",
                "print(\"Min number of features per item: {}\".format(np.min(num_feature_per_item)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per item: 57.07376249128515\n",
                        "Max number of features per item: 351\n",
                        "Min number of features per item: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "item2feature_filepath = '../Dataset/{}/train/item/item2feature.json'.format(dataset_name)\n",
                "with open(item2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(item2feature_filepath))\n",
                "    json.dump(item_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/item/item2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Compute Top User/Item Features"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "source": [
                "# TODO: Sanity Check"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}