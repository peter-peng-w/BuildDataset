{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
                "\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "# Create a Tokenizer with the default settings for English\n",
                "# including punctuation rules and exceptions\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset_name = \"yelp\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Read Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 50000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_review_filtered_clean.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 10000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train_review_filtered.json\n",
                        "50000 lines loaded.\n",
                        "100000 lines loaded.\n",
                        "150000 lines loaded.\n",
                        "Finish loading train dataset, totally 191227 lines.\n",
                        "Load file: ../Dataset/yelp/test_review_filtered_clean.json\n",
                        "10000 lines loaded.\n",
                        "20000 lines loaded.\n",
                        "30000 lines loaded.\n",
                        "40000 lines loaded.\n",
                        "Finish loading test dataset, totally 42702 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Convert List Data to Pandas Dataframe"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0        1071  1001       4  place was well lit , very clean , and staff ve...\n",
                            "1       10877  1001       3  the price was great and so was the subs ... th...\n",
                            "2        1114  1001       3  previous i was bundled into a takeout work ord...\n",
                            "3         120  1001       5  this is my favorite restaurant by far . food o...\n",
                            "4        1278  1001       4  nice little gem right here . ok , staff were v...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "191222   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "191223    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "191224   7379  9999       5                             everything was great .\n",
                            "191225   8530  9999       4  our last experience here went pretty well and ...\n",
                            "191226   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[191227 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1071</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>place was well lit , very clean , and staff ve...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10877</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the price was great and so was the subs ... th...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1114</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>3</td>\n",
                            "      <td>previous i was bundled into a takeout work ord...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>120</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this is my favorite restaurant by far . food o...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1278</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>nice little gem right here . ok , staff were v...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191222</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191223</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191224</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191225</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191226</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>191227 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "print(\"Number of users on train: {}\\tNumber of items on train: {}\".format(\n",
                "    len(df_train_data['user'].unique()), len(df_train_data['item'].unique())\n",
                "))\n",
                "print(\"Number of users on test: {}\\tNumber of items on test: {}\".format(\n",
                "    len(df_test_data['user'].unique()), len(df_test_data['item'].unique())\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on train: 4604\tNumber of items on train: 7837\n",
                        "Number of users on test: 4604\tNumber of items on test: 7602\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Compute Sentence Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def catDoc(textlist):\n",
                "    res = []\n",
                "    for tlist in textlist:\n",
                "        res.extend(tlist)\n",
                "    return res"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "def get_tfidf_embedding(text, feature_word_list):\n",
                "    \"\"\"\n",
                "    :param: text: list, sent_number * word\n",
                "    :return: \n",
                "        vectorizer: \n",
                "            vocabulary_: word2id\n",
                "            get_feature_names(): id2word\n",
                "        tfidf: array [sent_number, max_word_number]\n",
                "    \"\"\"\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    tfidf_transformer = TfidfTransformer()\n",
                "    tfidf = tfidf_transformer.fit_transform(word_count)\n",
                "    tfidf_weight = tfidf.toarray()\n",
                "    return vectorizer, tfidf_weight"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def get_tf_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    return word_count.toarray()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "def get_df_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    # from word count (i.e. tf) get document frequency (i.e. df)\n",
                "    df_count = np.sum(word_count.toarray()>0, axis=0)\n",
                "    return df_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def compress_array(a, id2word, vocab):\n",
                "    \"\"\"\n",
                "    :param a: matrix, [N, M], N is document number, M is word number\n",
                "    :param id2word: word id to word\n",
                "    :return: \n",
                "    \"\"\"\n",
                "    d = {}\n",
                "    # Loop over documents\n",
                "    for i in range(len(a)):\n",
                "        d[i] = {}\n",
                "        # Loop over words\n",
                "        for j in range(len(a[i])):\n",
                "            if a[i][j] != 0:\n",
                "                wid_voc = vocab[id2word[j]]\n",
                "                d[i][wid_voc] = a[i][j]\n",
                "    return d"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "feature_2_id_file = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature_2_id_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature_2_id_file))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "len(feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "498"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 14
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "feature_vocab['wifi']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'371'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "feature_word_list = list(feature_vocab.keys())\n",
                "print('Number of feature words: {}'.format(len(feature_word_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of feature words: 498\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "id2feature_dict = dict()\n",
                "for key,value in feature_vocab.items():\n",
                "    id2feature_dict[value] = key"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "id_2_feature_file = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(id_2_feature_file, 'w') as f:\n",
                "    print(\"Write file: {}\".format(id_2_feature_file))\n",
                "    json.dump(id2feature_dict, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Check Whether there are reviews with no sentences"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "invalid_data = 0\n",
                "for idx, row in df_train_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    if len(review_sents) == 0:\n",
                "        print(row)\n",
                "        invalid_data += 1\n",
                "print(\"Invalid reviews: {}\".format(invalid_data))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Invalid reviews: 0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct Sentence Vocab"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "# sentence vocab\n",
                "sentence_count = dict()\n",
                "sentence_with_no_feature = 0\n",
                "# Loop for each review\n",
                "for idx, row in df_train_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
                "    # _, tf_score = get_tfidf_embedding(review_sents, feature_word_list)\n",
                "    # Sum up the tf-value for each sentence so that if this sum is 0, this sentence should be removed\n",
                "    tfidf_sum_sents = np.sum(tf_score, axis=1)\n",
                "    for i in range(len(review_sents)):\n",
                "        if tfidf_sum_sents[i] != 0.0:\n",
                "            cur_sent = review_sents[i]\n",
                "            # check whether this sentence has more than 3 tokens\n",
                "            tokens = word_tokenize(cur_sent)\n",
                "            cnt_tokens = 0\n",
                "            for token in tokens:\n",
                "                if token.isdigit() or (token in punct):\n",
                "                    pass\n",
                "                else:\n",
                "                    cnt_tokens += 1\n",
                "            # only sentence with more than (or equal to) 2 effective tokens \n",
                "            # can be added into the sentence vocab\n",
                "            if cnt_tokens < 2:\n",
                "                pass\n",
                "            else:\n",
                "                sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
                "        else:\n",
                "            sentence_with_no_feature += 1\n",
                "    if (idx+1) % 10000 == 0:\n",
                "        print(\"Processed {} lines\".format(idx+1))\n",
                "print('Finish.')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 10000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 50000 lines\n",
                        "Processed 60000 lines\n",
                        "Processed 70000 lines\n",
                        "Processed 80000 lines\n",
                        "Processed 90000 lines\n",
                        "Processed 100000 lines\n",
                        "Processed 110000 lines\n",
                        "Processed 120000 lines\n",
                        "Processed 130000 lines\n",
                        "Processed 140000 lines\n",
                        "Processed 150000 lines\n",
                        "Processed 160000 lines\n",
                        "Processed 170000 lines\n",
                        "Processed 180000 lines\n",
                        "Processed 190000 lines\n",
                        "Finish.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "print(\"Number of sentences with feature word(s): {0}\\nNumber of sentences w/o feature word: {1}\".format(\n",
                "    len(sentence_count), sentence_with_no_feature\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of sentences with feature word(s): 492739\n",
                        "Number of sentences w/o feature word: 4292\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "# sort sentence based on counts (the majority should be 1)\n",
                "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "# sentence_vocab_list = list(sentence_count.keys())\n",
                "# Building mappings from sentences to ids and ids to sentences\n",
                "sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
                "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
                "id_to_sent = {str(id): sent for (sent, id) in sent_to_id.items()}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "assert len(sent_to_id) == len(id_to_sent)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "id_to_sent['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service was good .'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 25
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(id2sentence_filepath))\n",
                "    json.dump(id_to_sent, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/sentence/id2sentence.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2id_filepath))\n",
                "    json.dump(sent_to_id, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "# Load id2sentence and sentence2id, check whether they are the same as the newly processed mappings\n",
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    trainset_id2sent = json.load(f)\n",
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    trainset_sent2id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/id2sentence.json\n",
                        "Load file: ../Dataset/yelp/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "assert trainset_id2sent == id_to_sent\n",
                "assert trainset_sent2id == sent_to_id"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Get Sentence Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0        1071  1001       4  place was well lit , very clean , and staff ve...\n",
                            "1       10877  1001       3  the price was great and so was the subs ... th...\n",
                            "2        1114  1001       3  previous i was bundled into a takeout work ord...\n",
                            "3         120  1001       5  this is my favorite restaurant by far . food o...\n",
                            "4        1278  1001       4  nice little gem right here . ok , staff were v...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "191222   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "191223    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "191224   7379  9999       5                             everything was great .\n",
                            "191225   8530  9999       4  our last experience here went pretty well and ...\n",
                            "191226   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[191227 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1071</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>place was well lit , very clean , and staff ve...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10877</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the price was great and so was the subs ... th...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1114</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>3</td>\n",
                            "      <td>previous i was bundled into a takeout work ord...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>120</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this is my favorite restaurant by far . food o...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1278</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>nice little gem right here . ok , staff were v...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191222</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191223</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191224</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191225</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191226</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>191227 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 30
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "def check_vocab_is_same(sklearn_vocab, feature_vocab):\n",
                "    if len(sklearn_vocab) == len(feature_vocab):\n",
                "        for key, value in sklearn_vocab.items():\n",
                "            sklearn_vocab_id = value\n",
                "            feature_vocab_id = feature_vocab[key]\n",
                "            if int(feature_vocab_id) == sklearn_vocab_id:\n",
                "                continue\n",
                "            else:\n",
                "                return False\n",
                "    else:\n",
                "        return False\n",
                "    return True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "sentence_text_list = list(sent_to_id.keys())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "len(sentence_text_list)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "492739"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 33
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "sentence_text_list[:10]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['service was good .',\n",
                            " 'service was great .',\n",
                            " 'great service .',\n",
                            " 'the service was great .',\n",
                            " 'service was excellent .',\n",
                            " 'the food was good .',\n",
                            " 'great food .',\n",
                            " 'the service was good .',\n",
                            " 'friendly staff .',\n",
                            " 'the service was excellent .']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 34
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "cntvector, tfidf_weight = get_tfidf_embedding(sentence_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "df_count = get_df_score(sentence_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "df_count.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(498,)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 37
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "trainset_feature_df = dict()\n",
                "trainset_feature_df_norm = dict()\n",
                "for i in range(len(feature_word_list)):\n",
                "    trainset_feature_df[feature_word_list[i]] = df_count[i]\n",
                "    trainset_feature_df_norm[feature_word_list[i]] = df_count[i]/len(sentence_text_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "type(trainset_feature_df['wifi'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "numpy.int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 39
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "for key, value in trainset_feature_df.items():\n",
                "    if isinstance(value, np.int64):\n",
                "        trainset_feature_df[key] = int(value)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "type(trainset_feature_df['wifi'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "int"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 41
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "trainset_feat_df_file = '../Dataset/{}/train/feature/feature2df.json'.format(dataset_name)\n",
                "\n",
                "with open(trainset_feat_df_file, 'w') as f:\n",
                "    print(\"Write file: {}\".format(trainset_feat_df_file))\n",
                "    json.dump(trainset_feature_df, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/feature/feature2df.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "trainset_feature_df_sort = dict(sorted(trainset_feature_df.items(), key = lambda x: -x[1]))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "trainset_feature_df_sort_list = list(trainset_feature_df_sort.keys())\n",
                "trainset_feature_df_sort_rank = dict()\n",
                "for i in range(len(trainset_feature_df_sort_list)):\n",
                "    trainset_feature_df_sort_rank[trainset_feature_df_sort_list[i]] = i+1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "this_word = 'wifi'\n",
                "print(\"df value: {}\".format(trainset_feature_df[this_word]))\n",
                "print(\"rank of the feature: {}\".format(trainset_feature_df_sort_rank[this_word]))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "df value: 363\n",
                        "rank of the feature: 402\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "tfidf_weight.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(492739, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 46
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "check_vocab_is_same(cntvector.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 47
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "sentence_to_feature = dict()\n",
                "sentence_with_no_feature = 0\n",
                "tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
                "print(\"Shape of tf-idf sum: {}\".format(tfidf_sum_sents.shape))\n",
                "for i in range(len(sentence_text_list)):\n",
                "    cur_sent = sentence_text_list[i]\n",
                "    # if this sentence is in the sent_to_id vocabulary\n",
                "    assert cur_sent in sent_to_id\n",
                "    # get the sentence_id (str)\n",
                "    cur_sent_id = sent_to_id[cur_sent]\n",
                "    assert int(cur_sent_id) == i\n",
                "    # find all the feature that has non-zero tf-idf weight\n",
                "    feature_dict = dict()\n",
                "    for j in range(len(tfidf_weight[i])):\n",
                "        if tfidf_weight[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            feature_tfidf = tfidf_weight[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    if len(feature_dict) > 0:\n",
                "        sentence_to_feature[cur_sent_id] = feature_dict\n",
                "    else:\n",
                "        sentence_with_no_feature += 1\n",
                "    if (i+1) % 50000 == 0:\n",
                "        print(\"Processed {} lines\".format(i+1))\n",
                "print(\"Finish. Totally {} lines\".format(i+1))\n",
                "print(\"Totally {} sentences has at least 1 feature and {} sentences don't have feature.\".format(\n",
                "    len(sentence_to_feature), sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Shape of tf-idf sum: (492739,)\n",
                        "Processed 50000 lines\n",
                        "Processed 100000 lines\n",
                        "Processed 150000 lines\n",
                        "Processed 200000 lines\n",
                        "Processed 250000 lines\n",
                        "Processed 300000 lines\n",
                        "Processed 350000 lines\n",
                        "Processed 400000 lines\n",
                        "Processed 450000 lines\n",
                        "Finish. Totally 492739 lines\n",
                        "Totally 492739 sentences has at least 1 feature and 0 sentences don't have feature.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "sentence2feature_filepath = '../Dataset/{}/train/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(sentence_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/sentence/sentence2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "sentence_to_feature['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "{'1': 1.0}"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 50
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "id_to_sent['0']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service was good .'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "id2feature_dict['1']"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'service'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 53
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "num_feature_per_sentence = []\n",
                "for key, value in sentence_to_feature.items():\n",
                "    num_feature_per_sentence.append(len(value))\n",
                "    assert len(value) > 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "source": [
                "print(\"Mean number of features per sentence: {}\".format(np.mean(num_feature_per_sentence)))\n",
                "print(\"Max number of features per sentence: {}\".format(np.max(num_feature_per_sentence)))\n",
                "print(\"Min number of features per sentence: {}\".format(np.min(num_feature_per_sentence)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per sentence: 2.0819987863757485\n",
                        "Max number of features per sentence: 24\n",
                        "Min number of features per sentence: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Get User to Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "         item  user  rating                                             review\n",
                            "0        1071  1001       4  place was well lit , very clean , and staff ve...\n",
                            "1       10877  1001       3  the price was great and so was the subs ... th...\n",
                            "2        1114  1001       3  previous i was bundled into a takeout work ord...\n",
                            "3         120  1001       5  this is my favorite restaurant by far . food o...\n",
                            "4        1278  1001       4  nice little gem right here . ok , staff were v...\n",
                            "...       ...   ...     ...                                                ...\n",
                            "191222   4993  9999       5  my two favorites are the carne asada and barba...\n",
                            "191223    704  9999       4  the tacos themselves were a mixed bag . in par...\n",
                            "191224   7379  9999       5                             everything was great .\n",
                            "191225   8530  9999       4  our last experience here went pretty well and ...\n",
                            "191226   8682  9999       5  the surf and turf burrito ( carne asada and sh...\n",
                            "\n",
                            "[191227 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1071</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>place was well lit , very clean , and staff ve...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>10877</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the price was great and so was the subs ... th...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1114</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>3</td>\n",
                            "      <td>previous i was bundled into a takeout work ord...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>120</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this is my favorite restaurant by far . food o...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1278</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>nice little gem right here . ok , staff were v...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191222</th>\n",
                            "      <td>4993</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorites are the carne asada and barba...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191223</th>\n",
                            "      <td>704</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the tacos themselves were a mixed bag . in par...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191224</th>\n",
                            "      <td>7379</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>everything was great .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191225</th>\n",
                            "      <td>8530</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>our last experience here went pretty well and ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>191226</th>\n",
                            "      <td>8682</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the surf and turf burrito ( carne asada and sh...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>191227 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 56
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy User"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "group_by_user = df_train_data.groupby('user')\n",
                "user_id_list = []\n",
                "user_reviews = []\n",
                "# Loop over all user\n",
                "for user_df_chunk in list(group_by_user):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_df = user_df_chunk[1]\n",
                "    user_text = \" \".join(list(user_df['review']))\n",
                "    user_id_list.append(user_id)\n",
                "    user_reviews.append(user_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "source": [
                "print(\"Number of users: {}\".format(len(user_id_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users: 4604\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "source": [
                "assert len(user_id_list) == len(user_reviews)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute User Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "source": [
                "cntvector_user, tfidf_weight_user = get_tfidf_embedding(user_reviews, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "source": [
                "check_vocab_is_same(cntvector_user.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 61
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "source": [
                "tfidf_weight_user.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(4604, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 62
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "source": [
                "print(feature_word_list[:20])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['food', 'service', 'restaurant', 'staff', 'menu', 'chicken', 'everything', 'sauce', 'prices', 'bar', 'price', 'server', 'cheese', 'salad', 'flavor', 'fries', 'atmosphere', 'location', 'drinks', 'taste']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "source": [
                "user_to_feature = dict()\n",
                "for i in range(len(user_id_list)):\n",
                "    feature_dict = dict()\n",
                "    cur_user_id = user_id_list[i]\n",
                "    assert len(tfidf_weight_user[i]) == len(feature_vocab)\n",
                "    for j in range(len(tfidf_weight_user[i])):\n",
                "        if tfidf_weight_user[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            # NOTE: make sure that the feature_id is str format\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            assert feature_vocab[feature] == feature_id\n",
                "            feature_tfidf = tfidf_weight_user[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    assert len(feature_dict) > 0\n",
                "    user_to_feature[str(cur_user_id)] = feature_dict\n",
                "    if (i+1) % 1000 == 0:\n",
                "        print(\"{} user processed.\".format(i+1))\n",
                "print(\"Totally {} users\".format(i+1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed.\n",
                        "2000 user processed.\n",
                        "3000 user processed.\n",
                        "4000 user processed.\n",
                        "Totally 4604 users\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "source": [
                "len(user_to_feature)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4604"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 65
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "source": [
                "num_feature_per_user = []\n",
                "for key,value in user_to_feature.items():\n",
                "    num_feature_per_user.append(len(value))\n",
                "    assert len(value) > 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "source": [
                "print(\"Mean number of features per user: {}\".format(np.mean(num_feature_per_user)))\n",
                "print(\"Max number of features per user: {}\".format(np.max(num_feature_per_user)))\n",
                "print(\"Min number of features per user: {}\".format(np.min(num_feature_per_user)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per user: 94.76824500434405\n",
                        "Max number of features per user: 426\n",
                        "Min number of features per user: 15\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "source": [
                "len(user_to_feature['1001'])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "199"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 69
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save User to Feature Mapping into Json File"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "source": [
                "user2feature_filepath = '../Dataset/{}/train/user/user2feature.json'.format(dataset_name)\n",
                "with open(user2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(user2feature_filepath))\n",
                "    json.dump(user_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/user/user2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Get Item to Feature"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy Item"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "source": [
                "group_by_item = df_train_data.groupby('item')\n",
                "item_id_list = []\n",
                "item_reviews = []\n",
                "# Loop over all user\n",
                "for item_df_chunk in list(group_by_item):\n",
                "    item_id = str(item_df_chunk[0])\n",
                "    item_df = item_df_chunk[1]\n",
                "    item_text = \" \".join(list(item_df['review']))\n",
                "    item_id_list.append(item_id)\n",
                "    item_reviews.append(item_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "source": [
                "print(\"Number of items: {}\".format(len(item_id_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of items: 7837\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "source": [
                "assert len(item_id_list) == len(item_reviews)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute Item Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "source": [
                "cntvector_item, tfidf_weight_item = get_tfidf_embedding(item_reviews, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "source": [
                "check_vocab_is_same(cntvector_item.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 75
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "source": [
                "tfidf_weight_item.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(7837, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 76
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "source": [
                "item_to_feature = dict()\n",
                "for i in range(len(item_id_list)):\n",
                "    feature_dict = dict()\n",
                "    cur_item_id = item_id_list[i]\n",
                "    assert len(tfidf_weight_item[i]) == len(feature_vocab)\n",
                "    for j in range(len(tfidf_weight_item[i])):\n",
                "        if tfidf_weight_item[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            assert feature_id == feature_vocab[feature]\n",
                "            feature_tfidf = tfidf_weight_item[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    assert len(feature_dict) > 0\n",
                "    item_to_feature[cur_item_id] = feature_dict\n",
                "    if (i+1) % 1000 == 0:\n",
                "        print(\"{} items processed.\".format(i+1))\n",
                "print('Finish. Totally {} items'.format(i+1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 items processed.\n",
                        "2000 items processed.\n",
                        "3000 items processed.\n",
                        "4000 items processed.\n",
                        "5000 items processed.\n",
                        "6000 items processed.\n",
                        "7000 items processed.\n",
                        "Finish. Totally 7837 items\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "source": [
                "len(item_to_feature)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "7837"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 78
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "source": [
                "num_feature_per_item = []\n",
                "for key,value in item_to_feature.items():\n",
                "    num_feature_per_item.append(len(value))\n",
                "    assert len(value) > 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "print(\"Mean number of features per item: {}\".format(np.mean(num_feature_per_item)))\n",
                "print(\"Max number of features per item: {}\".format(np.max(num_feature_per_item)))\n",
                "print(\"Min number of features per item: {}\".format(np.min(num_feature_per_item)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per item: 86.95661605206074\n",
                        "Max number of features per item: 293\n",
                        "Min number of features per item: 8\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "source": [
                "item2feature_filepath = '../Dataset/{}/train/item/item2feature.json'.format(dataset_name)\n",
                "with open(item2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(item2feature_filepath))\n",
                "    json.dump(item_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train/item/item2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Compute Top User/Item Features"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "source": [
                "# TODO: Sanity Check"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}