{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
                "\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "# Create a Tokenizer with the default settings for English\n",
                "# including punctuation rules and exceptions\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset_name = \"yelp\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Read Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_review_filtered_clean.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 10000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/test_review_filtered_clean.json\n",
                        "10000 lines loaded.\n",
                        "20000 lines loaded.\n",
                        "30000 lines loaded.\n",
                        "40000 lines loaded.\n",
                        "Finish loading test dataset, totally 42702 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train_review_filtered.json\n",
                        "100000 lines loaded.\n",
                        "Finish loading train dataset, totally 191227 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item  user  rating                                             review\n",
                            "0      1098  1001       4  great food , great price , great atmosphere .....\n",
                            "1      1473  1001       4  the bbq pork also was way different this time ...\n",
                            "2       157  1001       4  but that dumb naan , or pita bread stuff was a...\n",
                            "3      1707  1001       4  _ price - average - please recognize fresh veg...\n",
                            "4      2911  1001       4  pizza was very good , fresh ingredients , , no...\n",
                            "...     ...   ...     ...                                                ...\n",
                            "42697  3933  9999       4  they do n't have a matinee price , but then ag...\n",
                            "42698  4154  9999       3  the main draw to this casinos over the others ...\n",
                            "42699  4565  9999       5  it 's not like normal stouts and the flavor is...\n",
                            "42700   624  9999       5  my two favorite meats for tacos are carne asad...\n",
                            "42701   758  9999       5  the chicken and dumplings is probably the best...\n",
                            "\n",
                            "[42702 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1098</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>great food , great price , great atmosphere .....</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1473</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the bbq pork also was way different this time ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>157</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>but that dumb naan , or pita bread stuff was a...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1707</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>_ price - average - please recognize fresh veg...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>2911</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>pizza was very good , fresh ingredients , , no...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42697</th>\n",
                            "      <td>3933</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>they do n't have a matinee price , but then ag...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42698</th>\n",
                            "      <td>4154</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the main draw to this casinos over the others ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42699</th>\n",
                            "      <td>4565</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>it 's not like normal stouts and the flavor is...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42700</th>\n",
                            "      <td>624</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorite meats for tacos are carne asad...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42701</th>\n",
                            "      <td>758</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the chicken and dumplings is probably the best...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>42702 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "train_groupby_item_user = df_train_data.groupby(['item', 'user'])\n",
                "train_groupby_item_user_dict = dict(tuple(train_groupby_item_user))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "for idx, row in df_test_data.iterrows():\n",
                "    user_id_str = row['user']\n",
                "    item_id_str = row['item']\n",
                "    assert (item_id_str, user_id_str) not in train_groupby_item_user_dict"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2ID and ID2Sentence Mapping From Training Set"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    trainset_sent_to_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    trainset_id_to_sent = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/id2sentence.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "assert len(trainset_sent_to_id) == len(trainset_id_to_sent)\n",
                "print(\"There are {} sentences in the training set.\".format(len(trainset_id_to_sent)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "There are 492739 sentences in the training set.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "# Feature words are the same between training and testing\n",
                "# since we can only know the review text from training set\n",
                "feature2id_filepath = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature2id_filepath))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "feature_word_list = list(feature_vocab.keys())\n",
                "assert len(feature_word_list) == len(feature_vocab)\n",
                "print('Number of feature words: {}'.format(len(feature_word_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of feature words: 498\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "id2feature_filepath = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(id2feature_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2feature_filepath))\n",
                "    id2feature_train = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Build Sentence Vocab on TestSet\n",
                "## Check Whether there are reviews with no sentence"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "invalid_data = 0\n",
                "for idx, row in df_test_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    if len(review_sents) == 0:\n",
                "        print(row)\n",
                "        invalid_data += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "print(invalid_data)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "def get_tf_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    return word_count.toarray()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "# sentence vocab\n",
                "sentence_count = dict()\n",
                "sentence_with_no_feature = 0\n",
                "# Loop for each review\n",
                "# TODO: Do we need to filter out sentences with less than 3 tokens same as during training?\n",
                "for idx, row in df_test_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
                "    tf_sum_sents = np.sum(tf_score, axis=1)\n",
                "    for i in range(len(review_sents)):\n",
                "        if tf_sum_sents[i] != 0.0:\n",
                "            cur_sent = review_sents[i]\n",
                "            sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
                "        else:\n",
                "            sentence_with_no_feature += 1\n",
                "    if (idx+1) % 2000 == 0:\n",
                "        print(\"Processed {} lines\".format(idx+1))\n",
                "print(\"Totally {} tracked sentences\".format(len(sentence_count)))\n",
                "print(\"There are {} sentences with no feature words\".format(sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 2000 lines\n",
                        "Processed 4000 lines\n",
                        "Processed 6000 lines\n",
                        "Processed 8000 lines\n",
                        "Processed 10000 lines\n",
                        "Processed 12000 lines\n",
                        "Processed 14000 lines\n",
                        "Processed 16000 lines\n",
                        "Processed 18000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 22000 lines\n",
                        "Processed 24000 lines\n",
                        "Processed 26000 lines\n",
                        "Processed 28000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 32000 lines\n",
                        "Processed 34000 lines\n",
                        "Processed 36000 lines\n",
                        "Processed 38000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 42000 lines\n",
                        "Totally 109833 tracked sentences\n",
                        "There are 897 sentences with no feature words\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "len(sentence_count)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "109833"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 20
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "# sort sentence based on counts (the majority should be 1)\n",
                "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "# sentence_vocab_list = list(sentence_count.keys())\n",
                "# Building mappings from sentences to ids and ids to sentences\n",
                "testset_sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
                "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
                "testset_id_to_sent = {str(id): sent for (sent, id) in testset_sent_to_id.items()}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save Sentence2ID into Json File (Test / Valid Set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "with open('../Dataset/{}/test/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_id_to_sent, f)\n",
                "\n",
                "with open('../Dataset/{}/test/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_sent_to_id, f)\n",
                "\n",
                "with open('../Dataset/{}/valid/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_id_to_sent, f)\n",
                "\n",
                "with open('../Dataset/{}/valid/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_sent_to_id, f)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Get Sentence Feature (Test / Valid Set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item  user  rating                                             review\n",
                            "0      1098  1001       4  great food , great price , great atmosphere .....\n",
                            "1      1473  1001       4  the bbq pork also was way different this time ...\n",
                            "2       157  1001       4  but that dumb naan , or pita bread stuff was a...\n",
                            "3      1707  1001       4  _ price - average - please recognize fresh veg...\n",
                            "4      2911  1001       4  pizza was very good , fresh ingredients , , no...\n",
                            "...     ...   ...     ...                                                ...\n",
                            "42697  3933  9999       4  they do n't have a matinee price , but then ag...\n",
                            "42698  4154  9999       3  the main draw to this casinos over the others ...\n",
                            "42699  4565  9999       5  it 's not like normal stouts and the flavor is...\n",
                            "42700   624  9999       5  my two favorite meats for tacos are carne asad...\n",
                            "42701   758  9999       5  the chicken and dumplings is probably the best...\n",
                            "\n",
                            "[42702 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1098</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>great food , great price , great atmosphere .....</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1473</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the bbq pork also was way different this time ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>157</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>but that dumb naan , or pita bread stuff was a...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1707</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>_ price - average - please recognize fresh veg...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>2911</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>pizza was very good , fresh ingredients , , no...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42697</th>\n",
                            "      <td>3933</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>they do n't have a matinee price , but then ag...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42698</th>\n",
                            "      <td>4154</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the main draw to this casinos over the others ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42699</th>\n",
                            "      <td>4565</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>it 's not like normal stouts and the flavor is...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42700</th>\n",
                            "      <td>624</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorite meats for tacos are carne asad...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42701</th>\n",
                            "      <td>758</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the chicken and dumplings is probably the best...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>42702 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 24
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "print(\"Number of users on test set: {}\".format(len(df_test_data['user'].unique())))\n",
                "print(\"Number of items on test set: {}\".format(len(df_test_data['item'].unique())))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on test set: 4604\n",
                        "Number of items on test set: 7602\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "# groupby item\n",
                "group_by_item_test = df_test_data.groupby('item')\n",
                "group_by_item_dict = dict(tuple(group_by_item_test))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "assert len(group_by_item_dict) == len(df_test_data['item'].unique())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "peritem_num_sent_testset = dict()\n",
                "peritemreview_num_sent_testset = list()\n",
                "for key, item_df_test in group_by_item_dict.items():\n",
                "    # print(key)\n",
                "    # print(item_df_test)\n",
                "    reviews_list = item_df_test['review']\n",
                "    sentence_count = 0\n",
                "    for review in reviews_list:\n",
                "        review_sent_count = 0\n",
                "        sentences_review = sent_tokenize(review)\n",
                "        for sent in sentences_review:\n",
                "            if sent in testset_sent_to_id:\n",
                "                sentence_count += 1\n",
                "                review_sent_count += 1\n",
                "        peritemreview_num_sent_testset.append(review_sent_count)\n",
                "    peritem_num_sent_testset[key] = sentence_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "assert len(peritemreview_num_sent_testset) == len(df_test_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "print(\"Number of review in testset: {}\".format(\n",
                "    len(peritemreview_num_sent_testset)))\n",
                "print(\"Mean number of sentence per review in testset: {}\".format(\n",
                "    np.mean(peritemreview_num_sent_testset)))\n",
                "print(\"Min number of sentence per review in testset {}\".format(\n",
                "    np.min(peritemreview_num_sent_testset)))\n",
                "print(\"Max number of sentence per review in testset {}\".format(\n",
                "    np.max(peritemreview_num_sent_testset)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of review in testset: 42702\n",
                        "Mean number of sentence per review in testset: 2.6423118355112174\n",
                        "Min number of sentence per review in testset 1\n",
                        "Max number of sentence per review in testset 20\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "print(\"Number of items in testset: {}\".format(\n",
                "    len(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Mean number of sentence per item in testset: {}\".format(\n",
                "    np.mean(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Min number of sentence per item in testset: {}\".format(\n",
                "    np.min(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Max number of sentence per item in testset: {}\".format(\n",
                "    np.max(list(peritem_num_sent_testset.values()))\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of items in testset: 7602\n",
                        "Mean number of sentence per item in testset: 14.842409892133649\n",
                        "Min number of sentence per item in testset: 1\n",
                        "Max number of sentence per item in testset: 157\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "# groupby user\n",
                "group_by_user_test = df_test_data.groupby('user')\n",
                "group_by_user_dict = dict(tuple(group_by_user_test))\n",
                "assert len(group_by_user_dict) == len(df_test_data['user'].unique())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "peruser_num_sent_testset = dict()\n",
                "peruserreview_num_sent_testset = list()\n",
                "for key, user_df_test in group_by_user_dict.items():\n",
                "    reviews_list = user_df_test['review']\n",
                "    sentence_count = 0\n",
                "    for review in reviews_list:\n",
                "        review_sent_count = 0\n",
                "        sentences_review = sent_tokenize(review)\n",
                "        for sent in sentences_review:\n",
                "            if sent in testset_sent_to_id:\n",
                "                sentence_count += 1\n",
                "                review_sent_count += 1\n",
                "        peruserreview_num_sent_testset.append(review_sent_count)\n",
                "    peruser_num_sent_testset[key] = sentence_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "assert len(peruserreview_num_sent_testset) == len(df_test_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "print(\"Number of review in testset: {}\".format(\n",
                "    len(peruserreview_num_sent_testset)))\n",
                "print(\"Mean number of sentence per review in testset: {}\".format(\n",
                "    np.mean(peruserreview_num_sent_testset)))\n",
                "print(\"Min number of sentence per review in testset {}\".format(\n",
                "    np.min(peruserreview_num_sent_testset)))\n",
                "print(\"Max number of sentence per review in testset {}\".format(\n",
                "    np.max(peruserreview_num_sent_testset)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of review in testset: 42702\n",
                        "Mean number of sentence per review in testset: 2.6423118355112174\n",
                        "Min number of sentence per review in testset 1\n",
                        "Max number of sentence per review in testset 20\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "print(\"Number of users in testset: {}\".format(\n",
                "    len(list(peruser_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Mean number of sentence per user in testset: {}\".format(\n",
                "    np.mean(list(peruser_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Min number of sentence per user in testset: {}\".format(\n",
                "    np.min(list(peruser_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Max number of sentence per user in testset: {}\".format(\n",
                "    np.max(list(peruser_num_sent_testset.values()))\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users in testset: 4604\n",
                        "Mean number of sentence per user in testset: 24.507384882710685\n",
                        "Min number of sentence per user in testset: 1\n",
                        "Max number of sentence per user in testset: 202\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "# get the list of sentence text on testset\n",
                "testset_sent_text_list = list(testset_sent_to_id.keys())\n",
                "testset_sent_text_list[:20]\n",
                "# NOTE: Based on the examples, \\\n",
                "# should we set the short sentence threshold to be 3 instead of 2 on train?\n",
                "# Currently the threshold is 2 ."
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['service was good .',\n",
                            " 'great service .',\n",
                            " 'the service was great .',\n",
                            " 'service was great .',\n",
                            " 'great food .',\n",
                            " 'friendly staff .',\n",
                            " 'the service was excellent .',\n",
                            " 'good service .',\n",
                            " 'the food was good .',\n",
                            " 'service was excellent .',\n",
                            " 'good food .',\n",
                            " 'the food was great .',\n",
                            " 'food was good .',\n",
                            " 'friendly service .',\n",
                            " 'prices are reasonable .',\n",
                            " 'excellent service .',\n",
                            " 'everything was delicious .',\n",
                            " 'great service !',\n",
                            " 'the food was ok .',\n",
                            " 'the service was good .']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 37
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "def get_tfidf_embedding(text, feature_word_list):\n",
                "    \"\"\"\n",
                "    :param: text: list, sent_number * word\n",
                "    :return: \n",
                "        vectorizer: \n",
                "            vocabulary_: word2id\n",
                "            get_feature_names(): id2word\n",
                "        tfidf: array [sent_number, max_word_number]\n",
                "    \"\"\"\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    tfidf_transformer = TfidfTransformer()\n",
                "    tfidf = tfidf_transformer.fit_transform(word_count)\n",
                "    tfidf_weight = tfidf.toarray()\n",
                "    return vectorizer, tfidf_weight"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "cntvector, tfidf_weight = get_tfidf_embedding(testset_sent_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "tfidf_weight.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(109833, 498)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 40
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "def check_vocab_is_same(sklearn_vocab, feature_vocab):\n",
                "    if len(sklearn_vocab) == len(feature_vocab):\n",
                "        for key, value in sklearn_vocab.items():\n",
                "            sklearn_vocab_id = value\n",
                "            feature_vocab_id = feature_vocab[key]\n",
                "            if int(feature_vocab_id) == sklearn_vocab_id:\n",
                "                continue\n",
                "            else:\n",
                "                return False\n",
                "    else:\n",
                "        return False\n",
                "    return True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "check_vocab_is_same(cntvector.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 42
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "testset_sentence_to_feature = dict()\n",
                "sentence_with_no_feature = 0\n",
                "tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
                "for i in range(len(testset_sent_text_list)):\n",
                "    cur_sent = testset_sent_text_list[i]\n",
                "    # if this sentence is in the sent_to_id vocabulary\n",
                "    assert cur_sent in testset_sent_to_id\n",
                "    # get the sentence_id (str)\n",
                "    cur_sent_id = testset_sent_to_id[cur_sent]\n",
                "    assert int(cur_sent_id) == i\n",
                "    # find all the feature that has non-zero tf-idf weight\n",
                "    feature_dict = dict()\n",
                "    for j in range(len(tfidf_weight[i])):\n",
                "        if tfidf_weight[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            feature_tfidf = tfidf_weight[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    if len(feature_dict) > 0:\n",
                "        testset_sentence_to_feature[cur_sent_id] = feature_dict\n",
                "    else:\n",
                "        sentence_with_no_feature += 1\n",
                "    if (i+1) % 10000 == 0:\n",
                "        print(\"Processed {} lines\".format(i+1))\n",
                "print(\"Finish. Totally {} lines\".format(i+1))\n",
                "print(\"Totally {} sentences has at least 1 feature and {} sentences don't have feature.\".format(\n",
                "    len(testset_sentence_to_feature), sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 10000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 50000 lines\n",
                        "Processed 60000 lines\n",
                        "Processed 70000 lines\n",
                        "Processed 80000 lines\n",
                        "Processed 90000 lines\n",
                        "Processed 100000 lines\n",
                        "Finish. Totally 109833 lines\n",
                        "Totally 109833 sentences has at least 1 feature and 0 sentences don't have feature.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "# save testset_sentence_to_feature into json file\n",
                "# testset sent_to_id is same as validset, also save this to validset\n",
                "sentence2feature_filepath = '../Dataset/{}/test/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(testset_sentence_to_feature, f)\n",
                "sentence2feature_filepath = '../Dataset/{}/valid/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(testset_sentence_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/test/sentence/sentence2feature.json\n",
                        "Write file: ../Dataset/yelp/valid/sentence/sentence2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "print(testset_sentence_to_feature['0'])\n",
                "print(testset_id_to_sent['0'])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'1': 1.0}\n",
                        "service was good .\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "print(testset_sentence_to_feature['12310'])\n",
                "print(testset_id_to_sent['12310'])\n",
                "for fea_id in testset_sentence_to_feature['12310'].keys():\n",
                "    print(id2feature_train[fea_id])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'12': 0.28007497234632434, '15': 0.5606947392266455, '163': 0.7792171836328879}\n",
                        "i tried the chili cheese fries and to my surprise the fries were crisp yet when you go to town on the chili , softness !\n",
                        "cheese\n",
                        "fries\n",
                        "chili\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "# get some statistics of sentence_to_feature on testset/validset\n",
                "num_feature_per_sentence = []\n",
                "for key, value in testset_sentence_to_feature.items():\n",
                "    num_feature_per_sentence.append(len(value))\n",
                "    assert len(value) > 0       # every sentence should have at least 1 feature"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "print(\"Mean number of features per sentence: {}\".format(np.mean(num_feature_per_sentence)))\n",
                "print(\"Max number of features per sentence: {}\".format(np.max(num_feature_per_sentence)))\n",
                "print(\"Min number of features per sentence: {}\".format(np.min(num_feature_per_sentence)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per sentence: 2.0614114155126417\n",
                        "Max number of features per sentence: 28\n",
                        "Min number of features per sentence: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load User to SentenceID"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "train_user2sentids_filepath = '../Dataset/{}/train/user/user2sentids.json'.format(dataset_name)\n",
                "with open(train_user2sentids_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_user2sentids_filepath))\n",
                "    trainset_user_to_sent_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/user/user2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Item to SentenceID"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "train_item2sentids_filepath = '../Dataset/{}/train/item/item2sentids.json'.format(dataset_name)\n",
                "with open(train_item2sentids_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_item2sentids_filepath))\n",
                "    trainset_item_to_sent_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/item/item2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load User-Item Pairs on TrainSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "train_useritem_pairs_filepath = '../Dataset/{}/train/useritem_pairs.json'.format(dataset_name)\n",
                "with open(train_useritem_pairs_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_useritem_pairs_filepath))\n",
                "    trainset_useritem_pairs = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/useritem_pairs.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "# Get the user set and item set on train-set\n",
                "train_user_set = set()\n",
                "train_item_set = set()\n",
                "for key,value in trainset_useritem_pairs.items():\n",
                "    uid = key\n",
                "    assert uid not in train_user_set\n",
                "    train_user_set.add(uid)\n",
                "    for iid in value:\n",
                "        train_item_set.add(iid)\n",
                "print(\"Number of users on the constructed train set: {}\".format(len(train_user_set)))\n",
                "print(\"Number of items on the constructed train set: {}\".format(len(train_item_set)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on the constructed train set: 4604\n",
                        "Number of items on the constructed train set: 7837\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# For Each Data Instance on TestSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy User"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "group_by_user_test = df_test_data.groupby('user')\n",
                "print(\"Number of users on test-set: {}\".format(len(group_by_user_test)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on test-set: 4604\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Construct Valid Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "import random\n",
                "sample_sent_num = 500           # this should be among 30, 200 and 500\n",
                "user_item_candidate_sent_ids_validset = dict()\n",
                "cnt_empty_true_sentence = 0\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "user_item_candidate_sentence_num = list()\n",
                "user_item_candidate_sentence_num_sampled = list()\n",
                "cnt_being_cut_useritem = 0\n",
                "# Loop over all users\n",
                "user_cnt = 0\n",
                "for user_df_chunk in list(group_by_user_test):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    user_df = user_df_chunk[1]\n",
                "    if user_id_str not in train_user_set:\n",
                "        continue\n",
                "    # get user sentences, these sentences are on TRAIN set\n",
                "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        if item_id_str not in train_item_set:\n",
                "            continue\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sentences, they are on TRAIN set\n",
                "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
                "        # get review_text's sent ids, they are on TEST set\n",
                "        cur_review_sent_ids = set()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the testset_sent_to_id dict\n",
                "        for sent in review_sents:\n",
                "            if sent in testset_sent_to_id:\n",
                "                cur_sent_id = testset_sent_to_id[sent]\n",
                "                # add this sentence into the set of current review\n",
                "                cur_review_sent_ids.add(cur_sent_id)\n",
                "        # construct the candidate set which is an union of user sentence and item sentence\n",
                "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "        # sample some sentences (they are on TRAIN set)\n",
                "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            cnt_being_cut_useritem += 1\n",
                "        else:\n",
                "            # FIXED!!\n",
                "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "        # add this into the dict\n",
                "        if len(cur_review_sent_ids) != 0:\n",
                "            # only add the ones that contain at least 1 true label sentence (on valid/test set)\n",
                "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), list(cur_review_sent_ids)]\n",
                "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
                "            user_item_candidate_sentence_num_sampled.append(len(sample_useritem_sent_ids))\n",
                "        else:\n",
                "            cnt_empty_true_sentence += 1\n",
                "\n",
                "    # add this item-level dict into the user-level dict\n",
                "    if len(item_candidate_sent_ids) == 0:\n",
                "        print(\"User: {} has no useful items, skip it.\".format(user_id_str))\n",
                "    else:\n",
                "        user_item_candidate_sent_ids_validset[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 1000 == 0:\n",
                "        print(\"{} user processed.\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {} users'.format(user_cnt))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, cnt_empty_true_sentence))\n",
                "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed.\n",
                        "2000 user processed.\n",
                        "3000 user processed.\n",
                        "4000 user processed.\n",
                        "Finish.\n",
                        "Totally 4604 users\n",
                        "Totally 42702 reviews. Among them 0 reviews has empty true label sentence\n",
                        "During constructing, 3237 user-item pair are being cutted due to their length\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "source": [
                "print(\"Totally {} user item pairs in the testset\".format(len(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"mean number of candidate sentence: {}\".format(np.median(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"max number of candidate sentence: {}\".format(np.max(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"min number of candidate sentence: {}\".format(np.min(user_item_candidate_sentence_num_sampled)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 42702 user item pairs in the testset\n",
                        "mean number of candidate sentence: 186.0\n",
                        "max number of candidate sentence: 500\n",
                        "min number of candidate sentence: 35\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "source": [
                "print(sorted(user_item_candidate_sentence_num)[-200:-100])\n",
                "\n",
                "\"\"\" This shows that if we restrict the candidate sentences to have a maximum number of 1200,\n",
                "we will cut-off about 1000 reviews. This will be applied on test set to avoid user-item paris\n",
                "with too many candidate sentences.\n",
                "\"\"\""
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1087, 1088, 1088, 1089, 1089, 1091, 1092, 1092, 1092, 1093, 1094, 1097, 1100, 1100, 1100, 1101, 1101, 1102, 1103, 1103, 1103, 1105, 1106, 1106, 1108, 1108, 1112, 1112, 1115, 1117, 1117, 1119, 1119, 1119, 1122, 1124, 1124, 1129, 1130, 1131, 1134, 1136, 1140, 1141, 1142, 1144, 1148, 1154, 1156, 1156, 1158, 1162, 1163, 1165, 1166, 1167, 1167, 1168, 1168, 1169, 1172, 1174, 1174, 1175, 1177, 1178, 1181, 1183, 1195, 1208, 1219, 1225, 1226, 1242, 1246, 1253, 1265, 1270, 1277, 1303, 1312, 1326, 1335, 1336, 1345, 1356, 1359, 1363, 1368, 1372, 1372, 1376, 1383, 1387, 1388, 1392, 1414, 1415, 1416, 1432]\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "' This shows that if we restrict the candidate sentences to have a maximum number of 1200,\\nwe will cut-off about 1000 reviews. This will be applied on test set to avoid user-item paris\\nwith too many candidate sentences.\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 60
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "source": [
                "len(user_item_candidate_sent_ids_validset)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4604"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 61
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "source": [
                "# save this into json file\n",
                "valid_useritem2sentids_filepath = '../Dataset/{}/valid/useritem2sentids_test.json'.format(dataset_name)\n",
                "with open(valid_useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(valid_useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids_validset, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/valid/useritem2sentids_test.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "source": [
                "check_user_id = \"9999\"\n",
                "check_item_id = \"4154\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 9999 \t item: 4154\n",
                        "number of sentence in candidate set: 116\n",
                        "number of sentence in true review set: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "source": [
                "for sentid in user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1]:\n",
                "    print(testset_id_to_sent[sentid])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "the main draw to this casinos over the others nearby is their restaurant venues .\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "source": [
                "# Checking How Many User/Item/Review are in the valid set\n",
                "cnt_user = 0\n",
                "cnt_review = 0\n",
                "cnt_item_set = set()\n",
                "for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
                "    user_id_str = str(trainset_user_chunk[0])\n",
                "    user_id = int(trainset_user_chunk[0])\n",
                "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "    for item_chunk in user_item_chunks:\n",
                "        item_id_str = str(item_chunk[0])\n",
                "        item_id = int(item_chunk[0])\n",
                "        # candidate_true_sent_ids = item_chunk[1]\n",
                "        # cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'sent_id': candidate_true_sent_ids}\n",
                "        # write this into the json file\n",
                "        # json.dump(cur_data_dict, f1)\n",
                "        # f1.write(\"\\n\")\n",
                "        # assert user_id_str in train_user_id_set\n",
                "        # assert item_id_str in train_item_id_set\n",
                "        cnt_item_set.add(item_id_str)\n",
                "        cnt_review += 1\n",
                "    cnt_user += 1\n",
                "\n",
                "print(\"Total number of reviews: {}\".format(cnt_review))\n",
                "print(\"Total number of user: {}\".format(cnt_user))\n",
                "print(\"Total number of item: {}\".format(len(cnt_item_set)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of reviews: 42702\n",
                        "Total number of user: 4604\n",
                        "Total number of item: 7602\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "source": [
                "# Write useritem2sentids_test into a line-by-line format\n",
                "valid_useritem2sentids_multiline_filepath = '../Dataset/{}/valid/useritem2sentids_test_multilines.json'.format(dataset_name)\n",
                "with open(valid_useritem2sentids_multiline_filepath, 'w') as f1:\n",
                "    print(\"Write file: {}\".format(valid_useritem2sentids_multiline_filepath))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
                "        user_id_str = str(trainset_user_chunk[0])\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            item_id_str = str(item_chunk[0])\n",
                "            item_id = int(item_chunk[0])\n",
                "            item_set.add(item_id_str)\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            true_revw_sent_ids = item_chunk[1][1]\n",
                "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert len(user_set) == cnt_user\n",
                "assert len(useritem_set) == cnt_review\n",
                "print(\"Total {} users\".format(cnt_user))\n",
                "print(\"Total {} items\".format(len(item_set)))\n",
                "print(\"Totat {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/valid/useritem2sentids_test_multilines.json\n",
                        "Total 4604 users\n",
                        "Total 7602 items\n",
                        "Totat 42702 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct Test Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "source": [
                "sample_sent_num = 1200\n",
                "user_item_candidate_sent_ids_testset = dict()\n",
                "cnt_empty_true_sentence = 0\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "user_item_candidate_sentence_num = list()\n",
                "user_item_candidate_sentence_num_sampled = list()\n",
                "cnt_being_cut_useritem = 0\n",
                "# Loop over all users\n",
                "user_cnt = 0\n",
                "for user_df_chunk in list(group_by_user_test):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    if user_id_str not in train_user_set:\n",
                "        continue\n",
                "    user_df = user_df_chunk[1]\n",
                "    # get user sentences, these sentences are on TRAIN set\n",
                "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        if item_id_str not in train_item_set:\n",
                "            continue\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sentences, they are on TRAIN set\n",
                "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
                "        # get review_text's sent ids, they are on TEST set\n",
                "        cur_review_sent_ids = set()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the testset_sent_to_id dict\n",
                "        for sent in review_sents:\n",
                "            if sent in testset_sent_to_id:\n",
                "                cur_sent_id = testset_sent_to_id[sent]\n",
                "                # add this sentence into the set of current review\n",
                "                cur_review_sent_ids.add(cur_sent_id)\n",
                "        # set union\n",
                "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "        # sample some sentences (they are on TRAIN set)\n",
                "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            cnt_being_cut_useritem += 1\n",
                "        else:\n",
                "            # FIXED!!\n",
                "            # sample_useritem_sent_ids = cur_user_sent_ids\n",
                "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "        # add this into the dict\n",
                "        if len(cur_review_sent_ids) != 0:\n",
                "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), list(cur_review_sent_ids)]\n",
                "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
                "            user_item_candidate_sentence_num_sampled.append(len(sample_useritem_sent_ids))\n",
                "        else:\n",
                "            cnt_empty_true_sentence += 1\n",
                "\n",
                "    # add this item-level dict into the user-level dict\n",
                "    user_item_candidate_sent_ids_testset[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 1000 == 0:\n",
                "        print(\"{} user processed.\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {} users'.format(user_cnt))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, cnt_empty_true_sentence))\n",
                "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed.\n",
                        "2000 user processed.\n",
                        "3000 user processed.\n",
                        "4000 user processed.\n",
                        "Finish.\n",
                        "Totally 4604 users\n",
                        "Totally 42702 reviews. Among them 0 reviews has empty true label sentence\n",
                        "During constructing, 131 user-item pair are being cutted due to their length\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "source": [
                "print(\"Totally {} user item pairs in the testset\".format(\n",
                "    len(user_item_candidate_sentence_num)))\n",
                "print(\"mean number of candidate sentence: {}\".format(\n",
                "    np.mean(user_item_candidate_sentence_num)))\n",
                "print(\"max number of candidate sentence: {}\".format(\n",
                "    np.max(user_item_candidate_sentence_num)))\n",
                "print(\"min number of candidate sentence: {}\".format(\n",
                "    np.min(user_item_candidate_sentence_num)))\n",
                "print(\"mean number of sampled candidate sentence: {}\".format(\n",
                "    np.mean(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"max number of sampled candidate sentence: {}\".format(\n",
                "    np.max(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"min number of sampled candidate sentence: {}\".format(\n",
                "    np.min(user_item_candidate_sentence_num_sampled)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 42702 user item pairs in the testset\n",
                        "mean number of candidate sentence: 239.02236429207062\n",
                        "max number of candidate sentence: 2211\n",
                        "min number of candidate sentence: 35\n",
                        "mean number of sampled candidate sentence: 237.80698796309306\n",
                        "max number of sampled candidate sentence: 1200\n",
                        "min number of sampled candidate sentence: 35\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "source": [
                "print(sorted(user_item_candidate_sentence_num)[-40:])\n",
                "print(sorted(user_item_candidate_sentence_num_sampled)[-40:])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1768, 1772, 1781, 1790, 1791, 1792, 1795, 1798, 1801, 1808, 1814, 1815, 1815, 1818, 1819, 1820, 1821, 1826, 1831, 1832, 1833, 1851, 1852, 1857, 1874, 1880, 1889, 1892, 1897, 1910, 1914, 1914, 1923, 1928, 1955, 1963, 2017, 2153, 2176, 2211]\n",
                        "[1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "source": [
                "len(user_item_candidate_sent_ids_testset)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4604"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 73
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "source": [
                "# save this into json file\n",
                "test_useritem2sentids_filepath = '../Dataset/{}/test/useritem2sentids_test.json'.format(dataset_name)\n",
                "with open(test_useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(test_useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids_testset, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/test/useritem2sentids_test.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "source": [
                "review_test_cnt = 0\n",
                "for user_chunk in user_item_candidate_sent_ids_testset.items():\n",
                "    user_id = user_chunk[0]\n",
                "    user_dict = user_chunk[1]\n",
                "    for user_item_chunk in user_dict.items():\n",
                "        item_id = user_item_chunk[0]\n",
                "        candidate_sents = user_item_chunk[0]\n",
                "        true_label_sents = user_item_chunk[1]\n",
                "        review_test_cnt += 1\n",
                "print(review_test_cnt)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "42702\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "source": [
                "check_user_id = \"9999\"\n",
                "check_item_id = \"4154\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 9999 \t item: 4154\n",
                        "number of sentence in candidate set: 116\n",
                        "number of sentence in true review set: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "source": [
                "# Write useritem2sentids_test into a line-by-line format\n",
                "test_useritem2sentids_multiline_filepath = '../Dataset/{}/test/useritem2sentids_test_multilines.json'.format(dataset_name)\n",
                "with open(test_useritem2sentids_multiline_filepath, 'w') as f1:\n",
                "    print(\"Write file: {}\".format(test_useritem2sentids_multiline_filepath))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids_testset.items()):\n",
                "        user_id_str = str(trainset_user_chunk[0])\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            item_id_str = str(item_chunk[0])\n",
                "            item_id = int(item_chunk[0])\n",
                "            item_set.add(item_id_str)\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            true_revw_sent_ids = item_chunk[1][1]\n",
                "            cur_data_dict = {\n",
                "                'user_id': user_id,\n",
                "                'item_id': item_id,\n",
                "                'candidate': candidate_sent_ids,\n",
                "                'review': true_revw_sent_ids\n",
                "            }\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert len(user_set) == cnt_user\n",
                "assert len(useritem_set) == cnt_review\n",
                "print(\"Total {} users\".format(cnt_user))\n",
                "print(\"Total {} items\".format(len(item_set)))\n",
                "print(\"Totat {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/test/useritem2sentids_test_multilines.json\n",
                        "Total 4604 users\n",
                        "Total 7602 items\n",
                        "Totat 42702 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "source": [
                "check_user_id = \"30\"\n",
                "check_item_id = \"354\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"[VALID] number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"[VALID] number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))\n",
                "print(\"[TEST]  number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"[TEST]  number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 30 \t item: 354\n",
                        "[VALID] number of sentence in candidate set: 500\n",
                        "[VALID] number of sentence in true review set: 2\n",
                        "[TEST]  number of sentence in candidate set: 1078\n",
                        "[TEST]  number of sentence in true review set: 2\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}