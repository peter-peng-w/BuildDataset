{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dataset_name = \"tripadvisor\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Read Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_review_filtered_clean.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train_review_filtered.json\n",
                        "100000 lines loaded.\n",
                        "200000 lines loaded.\n",
                        "Finish loading train dataset, totally 205595 lines.\n",
                        "Load file: ../Dataset/tripadvisor/test_review_filtered_clean.json\n",
                        "Finish loading test dataset, totally 19459 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# convert to pandas dataframe\n",
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "df_train_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "        item user  rating                                             review\n",
                            "0          0    0       5  having done some research , i requested a room...\n",
                            "1          1    0       1                         will not stay here again .\n",
                            "2         10    0       2  the hotel reception was very willing to try an...\n",
                            "3        100    0       3  this was older but clean . the breakfast was t...\n",
                            "4       1000    0       5  the food was exceptional - we really enjoyed t...\n",
                            "...      ...  ...     ...                                                ...\n",
                            "205590   752  999       5  the rooms are spacious , quiet , and clean . m...\n",
                            "205591   819  999       5  room was very nice . bed was comfortable , had...\n",
                            "205592   827  999       3  not really the best stay i ever had . room was...\n",
                            "205593   852  999       3  our room was not as nice as i had hoped . the ...\n",
                            "205594   898  999       5  villas are very confortable , spacy and clean ...\n",
                            "\n",
                            "[205595 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>having done some research , i requested a room...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>will not stay here again .</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>10</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>the hotel reception was very willing to try an...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>100</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>this was older but clean . the breakfast was t...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the food was exceptional - we really enjoyed t...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>205590</th>\n",
                            "      <td>752</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the rooms are spacious , quiet , and clean . m...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>205591</th>\n",
                            "      <td>819</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>room was very nice . bed was comfortable , had...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>205592</th>\n",
                            "      <td>827</td>\n",
                            "      <td>999</td>\n",
                            "      <td>3</td>\n",
                            "      <td>not really the best stay i ever had . room was...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>205593</th>\n",
                            "      <td>852</td>\n",
                            "      <td>999</td>\n",
                            "      <td>3</td>\n",
                            "      <td>our room was not as nice as i had hoped . the ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>205594</th>\n",
                            "      <td>898</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>villas are very confortable , spacy and clean ...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>205595 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item user  rating                                             review\n",
                            "0      1111    0       2  when i mentioned this to the front desk they d...\n",
                            "1      1379    0       2  i guess for a highway stop this motel is fine ...\n",
                            "2      1391    0       5  we stayed at the signature for four days to ce...\n",
                            "3      1579    0       4  the lake buena vista is a perfect place to sta...\n",
                            "4      1689    0       5  summer ( at the front desk ) was perfect ! she...\n",
                            "...     ...  ...     ...                                                ...\n",
                            "19454     0  999       5  this was a pleasant place , and with our annua...\n",
                            "19455   128  999       5  we enjoyed our stay at the hilton very much ! ...\n",
                            "19456   429  999       5  from the moment we arrived at the front desk u...\n",
                            "19457   816  999       4  wifi gratuit , nous n avons pas essayé le brea...\n",
                            "19458   916  999       4  hot - tub was perfect for relaxing after a lon...\n",
                            "\n",
                            "[19459 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1111</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>when i mentioned this to the front desk they d...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1379</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>i guess for a highway stop this motel is fine ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1391</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we stayed at the signature for four days to ce...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1579</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the lake buena vista is a perfect place to sta...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1689</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>summer ( at the front desk ) was perfect ! she...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19454</th>\n",
                            "      <td>0</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this was a pleasant place , and with our annua...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19455</th>\n",
                            "      <td>128</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we enjoyed our stay at the hilton very much ! ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19456</th>\n",
                            "      <td>429</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>from the moment we arrived at the front desk u...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19457</th>\n",
                            "      <td>816</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>wifi gratuit , nous n avons pas essayé le brea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19458</th>\n",
                            "      <td>916</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>hot - tub was perfect for relaxing after a lon...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>19459 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "len(df_train_data['rating'].unique())"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "5"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "print(sorted(list(df_train_data['rating'].unique())))\n",
                "# from 50 to 100, without 54"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1, 2, 3, 4, 5]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "list(df_train_data['review'])[100:105]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['personnel and service were outstanding . wonderful western décor on beautiful grounds . a short walk to center of town , but if you prefer a shuttle service is available .',\n",
                            " 'overall , the stay was fine . our bathroom sink faucet dripped and they charge $ 10 / day for high - speed internet . i was able to use a dial - up service for $ 1 per connection .',\n",
                            " 'great location too ! super friendly staff that really went out of their way . i think his name was gordon . in any event , this is my first choice in dc .',\n",
                            " \"the room was basically clean and toiletries replenished daily . beds were comfy . shuttle bus service was superb , the drivers were friendly , knowledgable , and efficient . it 's because the a / c vents went out into the hallways from the rooms . as long as you kept the a / c on and windows closed , your room was fine .\",\n",
                            " \"it was incredibly dirty in the halls and the room . the bed was like 100 years old and we could not sleep . seriously do n't stay here - there are plenty of other good hotels in the area .\"]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "# Load Features\n",
                "feature2id_filepath = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "id2feature_filepath = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(feature2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature2id_filepath))\n",
                "    feature2id_vocab = json.load(f)\n",
                "with open(id2feature_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2feature_filepath))\n",
                "    id2feature_vocab = json.load(f)\n",
                "assert len(feature2id_vocab) == len(id2feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/feature/feature2id.json\n",
                        "Load file: ../Dataset/tripadvisor/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "feature_words_set = set(list(feature2id_vocab.keys()))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# Write Train Combined Data.\n",
                "# NOTE: Here, we remove those sentences without any features.\n",
                "train_combined_review_file = \"../Dataset/{}/train_combined.json\".format(dataset_name)\n",
                "cnt_line_without_features = 0\n",
                "cnt_line = 0\n",
                "with open(train_combined_review_file, 'w') as fw:\n",
                "    print(\"Write file: {}\".format(train_combined_review_file))\n",
                "    for idx, row in df_train_data.iterrows():\n",
                "        item_id = row['item']\n",
                "        user_id = row['user']\n",
                "        rating = row['rating']\n",
                "        review_origin = row['review']\n",
                "        review_sents = sent_tokenize(review_origin)\n",
                "        review_sents_has_feature = list()\n",
                "        for rvw_sent in review_sents:\n",
                "            rvw_tokens = word_tokenize(rvw_sent)\n",
                "            rvw_has_feature = False\n",
                "            for token in rvw_tokens:\n",
                "                if token in feature_words_set:\n",
                "                    rvw_has_feature = True\n",
                "                    break\n",
                "            if rvw_has_feature:\n",
                "                review_sents_has_feature.append(rvw_sent)\n",
                "        if len(review_sents_has_feature) == 0:\n",
                "            cnt_line_without_features += 1\n",
                "            continue\n",
                "        review_has_feature = \" \".join(review_sents_has_feature)\n",
                "        line_data = {\n",
                "            \"user\": user_id, \"item\": item_id, \"rating\": rating, \"review\": review_has_feature\n",
                "        }\n",
                "        json.dump(line_data, fw)\n",
                "        fw.write(\"\\n\")\n",
                "        cnt_line += 1\n",
                "        if (idx+1) % 10000 == 0:\n",
                "            print(\"{} lines\".format(idx+1))\n",
                "print(\"Finished! Totally {0} lines of training data. Among them {1} lines has no features\".format(\n",
                "    idx+1, cnt_line_without_features\n",
                "))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/tripadvisor/train_combined.json\n",
                        "10000 lines\n",
                        "20000 lines\n",
                        "30000 lines\n",
                        "40000 lines\n",
                        "50000 lines\n",
                        "60000 lines\n",
                        "70000 lines\n",
                        "80000 lines\n",
                        "90000 lines\n",
                        "100000 lines\n",
                        "110000 lines\n",
                        "120000 lines\n",
                        "130000 lines\n",
                        "140000 lines\n",
                        "150000 lines\n",
                        "160000 lines\n",
                        "170000 lines\n",
                        "180000 lines\n",
                        "190000 lines\n",
                        "200000 lines\n",
                        "Finished! Totally 205595 lines of training data. Among them 3243 lines has no features\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "(cnt_line + cnt_line_without_features) == (idx+1)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Write Train Combined Data.\n",
                "# NOTE: Here, we remove those sentences without any features.\n",
                "test_combined_review_file = \"../Dataset/{}/test_combined.json\".format(dataset_name)\n",
                "cnt_line_without_features = 0\n",
                "cnt_line = 0\n",
                "with open(test_combined_review_file, 'w') as fw:\n",
                "    print(\"Write file: {}\".format(test_combined_review_file))\n",
                "    for idx, row in df_test_data.iterrows():\n",
                "        item_id = row['item']\n",
                "        user_id = row['user']\n",
                "        rating = row['rating']\n",
                "        review_origin = row['review']\n",
                "        review_sents = sent_tokenize(review_origin)\n",
                "        review_sents_has_feature = list()\n",
                "        for rvw_sent in review_sents:\n",
                "            rvw_tokens = word_tokenize(rvw_sent)\n",
                "            rvw_has_feature = False\n",
                "            for token in rvw_tokens:\n",
                "                if token in feature_words_set:\n",
                "                    rvw_has_feature = True\n",
                "                    break\n",
                "            if rvw_has_feature:\n",
                "                review_sents_has_feature.append(rvw_sent)\n",
                "        if len(review_sents_has_feature) == 0:\n",
                "            cnt_line_without_features += 1\n",
                "            continue\n",
                "        review_has_feature = \" \".join(review_sents_has_feature)\n",
                "        line_data = {\n",
                "            \"user\": user_id, \"item\": item_id, \"rating\": rating, \"review\": review_has_feature\n",
                "        }\n",
                "        json.dump(line_data, fw)\n",
                "        fw.write(\"\\n\")\n",
                "        cnt_line += 1\n",
                "        if (idx+1) % 10000 == 0:\n",
                "            print(\"{} lines\".format(idx+1))\n",
                "print(\"Finished! Totally {0} lines of test data. Among them {1} lines has no features\".format(\n",
                "    idx+1, cnt_line_without_features\n",
                "))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "## Split ##"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2ID and ID2Sentence Mapping From Train"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    trainset_sent_to_id = json.load(f)\n",
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    trainset_id_to_sent = json.load(f)\n",
                "assert len(trainset_sent_to_id) == len(trainset_id_to_sent)\n",
                "print(\"Number of sentences on train: {}\".format(len(trainset_id_to_sent)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/sentence2id.json\n",
                        "Load file: ../Dataset/yelp/train/sentence/id2sentence.json\n",
                        "Number of sentences on train: 492739\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2ID and ID2Sentence Mapping From Valid/Test"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "validset_id2sentence_filepath = '../Dataset/{}/valid/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(validset_id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(validset_id2sentence_filepath))\n",
                "    validset_id_to_sent = json.load(f)\n",
                "validset_sentence2id_filepath = '../Dataset/{}/valid/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(validset_sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(validset_sentence2id_filepath))\n",
                "    validset_sent_to_id = json.load(f)\n",
                "print(\"There are {} sentences in the validation set.\".format(len(validset_id_to_sent)))\n",
                "\n",
                "testset_id2sentence_filepath = '../Dataset/{}/test/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(testset_id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(testset_id2sentence_filepath))\n",
                "    testset_id_to_sent = json.load(f)\n",
                "testset_sentence2id_filepath = '../Dataset/{}/test/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(testset_sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(testset_sentence2id_filepath))\n",
                "    testset_sent_to_id = json.load(f)\n",
                "print(\"There are {} sentences in the test set.\".format(len(testset_id_to_sent)))\n",
                "\n",
                "assert testset_id_to_sent == validset_id_to_sent\n",
                "assert testset_sent_to_id == validset_sent_to_id"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/valid/sentence/id2sentence.json\n",
                        "Load file: ../Dataset/yelp/valid/sentence/sentence2id.json\n",
                        "There are 109833 sentences in the validation set.\n",
                        "Load file: ../Dataset/yelp/test/sentence/id2sentence.json\n",
                        "Load file: ../Dataset/yelp/test/sentence/sentence2id.json\n",
                        "There are 109833 sentences in the test set.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Features"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "feature2id_filepath = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "id2feature_filepath = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(feature2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature2id_filepath))\n",
                "    feature2id_vocab = json.load(f)\n",
                "with open(id2feature_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2feature_filepath))\n",
                "    id2feature_vocab = json.load(f)\n",
                "assert len(feature2id_vocab) == len(id2feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/feature/feature2id.json\n",
                        "Load file: ../Dataset/yelp/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence to Feature Tf Dict"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "trainset_sent2featuretf_file = '../Dataset/{}/train/sentence/sentence2featuretf.json'.format(dataset_name)\n",
                "testset_sent2featuretf_file = '../Dataset/{}/test/sentence/sentence2featuretf.json'.format(dataset_name)\n",
                "with open(trainset_sent2featuretf_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(trainset_sent2featuretf_file))\n",
                "    trainset_sent2featuretf = json.load(f)\n",
                "with open(testset_sent2featuretf_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(testset_sent2featuretf_file))\n",
                "    testset_sent2featuretf = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/sentence/sentence2featuretf.json\n",
                        "Load file: ../Dataset/yelp/test/sentence/sentence2featuretf.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load multi-line train/valid/test data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "trainset_data_multiline_file = \"../Dataset/{}/train/useritem2sentids_multilines.json\".format(dataset_name)\n",
                "validset_data_multiline_file = \"../Dataset/{}/valid/useritem2sentids_test_multilines.json\".format(dataset_name)\n",
                "testset_data_multiline_file = \"../Dataset/{}/test/useritem2sentids_test_multilines.json\".format(dataset_name)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "def construct_text_review_train(review_content, review_sent_ids):\n",
                "    combined_review = []\n",
                "    combined_review_ids = []\n",
                "    for ex_sent in sent_tokenize(review_content):\n",
                "        if ex_sent in trainset_sent_to_id:\n",
                "            ex_sent_id = trainset_sent_to_id[ex_sent]\n",
                "            if ex_sent_id in review_sent_ids and ex_sent_id not in combined_review_ids:\n",
                "                combined_review.append(ex_sent)\n",
                "                combined_review_ids.append(ex_sent_id)\n",
                "    try:\n",
                "        assert len(combined_review_ids) == len(review_sent_ids)\n",
                "    except:\n",
                "        print(\"Error! review is: {}\".format(review_content))\n",
                "    return \" \".join(combined_review), combined_review_ids"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "def construct_text_review_test(review_content, review_sent_ids):\n",
                "    combined_review = []\n",
                "    combined_review_ids = []\n",
                "    for ex_sent in sent_tokenize(review_content):\n",
                "        if ex_sent in testset_sent_to_id:\n",
                "            ex_sent_id = testset_sent_to_id[ex_sent]\n",
                "            if ex_sent_id in review_sent_ids and ex_sent_id not in combined_review_ids:\n",
                "                combined_review.append(ex_sent)\n",
                "                combined_review_ids.append(ex_sent_id)\n",
                "    try:\n",
                "        assert len(combined_review_ids) == len(review_sent_ids)\n",
                "    except:\n",
                "        print(\"Error! review is: {}\".format(review_content))\n",
                "    return \" \".join(combined_review), combined_review_ids"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "trainset_ui2rvw = dict()\n",
                "trainset_ui2rating = dict()\n",
                "for idx, train_rvw_data in df_train_data.iterrows():\n",
                "    user_id = train_rvw_data['user']\n",
                "    item_id = train_rvw_data['item']\n",
                "    rating = train_rvw_data['rating']\n",
                "    assert isinstance(rating, int)\n",
                "    rvw_text = train_rvw_data['review']\n",
                "    assert isinstance(rvw_text, str)\n",
                "    if user_id in trainset_ui2rvw:\n",
                "        assert item_id not in trainset_ui2rvw[user_id]\n",
                "        assert user_id in trainset_ui2rating\n",
                "        assert item_id not in trainset_ui2rating[user_id]\n",
                "        trainset_ui2rvw[user_id][item_id] = rvw_text\n",
                "        trainset_ui2rating[user_id][item_id] = rating\n",
                "    else:\n",
                "        assert user_id not in trainset_ui2rating\n",
                "        trainset_ui2rvw[user_id] = dict()\n",
                "        trainset_ui2rating[user_id] = dict()\n",
                "        trainset_ui2rvw[user_id][item_id] = rvw_text\n",
                "        trainset_ui2rating[user_id][item_id] = rating\n",
                "    if (idx+1) % 20000 == 0:\n",
                "        print(\"{} lines\".format(idx+1))\n",
                "print(\"Totally {} lines\".format(idx+1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "20000 lines\n",
                        "40000 lines\n",
                        "60000 lines\n",
                        "80000 lines\n",
                        "100000 lines\n",
                        "120000 lines\n",
                        "140000 lines\n",
                        "160000 lines\n",
                        "180000 lines\n",
                        "Totally 191227 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "# Load train set (multi-line)\n",
                "combined_train_review = []\n",
                "with open(trainset_data_multiline_file) as f:\n",
                "    print(\"Load file: {}\".format(trainset_data_multiline_file))\n",
                "    cnt_line = 0\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user_id']\n",
                "        item_id = line_data['item_id']\n",
                "        assert isinstance(user_id, str)\n",
                "        assert isinstance(item_id, str)\n",
                "        candidate_ids = line_data[\"candidate\"]\n",
                "        review_ids = line_data[\"review\"]\n",
                "        # get the review and rating of this user-item\n",
                "        this_review = trainset_ui2rvw[user_id][item_id]\n",
                "        this_rating = trainset_ui2rating[user_id][item_id]\n",
                "        this_combined_review, this_combined_review_sentids = construct_text_review_train(\n",
                "            this_review, review_ids)\n",
                "        combined_train_review.append(\n",
                "            [item_id, user_id, this_rating, this_combined_review, this_combined_review_sentids])\n",
                "        cnt_line += 1\n",
                "        if cnt_line % 10000 == 0:\n",
                "            print(\"Processed {} lines of data\".format(cnt_line))\n",
                "print(\"Finished! Totally {} lines.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train/useritem2sentids_multilines.json\n",
                        "Processed 10000 lines of data\n",
                        "Processed 20000 lines of data\n",
                        "Processed 30000 lines of data\n",
                        "Processed 40000 lines of data\n",
                        "Processed 50000 lines of data\n",
                        "Processed 60000 lines of data\n",
                        "Processed 70000 lines of data\n",
                        "Processed 80000 lines of data\n",
                        "Processed 90000 lines of data\n",
                        "Processed 100000 lines of data\n",
                        "Processed 110000 lines of data\n",
                        "Processed 120000 lines of data\n",
                        "Processed 130000 lines of data\n",
                        "Processed 140000 lines of data\n",
                        "Processed 150000 lines of data\n",
                        "Processed 160000 lines of data\n",
                        "Processed 170000 lines of data\n",
                        "Processed 180000 lines of data\n",
                        "Processed 190000 lines of data\n",
                        "Finished! Totally 191227 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "print(combined_train_review[:2])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[['1071', '1001', 4, 'place was well lit , very clean , and staff very friendly ... wow , , , i was thinking this is gon na hurt my pocket now : - ( ... on to the food and ordering , aioli was ok , as well as the tomato sauce . garlic cheese bread w crab - very delicious , and definitely enough crab to know there is actually crab in it ... cajun style shrimp boil - wife got this and she loved it , it had really great broth , and their homemade bread was awesome for dipping . staff / bartender was very informative and even the one to suggest trying that saffron cream before ordering ... very attentive and friendly ... as far as a seafood \" restaurant \" , i \\'m not sure how to classify this place , there food is really great , but there is a lack of full plate options / entrees .', ['6093', '6094', '6095']], ['10877', '1001', 3, 'the price was great and so was the subs ... this is a can do place , , , parking can be a bia , so be warned , , will add to a once a month rotation ...', ['6096']]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "len(combined_train_review)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "191227"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 21
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "testset_ui2rvw = dict()\n",
                "testset_ui2rating = dict()\n",
                "for idx, test_rvw_data in df_test_data.iterrows():\n",
                "    user_id = test_rvw_data['user']\n",
                "    item_id = test_rvw_data['item']\n",
                "    rating = test_rvw_data['rating']\n",
                "    assert isinstance(rating, int)\n",
                "    rvw_text = test_rvw_data['review']\n",
                "    assert isinstance(rvw_text, str)\n",
                "    if user_id in testset_ui2rvw:\n",
                "        assert item_id not in testset_ui2rvw[user_id]\n",
                "        assert user_id in testset_ui2rating\n",
                "        assert item_id not in testset_ui2rating[user_id]\n",
                "        testset_ui2rvw[user_id][item_id] = rvw_text\n",
                "        testset_ui2rating[user_id][item_id] = rating\n",
                "    else:\n",
                "        assert user_id not in testset_ui2rating\n",
                "        testset_ui2rvw[user_id] = dict()\n",
                "        testset_ui2rating[user_id] = dict()\n",
                "        testset_ui2rvw[user_id][item_id] = rvw_text\n",
                "        testset_ui2rating[user_id][item_id] = rating\n",
                "    if (idx+1) % 20000 == 0:\n",
                "        print(\"{} lines\".format(idx+1))\n",
                "print(\"Totally {} lines\".format(idx+1))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "20000 lines\n",
                        "40000 lines\n",
                        "Totally 42702 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "# Load test set (multi-line)\n",
                "combined_test_review = []\n",
                "with open(testset_data_multiline_file) as f:\n",
                "    print(\"Load file: {}\".format(testset_data_multiline_file))\n",
                "    cnt_line = 0\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = str(line_data['user_id'])\n",
                "        item_id = str(line_data['item_id'])\n",
                "        assert isinstance(user_id, str)\n",
                "        assert isinstance(item_id, str)\n",
                "        candidate_ids = line_data[\"candidate\"]\n",
                "        review_ids = line_data[\"review\"]\n",
                "        # get the review and rating of this user-item\n",
                "        this_review = testset_ui2rvw[user_id][item_id]\n",
                "        this_rating = testset_ui2rating[user_id][item_id]\n",
                "        this_combined_review, this_combined_review_sentids = construct_text_review_test(\n",
                "            this_review, review_ids)\n",
                "        combined_test_review.append(\n",
                "            [item_id, user_id, this_rating, this_combined_review, this_combined_review_sentids])\n",
                "        cnt_line += 1\n",
                "        if cnt_line % 20000 == 0:\n",
                "            print(\"Processed {} lines of data\".format(cnt_line))\n",
                "print(\"Finished! Totally {} lines.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/test/useritem2sentids_test_multilines.json\n",
                        "Processed 20000 lines of data\n",
                        "Processed 40000 lines of data\n",
                        "Finished! Totally 42702 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "len(combined_test_review)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "42702"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 24
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "print(combined_test_review[:2])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[['1098', '1001', 4, \"great food , great price , great atmosphere ... portion size was huge comparative to the lunch pricing ... iced tea was definitely not china mist or nestle , or lipton , very good also . 2 appetizers with beer , and 2 full entree 's for about 30 bucks out the door ... simply great ... will definitely be back , , often\", ['1102', '1103']], ['1473', '1001', 4, \"the bbq pork also was way different this time , so i 'm not sure what happened if , short staff for new years or what , , but will try a few more times before concluding it and removed from faves ... the singapore style rice noodles were the same n that saved the day ... other goto is their chinese broccoli ...\", ['1104']]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Write Train / Test Data into Multi-line Format"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "# Write Train Data\n",
                "train_combined_review_file = \"../Dataset/{}/train_combined.json\".format(dataset_name)\n",
                "cnt_line = 0\n",
                "with open(train_combined_review_file, 'w') as fw:\n",
                "    print(\"Write file: {}\".format(train_combined_review_file))\n",
                "    for train_combined_instance in combined_train_review:\n",
                "        item_id = train_combined_instance[0]\n",
                "        user_id = train_combined_instance[1]\n",
                "        rating = train_combined_instance[2]\n",
                "        review_content = train_combined_instance[3]\n",
                "        current_train_combined_dict = {\n",
                "            \"user\":user_id, 'item':item_id, 'rating':rating, 'review':review_content}\n",
                "        # dump this dict into the json file\n",
                "        json.dump(current_train_combined_dict, fw)\n",
                "        fw.write('\\n')\n",
                "        cnt_line += 1\n",
                "print(\"Finished! Totally {} lines of training data.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/train_combined.json\n",
                        "Finished! Totally 191227 lines of training data.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "# Write Test Data\n",
                "test_combined_review_file = \"../Dataset/{}/test_combined.json\".format(dataset_name)\n",
                "cnt_line = 0\n",
                "with open(test_combined_review_file, 'w') as fw:\n",
                "    print(\"Write file: {}\".format(test_combined_review_file))\n",
                "    for test_combined_instance in combined_test_review:\n",
                "        item_id = test_combined_instance[0]\n",
                "        user_id = test_combined_instance[1]\n",
                "        rating = test_combined_instance[2]\n",
                "        review_content = test_combined_instance[3]\n",
                "        current_test_combined_dict = {\n",
                "            \"user\":user_id, 'item':item_id, 'rating':rating, 'review':review_content}\n",
                "        # dump this dict into the json file\n",
                "        json.dump(current_test_combined_dict, fw)\n",
                "        fw.write('\\n')\n",
                "        cnt_line += 1\n",
                "print(\"Finished! Totally {} lines of test data.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/test_combined.json\n",
                        "Finished! Totally 42702 lines of test data.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Construct Valid Set\n",
                "\n",
                "### We also need a valid set. In the graph model, the current valid set use the same user-item pairs as in the test set but different candidate sets. However, we don't have candidate set here. Therefore, one easy way to construct the valid set is to randomly extract a subset of test set as valid set."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "valid_ratio = 0.4\n",
                "import random\n",
                "combined_valid_review = []\n",
                "for test_combined_instance in combined_test_review:\n",
                "    if random.random() <= valid_ratio:\n",
                "        combined_valid_review.append(test_combined_instance)\n",
                "print(\"Finished! Totally {} lines of valid data.\".format(len(combined_valid_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Finished! Totally 17086 lines of valid data.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "# Write Valid Data\n",
                "valid_combined_review_file = \"../Dataset/{}/valid_combined.json\".format(dataset_name)\n",
                "cnt_line = 0\n",
                "with open(valid_combined_review_file, 'w') as fw:\n",
                "    print(\"Write file: {}\".format(valid_combined_review_file))\n",
                "    for valid_combined_instance in combined_valid_review:\n",
                "        item_id = valid_combined_instance[0]\n",
                "        user_id = valid_combined_instance[1]\n",
                "        rating = valid_combined_instance[2]\n",
                "        review_content = valid_combined_instance[3]\n",
                "        current_valid_combined_dict = {\"user\":user_id, 'item':item_id, 'rating':rating, 'review':review_content}\n",
                "        # dump this dict into the json file\n",
                "        json.dump(current_valid_combined_dict, fw)\n",
                "        fw.write('\\n')\n",
                "        cnt_line += 1\n",
                "print(\"Finished! Totally {} lines of valid data.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/yelp/valid_combined.json\n",
                        "Finished! Totally 17086 lines of valid data.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "# convert test data into dataframe\n",
                "df_combined_test_review = pd.DataFrame(\n",
                "    combined_test_review, columns=['item', 'user', 'rating', 'review', 'review_sentids'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "df_combined_test_review"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item  user  rating                                             review  \\\n",
                            "0      1098  1001       4  great food , great price , great atmosphere .....   \n",
                            "1      1473  1001       4  the bbq pork also was way different this time ...   \n",
                            "2       157  1001       4  but that dumb naan , or pita bread stuff was a...   \n",
                            "3      1707  1001       4  _ price - average - please recognize fresh veg...   \n",
                            "4      2911  1001       4  pizza was very good , fresh ingredients , , no...   \n",
                            "...     ...   ...     ...                                                ...   \n",
                            "42697  3933  9999       4  they do n't have a matinee price , but then ag...   \n",
                            "42698  4154  9999       3  the main draw to this casinos over the others ...   \n",
                            "42699  4565  9999       5  it 's not like normal stouts and the flavor is...   \n",
                            "42700   624  9999       5  my two favorite meats for tacos are carne asad...   \n",
                            "42701   758  9999       5  the chicken and dumplings is probably the best...   \n",
                            "\n",
                            "                                         review_sentids  \n",
                            "0                                          [1102, 1103]  \n",
                            "1                                                [1104]  \n",
                            "2                                                [1105]  \n",
                            "3                                    [1106, 1107, 1108]  \n",
                            "4                                                [1109]  \n",
                            "...                                                 ...  \n",
                            "42697                                          [109817]  \n",
                            "42698                                          [109818]  \n",
                            "42699                          [109819, 109820, 109821]  \n",
                            "42700  [109822, 109823, 109824, 109825, 109826, 109827]  \n",
                            "42701          [109828, 109829, 109830, 109831, 109832]  \n",
                            "\n",
                            "[42702 rows x 5 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "      <th>review_sentids</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1098</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>great food , great price , great atmosphere .....</td>\n",
                            "      <td>[1102, 1103]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1473</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the bbq pork also was way different this time ...</td>\n",
                            "      <td>[1104]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>157</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>but that dumb naan , or pita bread stuff was a...</td>\n",
                            "      <td>[1105]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1707</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>_ price - average - please recognize fresh veg...</td>\n",
                            "      <td>[1106, 1107, 1108]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>2911</td>\n",
                            "      <td>1001</td>\n",
                            "      <td>4</td>\n",
                            "      <td>pizza was very good , fresh ingredients , , no...</td>\n",
                            "      <td>[1109]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42697</th>\n",
                            "      <td>3933</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>they do n't have a matinee price , but then ag...</td>\n",
                            "      <td>[109817]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42698</th>\n",
                            "      <td>4154</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the main draw to this casinos over the others ...</td>\n",
                            "      <td>[109818]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42699</th>\n",
                            "      <td>4565</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>it 's not like normal stouts and the flavor is...</td>\n",
                            "      <td>[109819, 109820, 109821]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42700</th>\n",
                            "      <td>624</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>my two favorite meats for tacos are carne asad...</td>\n",
                            "      <td>[109822, 109823, 109824, 109825, 109826, 109827]</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42701</th>\n",
                            "      <td>758</td>\n",
                            "      <td>9999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>the chicken and dumplings is probably the best...</td>\n",
                            "      <td>[109828, 109829, 109830, 109831, 109832]</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>42702 rows × 5 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 31
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "# # Load valid data\n",
                "# valid_combined_review_file = \"../Dataset/{}/valid_combined.json\".format(dataset_name)\n",
                "# combined_valid_review = []\n",
                "# with open(valid_combined_review_file, 'r') as fw:\n",
                "#     print(\"Load file: {}\".format(valid_combined_review_file))\n",
                "#     for line in fw:\n",
                "#         valid_combined_instance = json.loads(line)\n",
                "#         user_id = valid_combined_instance['user']\n",
                "#         item_id = valid_combined_instance['item']\n",
                "#         rating = valid_combined_instance['rating']\n",
                "#         review_text = valid_combined_instance['review']\n",
                "#         df_combined_this_instance = df_combined_test_review.loc[\n",
                "#             (df_combined_test_review['user'] == user_id) & (df_combined_test_review['item'] == item_id)]\n",
                "#         this_review_sentids = list(df_combined_this_instance['review_sentids'])[0]\n",
                "#         combined_valid_review.append(\n",
                "#             [item_id, user_id, rating, review_text, this_review_sentids]\n",
                "#         )\n",
                "# print(\"Finished, totally {} lines of valid data.\".format(len(combined_valid_review)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct Dataset Align with the format of NRT and NARRE"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "train_combined_review_file = \"../Dataset/{}/train_combined.json\".format(dataset_name)\n",
                "test_combined_review_file = \"../Dataset/{}/test_combined.json\".format(dataset_name)\n",
                "valid_combined_review_file = \"../Dataset/{}/valid_combined.json\".format(dataset_name)\n",
                "\n",
                "# Load train data\n",
                "train_combined_review = []\n",
                "cnt = 0\n",
                "with open(train_combined_review_file) as f:\n",
                "    print(\"Load file: {}\".format(train_combined_review_file))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = int(line_data['user'])        # convert str to int\n",
                "        item_id = int(line_data['item'])        # convert str to int\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_combined_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_combined_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/train_combined.json\n",
                        "100000 lines loaded.\n",
                        "Finish loading train dataset, totally 191227 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "# Load valid data\n",
                "valid_combined_review = []\n",
                "cnt = 0\n",
                "with open(valid_combined_review_file) as f:\n",
                "    print(\"Load file: {}\".format(valid_combined_review_file))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = int(line_data['user'])        # convert str to int\n",
                "        item_id = int(line_data['item'])        # convert str to int\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        valid_combined_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading valid dataset, totally {} lines.'.format(len(valid_combined_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/valid_combined.json\n",
                        "Finish loading valid dataset, totally 17086 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "# Load test data\n",
                "test_combined_review = []\n",
                "cnt = 0\n",
                "with open(test_combined_review_file) as f:\n",
                "    print(\"Load file: {}\".format(test_combined_review_file))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = int(line_data['user'])        # convert str to int\n",
                "        item_id = int(line_data['item'])        # convert str to int\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_combined_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_combined_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/test_combined.json\n",
                        "Finish loading test dataset, totally 42702 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "# Convert Train/Valid/Test DataFrame\n",
                "df_train_data_combined = pd.DataFrame(\n",
                "    train_combined_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_valid_data_combined = pd.DataFrame(\n",
                "    valid_combined_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data_combined = pd.DataFrame(\n",
                "    test_combined_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "train_item_ids = list(df_train_data_combined['item'].unique())\n",
                "train_user_ids = list(df_train_data_combined['user'].unique())\n",
                "valid_item_ids = list(df_valid_data_combined['item'].unique())\n",
                "valid_user_ids = list(df_valid_data_combined['user'].unique())\n",
                "test_item_ids = list(df_test_data_combined['item'].unique())\n",
                "test_user_ids = list(df_test_data_combined['user'].unique())\n",
                "# construct the set of the item/user ids on train set\n",
                "train_item_ids_set = set(train_item_ids)\n",
                "train_user_ids_set = set(train_user_ids)\n",
                "# check to make sure that item/user id on valid/test set appears in the train set\n",
                "for item_id in valid_item_ids:\n",
                "    assert item_id in train_item_ids_set\n",
                "for item_id in test_item_ids:\n",
                "    assert item_id in train_item_ids_set\n",
                "for user_id in valid_user_ids:\n",
                "    assert user_id in train_user_ids_set\n",
                "for user_id in test_user_ids:\n",
                "    assert user_id in train_user_ids_set"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "# construct userid-userid_idx mapping\n",
                "print(\"Number of user on train-set: {}\".format(len(train_user_ids)))\n",
                "assert len(train_user_ids) == len(train_user_ids_set)\n",
                "userid2idx = {}\n",
                "idx_cnt = 0\n",
                "for user_id in train_user_ids:\n",
                "    assert user_id not in userid2idx\n",
                "    userid2idx[user_id] = idx_cnt\n",
                "    idx_cnt += 1\n",
                "# construct itemid-itemid_idx mapping\n",
                "print(\"Number of item on train-set: {}\".format(len(train_item_ids)))\n",
                "assert len(train_item_ids) == len(train_item_ids_set)\n",
                "itemid2idx = {}\n",
                "idx_cnt = 0\n",
                "for item_id in train_item_ids:\n",
                "    assert item_id not in itemid2idx\n",
                "    itemid2idx[item_id] = idx_cnt\n",
                "    idx_cnt += 1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of user on train-set: 4604\n",
                        "Number of item on train-set: 7837\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "userid2idx[1001]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 41
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "list(userid2idx.items())[:20]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[(1001, 0),\n",
                            " (10011, 1),\n",
                            " (1002, 2),\n",
                            " (10020, 3),\n",
                            " (10021, 4),\n",
                            " (10024, 5),\n",
                            " (10028, 6),\n",
                            " (1003, 7),\n",
                            " (10037, 8),\n",
                            " (1004, 9),\n",
                            " (10040, 10),\n",
                            " (10043, 11),\n",
                            " (10044, 12),\n",
                            " (10049, 13),\n",
                            " (1005, 14),\n",
                            " (10058, 15),\n",
                            " (10061, 16),\n",
                            " (10062, 17),\n",
                            " (10069, 18),\n",
                            " (10082, 19)]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 38
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "# save userid-userid_idx mapping into txt file\n",
                "# i.e. at line 1, we write the userid with idx of 0\n",
                "user_idx_file = \"./combined_data/{}/users.txt\".format(dataset_name)\n",
                "item_idx_file = \"./combined_data/{}/items.txt\".format(dataset_name)\n",
                "with open(user_idx_file, 'w') as f_usr:\n",
                "    f_usr.write('\\n'.join(map(str, userid2idx.keys())))\n",
                "with open(item_idx_file, 'w') as f_itm:\n",
                "    f_itm.write('\\n'.join(map(str, itemid2idx.keys())))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "# save user2uid and item2iid mapping dict\n",
                "user2uid_file = \"./combined_data/{}/user2uid.json\".format(dataset_name)\n",
                "item2iid_file = \"./combined_data/{}/item2iid.json\".format(dataset_name)\n",
                "user2idx = {str(k):str(v) for k,v in userid2idx.items()}\n",
                "item2idx = {str(k):str(v) for k,v in itemid2idx.items()}\n",
                "with open(user2uid_file, 'w') as f:\n",
                "    print(\"Write file: {}\".format(user2uid_file))\n",
                "    json.dump(user2idx, f)\n",
                "with open(item2iid_file, 'w') as f:\n",
                "    print(\"Write file: {}\".format(item2iid_file))\n",
                "    json.dump(item2idx, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ./combined_data/yelp/user2uid.json\n",
                        "Write file: ./combined_data/yelp/item2iid.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "# Construct Train Dataset. \n",
                "# 1. Add feature words for each sentence.\n",
                "# 2. Convert the user/item id to relevant index.\n",
                "train_combined_review_with_feature = []\n",
                "train_combined_review_with_feature_comb_sent = []\n",
                "for train_data_instance in combined_train_review:\n",
                "    item_id = int(train_data_instance[0])\n",
                "    user_id = int(train_data_instance[1])\n",
                "    rating = train_data_instance[2]\n",
                "    review_text = train_data_instance[3]\n",
                "    review_sentids = train_data_instance[4]\n",
                "    # convert user/item id to idx\n",
                "    item_idx = itemid2idx[item_id]\n",
                "    user_idx = userid2idx[user_id]\n",
                "    # add feature words for each sentence\n",
                "    review_sents_with_features = []\n",
                "    feat_words_set = set()\n",
                "    rvw_sents_list = list()\n",
                "    for rvw_sent_id in review_sentids:\n",
                "        # get this sentence's feature tf\n",
                "        feature_tf = trainset_sent2featuretf[rvw_sent_id]\n",
                "        feature_ids = feature_tf.keys()\n",
                "        feature_words = [id2feature_vocab[fea_id] for fea_id in feature_ids]\n",
                "        # get the sentence text content of this sentid\n",
                "        rvw_sent = trainset_id_to_sent[rvw_sent_id]\n",
                "        review_sents_with_features.append([feature_words, rvw_sent])\n",
                "        for word in feature_words:\n",
                "            feat_words_set.add(word)\n",
                "        rvw_sents_list.append(rvw_sent)\n",
                "    feat_words_comb = list(feat_words_set)\n",
                "    rvw_sents_comb = \" \".join(rvw_sents_list)\n",
                "    # pack item_idx, user_idx, rating, review(with feature)\n",
                "    train_combined_review_with_feature.append(\n",
                "        [item_idx, user_idx, rating, review_sents_with_features]\n",
                "    )\n",
                "    train_combined_review_with_feature_comb_sent.append(\n",
                "        [item_idx, user_idx, rating, [[feat_words_comb, rvw_sents_comb]]]\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "train_combined_review_with_feature[:2]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[0,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['food', 'staff', 'sauce', 'tomato', 'aioli'],\n",
                            "    'place was well lit , very clean , and staff very friendly ... wow , , , i was thinking this is gon na hurt my pocket now : - ( ... on to the food and ordering , aioli was ok , as well as the tomato sauce .'],\n",
                            "   [['cheese', 'bread', 'shrimp', 'broth', 'garlic', 'crab', 'homemade'],\n",
                            "    'garlic cheese bread w crab - very delicious , and definitely enough crab to know there is actually crab in it ... cajun style shrimp boil - wife got this and she loved it , it had really great broth , and their homemade bread was awesome for dipping .'],\n",
                            "   [['food',\n",
                            "     'restaurant',\n",
                            "     'staff',\n",
                            "     'cream',\n",
                            "     'options',\n",
                            "     'plate',\n",
                            "     'seafood',\n",
                            "     'entrees'],\n",
                            "    'staff / bartender was very informative and even the one to suggest trying that saffron cream before ordering ... very attentive and friendly ... as far as a seafood \" restaurant \" , i \\'m not sure how to classify this place , there food is really great , but there is a lack of full plate options / entrees .']]],\n",
                            " [1,\n",
                            "  0,\n",
                            "  3,\n",
                            "  [[['price', 'parking', 'subs'],\n",
                            "    'the price was great and so was the subs ... this is a can do place , , , parking can be a bia , so be warned , , will add to a once a month rotation ...']]]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 45
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "train_combined_review_with_feature_comb_sent[:2]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[0,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['bread',\n",
                            "     'options',\n",
                            "     'crab',\n",
                            "     'restaurant',\n",
                            "     'seafood',\n",
                            "     'homemade',\n",
                            "     'garlic',\n",
                            "     'tomato',\n",
                            "     'shrimp',\n",
                            "     'staff',\n",
                            "     'aioli',\n",
                            "     'cream',\n",
                            "     'sauce',\n",
                            "     'broth',\n",
                            "     'entrees',\n",
                            "     'food',\n",
                            "     'cheese',\n",
                            "     'plate'],\n",
                            "    'place was well lit , very clean , and staff very friendly ... wow , , , i was thinking this is gon na hurt my pocket now : - ( ... on to the food and ordering , aioli was ok , as well as the tomato sauce . garlic cheese bread w crab - very delicious , and definitely enough crab to know there is actually crab in it ... cajun style shrimp boil - wife got this and she loved it , it had really great broth , and their homemade bread was awesome for dipping . staff / bartender was very informative and even the one to suggest trying that saffron cream before ordering ... very attentive and friendly ... as far as a seafood \" restaurant \" , i \\'m not sure how to classify this place , there food is really great , but there is a lack of full plate options / entrees .']]],\n",
                            " [1,\n",
                            "  0,\n",
                            "  3,\n",
                            "  [[['price', 'parking', 'subs'],\n",
                            "    'the price was great and so was the subs ... this is a can do place , , , parking can be a bia , so be warned , , will add to a once a month rotation ...']]]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 46
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "# Construct Test Dataset. \n",
                "# 1. Add feature words for each sentence.\n",
                "# 2. Convert the user/item id to relevant index.\n",
                "test_combined_review_with_feature = []\n",
                "test_combined_review_with_feature_comb_sent = []\n",
                "for test_data_instance in combined_test_review:\n",
                "    item_id = int(test_data_instance[0])\n",
                "    user_id = int(test_data_instance[1])\n",
                "    rating = test_data_instance[2]\n",
                "    review_text = test_data_instance[3]\n",
                "    review_sentids = test_data_instance[4]\n",
                "    # convert user/item id to idx\n",
                "    item_idx = itemid2idx[item_id]\n",
                "    user_idx = userid2idx[user_id]\n",
                "    # add feature words for each sentence\n",
                "    review_sents_with_features = []\n",
                "    feat_words_set = set()\n",
                "    rvw_sents_list = list()\n",
                "    for rvw_sent_id in review_sentids:\n",
                "        # get this sentence's feature tf\n",
                "        feature_tf = testset_sent2featuretf[rvw_sent_id]\n",
                "        feature_ids = feature_tf.keys()\n",
                "        feature_words = [id2feature_vocab[fea_id] for fea_id in feature_ids]\n",
                "        # get the sentence text content of this sentid\n",
                "        rvw_sent = testset_id_to_sent[rvw_sent_id]\n",
                "        review_sents_with_features.append([feature_words, rvw_sent])\n",
                "        for word in feature_words:\n",
                "            feat_words_set.add(word)\n",
                "        rvw_sents_list.append(rvw_sent)\n",
                "    feat_words_comb = list(feat_words_set)\n",
                "    rvw_sents_comb = \" \".join(rvw_sents_list)\n",
                "    # pack item_idx, user_idx, rating, review(with feature)\n",
                "    test_combined_review_with_feature.append(\n",
                "        [item_idx, user_idx, rating, review_sents_with_features]\n",
                "    )\n",
                "    test_combined_review_with_feature_comb_sent.append(\n",
                "        [item_idx, user_idx, rating, [[feat_words_comb, rvw_sents_comb]]]\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "test_combined_review_with_feature[:2]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[2251,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['food',\n",
                            "     'price',\n",
                            "     'atmosphere',\n",
                            "     'portion',\n",
                            "     'lunch',\n",
                            "     'size',\n",
                            "     'tea',\n",
                            "     'pricing'],\n",
                            "    'great food , great price , great atmosphere ... portion size was huge comparative to the lunch pricing ... iced tea was definitely not china mist or nestle , or lipton , very good also .'],\n",
                            "   [['beer', 'appetizers', 'bucks'],\n",
                            "    \"2 appetizers with beer , and 2 full entree 's for about 30 bucks out the door ... simply great ... will definitely be back , , often\"]]],\n",
                            " [3039,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['staff', 'rice', 'pork', 'noodles', 'bbq', 'broccoli'],\n",
                            "    \"the bbq pork also was way different this time , so i 'm not sure what happened if , short staff for new years or what , , but will try a few more times before concluding it and removed from faves ... the singapore style rice noodles were the same n that saved the day ... other goto is their chinese broccoli ...\"]]]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 48
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "test_combined_review_with_feature_comb_sent[:2]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[2251,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['size',\n",
                            "     'price',\n",
                            "     'pricing',\n",
                            "     'appetizers',\n",
                            "     'portion',\n",
                            "     'bucks',\n",
                            "     'atmosphere',\n",
                            "     'beer',\n",
                            "     'lunch',\n",
                            "     'food',\n",
                            "     'tea'],\n",
                            "    \"great food , great price , great atmosphere ... portion size was huge comparative to the lunch pricing ... iced tea was definitely not china mist or nestle , or lipton , very good also . 2 appetizers with beer , and 2 full entree 's for about 30 bucks out the door ... simply great ... will definitely be back , , often\"]]],\n",
                            " [3039,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['pork', 'rice', 'broccoli', 'staff', 'noodles', 'bbq'],\n",
                            "    \"the bbq pork also was way different this time , so i 'm not sure what happened if , short staff for new years or what , , but will try a few more times before concluding it and removed from faves ... the singapore style rice noodles were the same n that saved the day ... other goto is their chinese broccoli ...\"]]]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 49
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "# Construct Valid Dataset. \n",
                "# 1. Add feature words for each sentence.\n",
                "# 2. Convert the user/item id to relevant index.\n",
                "valid_combined_review_with_feature = []\n",
                "valid_combined_review_with_feature_comb_sent = []\n",
                "for valid_data_instance in combined_valid_review:\n",
                "    item_id = int(valid_data_instance[0])\n",
                "    user_id = int(valid_data_instance[1])\n",
                "    rating = valid_data_instance[2]\n",
                "    review_text = valid_data_instance[3]\n",
                "    review_sentids = valid_data_instance[4]\n",
                "    # convert user/item id to idx\n",
                "    item_idx = itemid2idx[item_id]\n",
                "    user_idx = userid2idx[user_id]\n",
                "    # add feature words for each sentence\n",
                "    review_sents_with_features = []\n",
                "    feat_words_set = set()\n",
                "    rvw_sents_list = list()\n",
                "    for rvw_sent_id in review_sentids:\n",
                "        # get this sentence's feature tf\n",
                "        feature_tf = testset_sent2featuretf[rvw_sent_id]\n",
                "        feature_ids = feature_tf.keys()\n",
                "        feature_words = [id2feature_vocab[fea_id] for fea_id in feature_ids]\n",
                "        # get the sentence text content of this sentid\n",
                "        rvw_sent = testset_id_to_sent[rvw_sent_id]\n",
                "        review_sents_with_features.append([feature_words, rvw_sent])\n",
                "        for word in feature_words:\n",
                "            feat_words_set.add(word)\n",
                "        rvw_sents_list.append(rvw_sent)\n",
                "    feat_words_comb = list(feat_words_set)\n",
                "    rvw_sents_comb = \" \".join(rvw_sents_list)\n",
                "    # pack item_idx, user_idx, rating, review(with feature)\n",
                "    valid_combined_review_with_feature.append(\n",
                "        [item_idx, user_idx, rating, review_sents_with_features]\n",
                "    )\n",
                "    valid_combined_review_with_feature_comb_sent.append(\n",
                "        [item_idx, user_idx, rating, [[feat_words_comb, rvw_sents_comb]]]\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "valid_combined_review_with_feature[:2]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[2251,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['food',\n",
                            "     'price',\n",
                            "     'atmosphere',\n",
                            "     'portion',\n",
                            "     'lunch',\n",
                            "     'size',\n",
                            "     'tea',\n",
                            "     'pricing'],\n",
                            "    'great food , great price , great atmosphere ... portion size was huge comparative to the lunch pricing ... iced tea was definitely not china mist or nestle , or lipton , very good also .'],\n",
                            "   [['beer', 'appetizers', 'bucks'],\n",
                            "    \"2 appetizers with beer , and 2 full entree 's for about 30 bucks out the door ... simply great ... will definitely be back , , often\"]]],\n",
                            " [3039,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['staff', 'rice', 'pork', 'noodles', 'bbq', 'broccoli'],\n",
                            "    \"the bbq pork also was way different this time , so i 'm not sure what happened if , short staff for new years or what , , but will try a few more times before concluding it and removed from faves ... the singapore style rice noodles were the same n that saved the day ... other goto is their chinese broccoli ...\"]]]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "valid_combined_review_with_feature_comb_sent[:2]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[2251,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['size',\n",
                            "     'price',\n",
                            "     'pricing',\n",
                            "     'appetizers',\n",
                            "     'portion',\n",
                            "     'bucks',\n",
                            "     'atmosphere',\n",
                            "     'beer',\n",
                            "     'lunch',\n",
                            "     'food',\n",
                            "     'tea'],\n",
                            "    \"great food , great price , great atmosphere ... portion size was huge comparative to the lunch pricing ... iced tea was definitely not china mist or nestle , or lipton , very good also . 2 appetizers with beer , and 2 full entree 's for about 30 bucks out the door ... simply great ... will definitely be back , , often\"]]],\n",
                            " [3039,\n",
                            "  0,\n",
                            "  4,\n",
                            "  [[['pork', 'rice', 'broccoli', 'staff', 'noodles', 'bbq'],\n",
                            "    \"the bbq pork also was way different this time , so i 'm not sure what happened if , short staff for new years or what , , but will try a few more times before concluding it and removed from faves ... the singapore style rice noodles were the same n that saved the day ... other goto is their chinese broccoli ...\"]]]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 52
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Write Train/Valid/Test with feature Datasets into Files"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "train_combined_with_feature_comb_sent_file = \"./combined_data/{}/split/train.txt\".format(dataset_name)\n",
                "test_combined_with_feature_comb_sent_file = \"./combined_data/{}/split/test.txt\".format(dataset_name)\n",
                "valid_combined_with_feature_comb_sent_file = \"./combined_data/{}/split/valid.txt\".format(dataset_name)\n",
                "train_combined_with_feature_file = \"./combined_data/{}/split/train_split_sent.txt\".format(dataset_name)\n",
                "test_combined_with_feature_file = \"./combined_data/{}/split/test_split_sent.txt\".format(dataset_name)\n",
                "valid_combined_with_feature_file = \"./combined_data/{}/split/valid_split_sent.txt\".format(dataset_name)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "# write train dataset\n",
                "cnt_line = 0\n",
                "for train_combined_with_f_instance in train_combined_review_with_feature:\n",
                "    with open(train_combined_with_feature_file, 'a') as f_wf:\n",
                "        json.dump(train_combined_with_f_instance, f_wf)\n",
                "        cnt_line += 1\n",
                "        if cnt_line == len(train_combined_review_with_feature):\n",
                "            # for the last line, don't add a new empty line\n",
                "            pass\n",
                "        else:\n",
                "            f_wf.write('\\n')\n",
                "print(\"Finish writing train set.\")\n",
                "cnt_line = 0\n",
                "for train_combined_with_f_instance in train_combined_review_with_feature_comb_sent:\n",
                "    with open(train_combined_with_feature_comb_sent_file, 'a') as f_wf:\n",
                "        json.dump(train_combined_with_f_instance, f_wf)\n",
                "        cnt_line += 1\n",
                "        if cnt_line == len(train_combined_review_with_feature_comb_sent):\n",
                "            # for the last line, don't add a new empty line\n",
                "            pass\n",
                "        else:\n",
                "            f_wf.write('\\n')\n",
                "print(\"Finish writing train set (comb review sentences).\")\n",
                "# write test dataset\n",
                "cnt_line = 0\n",
                "for test_combined_with_f_instance in test_combined_review_with_feature:\n",
                "    with open(test_combined_with_feature_file, 'a') as f_wf:\n",
                "        json.dump(test_combined_with_f_instance, f_wf)\n",
                "        cnt_line += 1\n",
                "        if cnt_line == len(test_combined_review_with_feature):\n",
                "            # for the last line, don't add a new empty line\n",
                "            pass\n",
                "        else:\n",
                "            f_wf.write('\\n')\n",
                "print(\"Finish writing test set.\")\n",
                "cnt_line = 0\n",
                "for test_combined_with_f_instance in test_combined_review_with_feature_comb_sent:\n",
                "    with open(test_combined_with_feature_comb_sent_file, 'a') as f_wf:\n",
                "        json.dump(test_combined_with_f_instance, f_wf)\n",
                "        cnt_line += 1\n",
                "        if cnt_line == len(test_combined_review_with_feature_comb_sent):\n",
                "            # for the last line, don't add a new empty line\n",
                "            pass\n",
                "        else:\n",
                "            f_wf.write('\\n')\n",
                "print(\"Finish writing test set (comb review sentences).\")\n",
                "# write valid dataset\n",
                "cnt_line = 0\n",
                "for valid_combined_with_f_instance in valid_combined_review_with_feature:\n",
                "    with open(valid_combined_with_feature_file, 'a') as f_wf:\n",
                "        json.dump(valid_combined_with_f_instance, f_wf)\n",
                "        cnt_line += 1\n",
                "        if cnt_line == len(valid_combined_review_with_feature):\n",
                "            # for the last line, don't add a new empty line\n",
                "            pass\n",
                "        else:\n",
                "            f_wf.write('\\n')\n",
                "print(\"Finish writing valid set.\")\n",
                "cnt_line = 0\n",
                "for valid_combined_with_f_instance in valid_combined_review_with_feature_comb_sent:\n",
                "    with open(valid_combined_with_feature_comb_sent_file, 'a') as f_wf:\n",
                "        json.dump(valid_combined_with_f_instance, f_wf)\n",
                "        cnt_line += 1\n",
                "        if cnt_line == len(valid_combined_review_with_feature_comb_sent):\n",
                "            # for the last line, don't add a new empty line\n",
                "            pass\n",
                "        else:\n",
                "            f_wf.write('\\n')\n",
                "print(\"Finish writing valid set (comb review sentences).\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Finish writing train set.\n",
                        "Finish writing test set.\n",
                        "Finish writing valid set.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Write Aligned Proxy Text"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Valid / Test Proxy"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "cnt_line = 0\n",
                "valid_ui2proxy = dict()\n",
                "valid_ui2sent_proxy_file = '../Dataset/{}/valid/useritem2sentids_withproxy_multilines.json'.format(dataset_name)\n",
                "with open(valid_ui2sent_proxy_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(valid_ui2sent_proxy_file))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = str(line_data['user'])\n",
                "        item_id = str(line_data['item'])\n",
                "        proxy_text = line_data['select_text']\n",
                "        if user_id in valid_ui2proxy:\n",
                "            assert item_id not in valid_ui2proxy[user_id]\n",
                "            valid_ui2proxy[user_id][item_id] = proxy_text\n",
                "        else:\n",
                "            valid_ui2proxy[user_id] = dict()\n",
                "            valid_ui2proxy[user_id][item_id] = proxy_text\n",
                "        cnt_line += 1\n",
                "print(\"Number of reviews in valid set: {}\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/valid/useritem2sentids_withproxy_multilines.json\n",
                        "Number of reviews in valid set: 85748\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "cnt_line = 0\n",
                "test_ui2proxy = dict()\n",
                "test_ui2sent_proxy_file = '../Dataset/{}/test/useritem2sentids_withproxy_multilines.json'.format(dataset_name)\n",
                "with open(test_ui2sent_proxy_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(test_ui2sent_proxy_file))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = str(line_data['user'])\n",
                "        item_id = str(line_data['item'])\n",
                "        proxy_text = line_data['select_text']\n",
                "        if user_id in test_ui2proxy:\n",
                "            assert item_id not in test_ui2proxy[user_id]\n",
                "            test_ui2proxy[user_id][item_id] = proxy_text\n",
                "        else:\n",
                "            test_ui2proxy[user_id] = dict()\n",
                "            test_ui2proxy[user_id][item_id] = proxy_text\n",
                "        cnt_line += 1\n",
                "print(\"Number of reviews in test set: {}\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/test/useritem2sentids_withproxy_multilines.json\n",
                        "Number of reviews in test set: 85748\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "# Load valid data\n",
                "valid_combined_review_file = \"../Dataset/{}/valid_combined.json\".format(dataset_name)\n",
                "valid_combined_proxy_file = \"../Dataset/{}/valid_proxy_combined.json\".format(dataset_name)\n",
                "valid_review_text_file = \"../Dataset/{}/valid_review.txt\".format(dataset_name)\n",
                "valid_proxy_text_file = \"../Dataset/{}/valid_proxy.txt\".format(dataset_name)\n",
                "cnt_line = 0\n",
                "with open(valid_combined_review_file, 'r') as f1:\n",
                "    with open(valid_combined_proxy_file, 'w') as f2:\n",
                "        with open(valid_review_text_file, 'w') as f3:\n",
                "            with open(valid_proxy_text_file, 'w') as f4:\n",
                "                print(\"Load file: {}\".format(valid_combined_review_file))\n",
                "                print(\"Write file: {}\".format(valid_combined_proxy_file))\n",
                "                print(\"Write file: {}\".format(valid_review_text_file))\n",
                "                print(\"Write file: {}\".format(valid_proxy_text_file))\n",
                "                for line in f1:\n",
                "                    cnt_line += 1\n",
                "                    valid_combined_instance = json.loads(line)\n",
                "                    user_id = valid_combined_instance['user']\n",
                "                    item_id = valid_combined_instance['item']\n",
                "                    review_text = valid_combined_instance['review']\n",
                "                    proxy_text = valid_ui2proxy[user_id][item_id]\n",
                "                    # write proxy text into file\n",
                "                    proxy_line_data = {\n",
                "                        'user': user_id,\n",
                "                        'item': item_id,\n",
                "                        'proxy': proxy_text\n",
                "                    }\n",
                "                    json.dump(proxy_line_data, f2)\n",
                "                    f2.write('\\n')\n",
                "                    # write review text\n",
                "                    f3.write(review_text)\n",
                "                    f3.write('\\n')\n",
                "                    # write proxy text\n",
                "                    f4.write(proxy_text)\n",
                "                    f4.write('\\n')\n",
                "print(\"Finished, totally {} lines of valid data.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/valid_combined.json\n",
                        "Write file: ../Dataset/yelp/valid_proxy_combined.json\n",
                        "Write file: ../Dataset/yelp/valid_review.txt\n",
                        "Write file: ../Dataset/yelp/valid_proxy.txt\n",
                        "Finished, totally 34369 lines of valid data.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "# Load valid data\n",
                "test_combined_review_file = \"../Dataset/{}/test_combined.json\".format(dataset_name)\n",
                "test_combined_proxy_file = \"../Dataset/{}/test_proxy_combined.json\".format(dataset_name)\n",
                "test_review_text_file = \"../Dataset/{}/test_review.txt\".format(dataset_name)\n",
                "test_proxy_text_file = \"../Dataset/{}/test_proxy.txt\".format(dataset_name)\n",
                "cnt_line = 0\n",
                "with open(test_combined_review_file, 'r') as f1:\n",
                "    with open(test_combined_proxy_file, 'w') as f2:\n",
                "        with open(test_review_text_file, 'w') as f3:\n",
                "            with open(test_proxy_text_file, 'w') as f4:\n",
                "                print(\"Load file: {}\".format(test_combined_review_file))\n",
                "                print(\"Write file: {}\".format(test_combined_proxy_file))\n",
                "                print(\"Write file: {}\".format(test_review_text_file))\n",
                "                print(\"Write file: {}\".format(test_proxy_text_file))\n",
                "                for line in f1:\n",
                "                    cnt_line += 1\n",
                "                    test_combined_instance = json.loads(line)\n",
                "                    user_id = test_combined_instance['user']\n",
                "                    item_id = test_combined_instance['item']\n",
                "                    review_text = test_combined_instance['review']\n",
                "                    proxy_text = test_ui2proxy[user_id][item_id]\n",
                "                    # write proxy text into file\n",
                "                    proxy_line_data = {\n",
                "                        'user': user_id,\n",
                "                        'item': item_id,\n",
                "                        'proxy': proxy_text\n",
                "                    }\n",
                "                    json.dump(proxy_line_data, f2)\n",
                "                    f2.write('\\n')\n",
                "                    # write review text\n",
                "                    f3.write(review_text)\n",
                "                    f3.write('\\n')\n",
                "                    # write proxy text\n",
                "                    f4.write(proxy_text)\n",
                "                    f4.write('\\n')\n",
                "print(\"Finished, totally {} lines of test data.\".format(cnt_line))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/yelp/test_combined.json\n",
                        "Write file: ../Dataset/yelp/test_proxy_combined.json\n",
                        "Write file: ../Dataset/yelp/test_review.txt\n",
                        "Write file: ../Dataset/yelp/test_proxy.txt\n",
                        "Finished, totally 85748 lines of test data.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}