{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "import re\n",
                "import json\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
                "\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from spacy.lang.en import English\n",
                "nlp = English()\n",
                "# Create a Tokenizer with the default settings for English\n",
                "# including punctuation rules and exceptions\n",
                "tokenizer = nlp.tokenizer\n",
                "import string\n",
                "punct = string.punctuation\n",
                "from sklearn.feature_extraction import _stop_words"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "dataset_name = \"tripadvisor\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Read Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load test dataset\n",
                "test_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'test_review_filtered_clean.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        test_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 10000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/test_review_filtered_clean.json\n",
                        "10000 lines loaded.\n",
                        "Finish loading test dataset, totally 19444 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "dir_path = '../Dataset/{}'.format(dataset_name)\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    print(\"Load file: {}\".format(file_path))\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train_review_filtered.json\n",
                        "100000 lines loaded.\n",
                        "200000 lines loaded.\n",
                        "Finish loading train dataset, totally 205595 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
                "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item user  rating                                             review\n",
                            "0      1111    0       2  when i mentioned this to the front desk they d...\n",
                            "1      1379    0       3  the service was good . our room , was not the ...\n",
                            "2      1391    0       5  we stayed at the signature for four days to ce...\n",
                            "3      1579    0       4  the lake buena vista is a perfect place to sta...\n",
                            "4      1689    0       5  summer ( at the front desk ) was perfect ! she...\n",
                            "...     ...  ...     ...                                                ...\n",
                            "19439     0  999       5  this was a pleasant place , and with our annua...\n",
                            "19440   128  999       5  we enjoyed our stay at the hilton very much ! ...\n",
                            "19441   429  999       5  from the moment we arrived at the front desk u...\n",
                            "19442   816  999       4  wifi gratuit , nous n avons pas essayé le brea...\n",
                            "19443   916  999       4  hot - tub was perfect for relaxing after a lon...\n",
                            "\n",
                            "[19444 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1111</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>when i mentioned this to the front desk they d...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1379</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the service was good . our room , was not the ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1391</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we stayed at the signature for four days to ce...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1579</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the lake buena vista is a perfect place to sta...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1689</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>summer ( at the front desk ) was perfect ! she...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19439</th>\n",
                            "      <td>0</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this was a pleasant place , and with our annua...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19440</th>\n",
                            "      <td>128</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we enjoyed our stay at the hilton very much ! ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19441</th>\n",
                            "      <td>429</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>from the moment we arrived at the front desk u...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19442</th>\n",
                            "      <td>816</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>wifi gratuit , nous n avons pas essayé le brea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19443</th>\n",
                            "      <td>916</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>hot - tub was perfect for relaxing after a lon...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>19444 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "train_groupby_item_user = df_train_data.groupby(['item', 'user'])\n",
                "train_groupby_item_user_dict = dict(tuple(train_groupby_item_user))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "for idx, row in df_test_data.iterrows():\n",
                "    user_id_str = row['user']\n",
                "    item_id_str = row['item']\n",
                "    assert (item_id_str, user_id_str) not in train_groupby_item_user_dict"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Sentence2ID and ID2Sentence Mapping From Training Set"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "sentence2id_filepath = '../Dataset/{}/train/sentence/sentence2id.json'.format(dataset_name)\n",
                "with open(sentence2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(sentence2id_filepath))\n",
                "    trainset_sent_to_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/sentence/sentence2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "id2sentence_filepath = '../Dataset/{}/train/sentence/id2sentence.json'.format(dataset_name)\n",
                "with open(id2sentence_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2sentence_filepath))\n",
                "    trainset_id_to_sent = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/sentence/id2sentence.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "assert len(trainset_sent_to_id) == len(trainset_id_to_sent)\n",
                "print(\"There are {} sentences in the training set.\".format(len(trainset_id_to_sent)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "There are 740398 sentences in the training set.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# Feature words are the same between training and testing\n",
                "# since we can only know the review text from training set\n",
                "feature2id_filepath = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature2id_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature2id_filepath))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "feature_word_list = list(feature_vocab.keys())\n",
                "assert len(feature_word_list) == len(feature_vocab)\n",
                "print('Number of feature words: {}'.format(len(feature_word_list)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of feature words: 503\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "id2feature_filepath = '../Dataset/{}/train/feature/id2feature.json'.format(dataset_name)\n",
                "with open(id2feature_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(id2feature_filepath))\n",
                "    id2feature_train = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/feature/id2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Build Sentence Vocab on TestSet\n",
                "## Check Whether there are reviews with no sentence"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "invalid_data = 0\n",
                "for idx, row in df_test_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    if len(review_sents) == 0:\n",
                "        print(row)\n",
                "        invalid_data += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "print(invalid_data)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "def get_tf_score(text, feature_word_list):\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    return word_count.toarray()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "# sentence vocab\n",
                "sentence_count = dict()\n",
                "sentence_with_no_feature = 0\n",
                "# Loop for each review\n",
                "# TODO: Do we need to filter out sentences with less than 3 tokens same as during training?\n",
                "for idx, row in df_test_data.iterrows():\n",
                "    review_text = row['review']\n",
                "    review_sents = sent_tokenize(review_text)\n",
                "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
                "    tf_sum_sents = np.sum(tf_score, axis=1)\n",
                "    for i in range(len(review_sents)):\n",
                "        if tf_sum_sents[i] != 0.0:\n",
                "            cur_sent = review_sents[i]\n",
                "            sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
                "        else:\n",
                "            sentence_with_no_feature += 1\n",
                "    if (idx+1) % 2000 == 0:\n",
                "        print(\"Processed {} lines\".format(idx+1))\n",
                "print(\"Totally {} tracked sentences\".format(len(sentence_count)))\n",
                "print(\"There are {} sentences with no feature words\".format(sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 2000 lines\n",
                        "Processed 4000 lines\n",
                        "Processed 6000 lines\n",
                        "Processed 8000 lines\n",
                        "Processed 10000 lines\n",
                        "Processed 12000 lines\n",
                        "Processed 14000 lines\n",
                        "Processed 16000 lines\n",
                        "Processed 18000 lines\n",
                        "Totally 75502 tracked sentences\n",
                        "There are 1095 sentences with no feature words\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "len(sentence_count)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "75502"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 22
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "# sort sentence based on counts (the majority should be 1)\n",
                "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "# sentence_vocab_list = list(sentence_count.keys())\n",
                "# Building mappings from sentences to ids and ids to sentences\n",
                "testset_sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
                "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
                "testset_id_to_sent = {str(id): sent for (sent, id) in testset_sent_to_id.items()}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Save Sentence2ID into Json File (Test / Valid Set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "with open('../Dataset/{}/test/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_id_to_sent, f)\n",
                "\n",
                "with open('../Dataset/{}/test/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_sent_to_id, f)\n",
                "\n",
                "with open('../Dataset/{}/valid/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_id_to_sent, f)\n",
                "\n",
                "with open('../Dataset/{}/valid/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
                "    json.dump(testset_sent_to_id, f)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Get Sentence Feature (Test / Valid Set)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "# Some Reviews on Test may not contain any features. Remove these reviews.\n",
                "test_review_has_feature = []\n",
                "review_has_no_feature_num = 0\n",
                "review_has_no_feature_num_list = list()\n",
                "review_has_duplicate_sentences_num = 0\n",
                "review_has_duplicate_sentences_list = list()\n",
                "\n",
                "for idx, test_rvw_j in enumerate(test_review):\n",
                "    rvw_text_j = test_rvw_j[-1]\n",
                "    rvw_sents_j = sent_tokenize(rvw_text_j)\n",
                "    rvw_sent_ids_j = set()\n",
                "    rvw_sent_ids_j_list = list()\n",
                "    review_has_duplicate_sentences = False\n",
                "    for sent in rvw_sents_j:\n",
                "        if sent in testset_sent_to_id:\n",
                "            sent_id = testset_sent_to_id[sent]\n",
                "            if sent_id not in rvw_sent_ids_j:\n",
                "                rvw_sent_ids_j.add(sent_id)\n",
                "                rvw_sent_ids_j_list.append(sent_id)\n",
                "            else:\n",
                "                review_has_duplicate_sentences = True\n",
                "    if review_has_duplicate_sentences:\n",
                "        review_has_duplicate_sentences_num += 1\n",
                "        review_has_duplicate_sentences_list.append(rvw_text_j)\n",
                "    cnt_num_sents_j = len(rvw_sent_ids_j)\n",
                "    assert rvw_sent_ids_j == set(rvw_sent_ids_j_list)\n",
                "    # remove reviews that has no sentences with feature(s)\n",
                "    if cnt_num_sents_j == 0:\n",
                "        print(test_rvw_j)\n",
                "        review_has_no_feature_num += 1\n",
                "        review_has_no_feature_num_list.append(test_rvw_j)\n",
                "    else:\n",
                "        test_review_has_feature.append(test_rvw_j)\n",
                "\n",
                "print(\"Total number of test reviews: {}\".format(len(test_review)))\n",
                "print(\"Number of reviews that have at least 1 features: {}\".format(\n",
                "    len(test_review_has_feature)))\n",
                "print(\"Number of reviews that have no features: {}\".format(\n",
                "    review_has_no_feature_num))\n",
                "print(\"Number of reviews that have duplicate sentences: {}\".format(\n",
                "    review_has_duplicate_sentences_num))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['1370', '1', 5, '100 % empfehlenswert .']\n",
                        "['160', '1186', 4, '$ 9.00 for 9 pages was very pricey .']\n",
                        "['3120', '1482', 1, 'parken 25 $ + tax pro tag .']\n",
                        "['126', '2', 5, 'parkering var veldig enkelt , men det koster 16 $ natten .']\n",
                        "['69', '260', 5, 'side note , wi - fi is an additional $ 10 .']\n",
                        "Total number of test reviews: 19444\n",
                        "Number of reviews that have at least 1 features: 19439\n",
                        "Number of reviews that have no features: 5\n",
                        "Number of reviews that have duplicate sentences: 9\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "review_has_duplicate_sentences_list"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['dinner in the lobby tim was outstanding service and friendly . dinner in the lobby tim was outstanding service and friendly . dinner in the lobby tim was outstanding service and friendly . dinner in the lobby tim was outstanding service and friendly',\n",
                            " 'nice isolated hotel from main stream austin ; ie not near any real restaurants . rooms large , furniture nice , bed very comfy , towels soft & clean . very friendly front desk staff , fast internet . nice isolated hotel from main stream austin ; ie not near any real restaurants . rooms large , furniture nice , bed very comfy , towels soft & clean . very friendly front desk staff , fast internet .',\n",
                            " \"stay someplace else . do n't stay here . do n't stay here . do n't stay here .\",\n",
                            " \"happy to have on - site parking , but @ $ 28 / day is a little pricey . the bathroom was tiny , but at least we did n't see any pubic hairs that did n't belong to us . when we returned from dinner we were ready to retire , but not so our neighbors . the scarf on the bed had some sort of stain on it - perhaps a chocolate smear or blood ? the bathroom was tiny , but at least we did n't see any pubic hairs that did n't belong to us . the armoire that housed the tv had one door ripped off so you could n't close the doors to hide it . i would return if the location insisted upon it , instead of the hotel calling me back ...\",\n",
                            " 'this is my first time staying at a towne suites property . pros : 1 . good location . also it is about three ( 3 ) blocks from angel stadium . other reviews said there is a disneyland bus that picks you up from the hotel . i did not try this out , as we had a parking pass for the amusement park . employees were nice and helpful . 4 . plenty of parking and they do not charge to park . 5 . one in the bedroom and one in the living room . 6 . paul mitchell travel size shampoo , conditioner and hand lotion . 7 . breakfast . some of the reviews like the idea of “ to go breakfast ” and others did not . i thought it was a good idea for the space they had in the lobby . cons : 1 . one of the reviews i read for this hotel stated that there room felt like a dorm . the bed was a full size bed and uncomfortable . the kitchen had an old electric stove . there was only on lamp light in the room , which made the room dark . 2 . location of the room . the room was right next to an elevator . the sheet that was on the bed had a stain size of a pencil end . we took out those sheets and they too had a stain . it was two ( 2 ) blood stains . both were also a size of an end of a pencil . 4 . the bench seat for the disabled in the shower was loose . 5 . bathroom had a pedestal sink and a small counter . hardly any space for toiletries 6 . there is a train that runs behind the property . from where our room was located , we did not hear them . i usually make my reservations online for a marriot property . when i talked to marriot reservations , i told them i wanted a one ( 1 ) bedroom suite . the room had a full size bed for two where i thought i requested a queen size bed . the room also had a smoke odor . i called the front desk to see if they had other rooms . the front desk said they were all booked up and did not have any other rooms . i do not remember requesting this room type since neither i nor my wife are disabled . things to remember when making a reservation : 1 . size of bed . 2 . type of room ( accessible room ? ) 3 . room tip : things to remember when making a reservation : 1 . size of bed . 2 . type of room ( accessible room ...',\n",
                            " 'it was ranked the number one hotel in myrtle beach . our family of five stayed in a two bedroom suite with ocean views . all were very clean . no lifeguards at the pools . - kids loved their bedroom with bunk beds and queen . they rotated beds nightly to try them all ! location . location . location . do not expect to comfortably eat breakfast here . the garbage was overflowing to the floor . my husband drove to starbucks for coffees and breakfast sandwiches . we had a better and much less expensive experience renting a condo unit .',\n",
                            " 'i signed up for the vvip and was given a choice of domestic beer or red wine upon check in . i was in the 6th floor . very quiet . she gave us delicious chocolates and ice for our ice bucket . the room was spacious and very clean . love love love everything about the room , the decor , the bathroom amenities . i really liked the lighting in the room and bathroom . the complimentary breakfast was awesome . i paid 179.99 plus tax w / my aaa discount . so worth the price and will definitely go back ! room tip : choose the highest floor overlooking the courtyard . very quiet .',\n",
                            " \"pro 's : location . location . location . literally less then 8 resorts down is the famous fountain bleu resort . ( which if we had the extra $ 125 to spend we would have opted to stay there , lol . : ) friendly and knowledgable staff and valet @ mbr . directly on the miami beach baby ! $ 30 per day mandatory valet fee .\",\n",
                            " \"first off , i paid for a prepaid rate and it was not prepaid . i figured , no big deal , i 'll pay when i get there . i called ahead and asked what the incidentals hold would be , and they told me $ 50 . again , no big deal . gradually over several days $ 210 was taken out . again , no big deal . my room charges were $ 273 . as for the hotel itself , for the price it is adequate . it 's close to a main road , so you do get a lot of traffic noise . if not for the billing issues , i would stay again because of the price . now you would have to pay me to stay there .\"]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 27
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "df_test_data"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item user  rating                                             review\n",
                            "0      1111    0       2  when i mentioned this to the front desk they d...\n",
                            "1      1379    0       3  the service was good . our room , was not the ...\n",
                            "2      1391    0       5  we stayed at the signature for four days to ce...\n",
                            "3      1579    0       4  the lake buena vista is a perfect place to sta...\n",
                            "4      1689    0       5  summer ( at the front desk ) was perfect ! she...\n",
                            "...     ...  ...     ...                                                ...\n",
                            "19439     0  999       5  this was a pleasant place , and with our annua...\n",
                            "19440   128  999       5  we enjoyed our stay at the hilton very much ! ...\n",
                            "19441   429  999       5  from the moment we arrived at the front desk u...\n",
                            "19442   816  999       4  wifi gratuit , nous n avons pas essayé le brea...\n",
                            "19443   916  999       4  hot - tub was perfect for relaxing after a lon...\n",
                            "\n",
                            "[19444 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1111</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>when i mentioned this to the front desk they d...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1379</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the service was good . our room , was not the ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1391</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we stayed at the signature for four days to ce...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1579</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the lake buena vista is a perfect place to sta...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1689</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>summer ( at the front desk ) was perfect ! she...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19439</th>\n",
                            "      <td>0</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this was a pleasant place , and with our annua...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19440</th>\n",
                            "      <td>128</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we enjoyed our stay at the hilton very much ! ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19441</th>\n",
                            "      <td>429</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>from the moment we arrived at the front desk u...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19442</th>\n",
                            "      <td>816</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>wifi gratuit , nous n avons pas essayé le brea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19443</th>\n",
                            "      <td>916</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>hot - tub was perfect for relaxing after a lon...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>19444 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 28
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "df_test_data_feat = pd.DataFrame(\n",
                "    test_review_has_feature, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "df_test_data_feat"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "       item user  rating                                             review\n",
                            "0      1111    0       2  when i mentioned this to the front desk they d...\n",
                            "1      1379    0       3  the service was good . our room , was not the ...\n",
                            "2      1391    0       5  we stayed at the signature for four days to ce...\n",
                            "3      1579    0       4  the lake buena vista is a perfect place to sta...\n",
                            "4      1689    0       5  summer ( at the front desk ) was perfect ! she...\n",
                            "...     ...  ...     ...                                                ...\n",
                            "19434     0  999       5  this was a pleasant place , and with our annua...\n",
                            "19435   128  999       5  we enjoyed our stay at the hilton very much ! ...\n",
                            "19436   429  999       5  from the moment we arrived at the front desk u...\n",
                            "19437   816  999       4  wifi gratuit , nous n avons pas essayé le brea...\n",
                            "19438   916  999       4  hot - tub was perfect for relaxing after a lon...\n",
                            "\n",
                            "[19439 rows x 4 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>item</th>\n",
                            "      <th>user</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>review</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1111</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>when i mentioned this to the front desk they d...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1379</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>the service was good . our room , was not the ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1391</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we stayed at the signature for four days to ce...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1579</td>\n",
                            "      <td>0</td>\n",
                            "      <td>4</td>\n",
                            "      <td>the lake buena vista is a perfect place to sta...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1689</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5</td>\n",
                            "      <td>summer ( at the front desk ) was perfect ! she...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19434</th>\n",
                            "      <td>0</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>this was a pleasant place , and with our annua...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19435</th>\n",
                            "      <td>128</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>we enjoyed our stay at the hilton very much ! ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19436</th>\n",
                            "      <td>429</td>\n",
                            "      <td>999</td>\n",
                            "      <td>5</td>\n",
                            "      <td>from the moment we arrived at the front desk u...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19437</th>\n",
                            "      <td>816</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>wifi gratuit , nous n avons pas essayé le brea...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19438</th>\n",
                            "      <td>916</td>\n",
                            "      <td>999</td>\n",
                            "      <td>4</td>\n",
                            "      <td>hot - tub was perfect for relaxing after a lon...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>19439 rows × 4 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 32
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "print(\"Number of users on test set: {}\".format(len(df_test_data_feat['user'].unique())))\n",
                "print(\"Number of items on test set: {}\".format(len(df_test_data_feat['item'].unique())))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on test set: 4936\n",
                        "Number of items on test set: 4120\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "# groupby item\n",
                "group_by_item_test = df_test_data_feat.groupby('item')\n",
                "group_by_item_dict = dict(tuple(group_by_item_test))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "assert len(group_by_item_dict) == len(df_test_data_feat['item'].unique())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "peritem_num_sent_testset = dict()\n",
                "peritemreview_num_sent_testset = list()\n",
                "for key, item_df_test in group_by_item_dict.items():\n",
                "    # print(key)\n",
                "    # print(item_df_test)\n",
                "    reviews_list = item_df_test['review']\n",
                "    sentence_count = 0\n",
                "    for review in reviews_list:\n",
                "        review_sent_count = 0\n",
                "        sentences_review = sent_tokenize(review)\n",
                "        for sent in sentences_review:\n",
                "            if sent in testset_sent_to_id:\n",
                "                sentence_count += 1\n",
                "                review_sent_count += 1\n",
                "        peritemreview_num_sent_testset.append(review_sent_count)\n",
                "    peritem_num_sent_testset[key] = sentence_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "assert len(peritemreview_num_sent_testset) == len(df_test_data_feat)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "print(\"Number of review in testset: {}\".format(\n",
                "    len(peritemreview_num_sent_testset)))\n",
                "print(\"Mean number of sentence per review in testset: {}\".format(\n",
                "    np.mean(peritemreview_num_sent_testset)))\n",
                "print(\"Min number of sentence per review in testset {}\".format(\n",
                "    np.min(peritemreview_num_sent_testset)))\n",
                "print(\"Max number of sentence per review in testset {}\".format(\n",
                "    np.max(peritemreview_num_sent_testset)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of review in testset: 19439\n",
                        "Mean number of sentence per review in testset: 4.0509799886825455\n",
                        "Min number of sentence per review in testset 1\n",
                        "Max number of sentence per review in testset 61\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "print(\"Number of items in testset: {}\".format(\n",
                "    len(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Mean number of sentence per item in testset: {}\".format(\n",
                "    np.mean(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Min number of sentence per item in testset: {}\".format(\n",
                "    np.min(list(peritem_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Max number of sentence per item in testset: {}\".format(\n",
                "    np.max(list(peritem_num_sent_testset.values()))\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of items in testset: 4120\n",
                        "Mean number of sentence per item in testset: 19.113349514563108\n",
                        "Min number of sentence per item in testset: 1\n",
                        "Max number of sentence per item in testset: 179\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "# groupby user\n",
                "group_by_user_test = df_test_data_feat.groupby('user')\n",
                "group_by_user_dict = dict(tuple(group_by_user_test))\n",
                "assert len(group_by_user_dict) == len(df_test_data_feat['user'].unique())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "peruser_num_sent_testset = dict()\n",
                "peruserreview_num_sent_testset = list()\n",
                "for key, user_df_test in group_by_user_dict.items():\n",
                "    reviews_list = user_df_test['review']\n",
                "    sentence_count = 0\n",
                "    for review in reviews_list:\n",
                "        review_sent_count = 0\n",
                "        sentences_review = sent_tokenize(review)\n",
                "        for sent in sentences_review:\n",
                "            if sent in testset_sent_to_id:\n",
                "                sentence_count += 1\n",
                "                review_sent_count += 1\n",
                "        peruserreview_num_sent_testset.append(review_sent_count)\n",
                "    peruser_num_sent_testset[key] = sentence_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "assert len(peruserreview_num_sent_testset) == len(df_test_data_feat)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "print(\"Number of review in testset: {}\".format(\n",
                "    len(peruserreview_num_sent_testset)))\n",
                "print(\"Mean number of sentence per review in testset: {}\".format(\n",
                "    np.mean(peruserreview_num_sent_testset)))\n",
                "print(\"Min number of sentence per review in testset {}\".format(\n",
                "    np.min(peruserreview_num_sent_testset)))\n",
                "print(\"Max number of sentence per review in testset {}\".format(\n",
                "    np.max(peruserreview_num_sent_testset)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of review in testset: 19439\n",
                        "Mean number of sentence per review in testset: 4.0509799886825455\n",
                        "Min number of sentence per review in testset 1\n",
                        "Max number of sentence per review in testset 61\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "print(\"Number of users in testset: {}\".format(\n",
                "    len(list(peruser_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Mean number of sentence per user in testset: {}\".format(\n",
                "    np.mean(list(peruser_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Min number of sentence per user in testset: {}\".format(\n",
                "    np.min(list(peruser_num_sent_testset.values()))\n",
                "))\n",
                "print(\"Max number of sentence per user in testset: {}\".format(\n",
                "    np.max(list(peruser_num_sent_testset.values()))\n",
                "))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users in testset: 4936\n",
                        "Mean number of sentence per user in testset: 15.953606158833063\n",
                        "Min number of sentence per user in testset: 1\n",
                        "Max number of sentence per user in testset: 143\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compute Tf-idf"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "# get the list of sentence text on testset\n",
                "testset_sent_text_list = list(testset_sent_to_id.keys())\n",
                "testset_sent_text_list[:20]\n",
                "# NOTE: Based on the examples, \\\n",
                "# should we set the short sentence threshold to be 3 instead of 2 on train?\n",
                "# Currently the threshold is 2 ."
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['great location .',\n",
                            " 'i would stay here again .',\n",
                            " 'the staff was very friendly and helpful .',\n",
                            " 'i would definitely stay here again .',\n",
                            " 'friendly staff .',\n",
                            " 'the room was clean .',\n",
                            " 'the staff was friendly and helpful .',\n",
                            " 'staff was friendly .',\n",
                            " 'would stay here again .',\n",
                            " 'good location .',\n",
                            " 'i would stay there again .',\n",
                            " 'the staff was very friendly .',\n",
                            " 'the bed was very comfortable .',\n",
                            " 'we would definitely stay here again .',\n",
                            " 'i would stay again .',\n",
                            " 'the beds were very comfortable .',\n",
                            " 'the bed was comfortable .',\n",
                            " 'would definitely stay here again .',\n",
                            " 'great place to stay .',\n",
                            " 'very clean .']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 46
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "def get_tfidf_embedding(text, feature_word_list):\n",
                "    \"\"\"\n",
                "    :param: text: list, sent_number * word\n",
                "    :return: \n",
                "        vectorizer: \n",
                "            vocabulary_: word2id\n",
                "            get_feature_names(): id2word\n",
                "        tfidf: array [sent_number, max_word_number]\n",
                "    \"\"\"\n",
                "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
                "    word_count = vectorizer.fit_transform(text)\n",
                "    tfidf_transformer = TfidfTransformer()\n",
                "    tfidf = tfidf_transformer.fit_transform(word_count)\n",
                "    tfidf_weight = tfidf.toarray()\n",
                "    return vectorizer, tfidf_weight"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "cntvector, tfidf_weight = get_tfidf_embedding(testset_sent_text_list, feature_word_list)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "tfidf_weight.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(75502, 503)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 49
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "def check_vocab_is_same(sklearn_vocab, feature_vocab):\n",
                "    if len(sklearn_vocab) == len(feature_vocab):\n",
                "        for key, value in sklearn_vocab.items():\n",
                "            sklearn_vocab_id = value\n",
                "            feature_vocab_id = feature_vocab[key]\n",
                "            if int(feature_vocab_id) == sklearn_vocab_id:\n",
                "                continue\n",
                "            else:\n",
                "                return False\n",
                "    else:\n",
                "        return False\n",
                "    return True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "check_vocab_is_same(cntvector.vocabulary_, feature_vocab)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 51
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "testset_sentence_to_feature = dict()\n",
                "sentence_with_no_feature = 0\n",
                "tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
                "for i in range(len(testset_sent_text_list)):\n",
                "    cur_sent = testset_sent_text_list[i]\n",
                "    # if this sentence is in the sent_to_id vocabulary\n",
                "    assert cur_sent in testset_sent_to_id\n",
                "    # get the sentence_id (str)\n",
                "    cur_sent_id = testset_sent_to_id[cur_sent]\n",
                "    assert int(cur_sent_id) == i\n",
                "    # find all the feature that has non-zero tf-idf weight\n",
                "    feature_dict = dict()\n",
                "    for j in range(len(tfidf_weight[i])):\n",
                "        if tfidf_weight[i][j] != 0.0:\n",
                "            # get the feature\n",
                "            feature_id = str(j)\n",
                "            feature = feature_word_list[j]\n",
                "            feature_tfidf = tfidf_weight[i][j]\n",
                "            feature_dict[feature_id] = feature_tfidf\n",
                "    if len(feature_dict) > 0:\n",
                "        testset_sentence_to_feature[cur_sent_id] = feature_dict\n",
                "    else:\n",
                "        sentence_with_no_feature += 1\n",
                "    if (i+1) % 10000 == 0:\n",
                "        print(\"Processed {} lines\".format(i+1))\n",
                "print(\"Finish. Totally {} lines\".format(i+1))\n",
                "print(\"Totally {} sentences has at least 1 feature and {} sentences don't have feature.\".format(\n",
                "    len(testset_sentence_to_feature), sentence_with_no_feature))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processed 10000 lines\n",
                        "Processed 20000 lines\n",
                        "Processed 30000 lines\n",
                        "Processed 40000 lines\n",
                        "Processed 50000 lines\n",
                        "Processed 60000 lines\n",
                        "Processed 70000 lines\n",
                        "Finish. Totally 75502 lines\n",
                        "Totally 75502 sentences has at least 1 feature and 0 sentences don't have feature.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "# save testset_sentence_to_feature into json file\n",
                "# testset sent_to_id is same as validset, also save this to validset\n",
                "sentence2feature_filepath = '../Dataset/{}/test/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(testset_sentence_to_feature, f)\n",
                "sentence2feature_filepath = '../Dataset/{}/valid/sentence/sentence2feature.json'.format(dataset_name)\n",
                "with open(sentence2feature_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(sentence2feature_filepath))\n",
                "    json.dump(testset_sentence_to_feature, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/tripadvisor/test/sentence/sentence2feature.json\n",
                        "Write file: ../Dataset/tripadvisor/valid/sentence/sentence2feature.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "print(testset_sentence_to_feature['0'])\n",
                "print(testset_id_to_sent['0'])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'9': 1.0}\n",
                        "great location .\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "source": [
                "print(testset_sentence_to_feature['75501'])\n",
                "print(testset_id_to_sent['75501'])\n",
                "for fea_id in testset_sentence_to_feature['75501'].keys():\n",
                "    print(id2feature_train[fea_id])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'101': 1.0}\n",
                        "when we were there they had a deal buy 1 entree and get a second for $ 2 .\n",
                        "deal\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "source": [
                "# get some statistics of sentence_to_feature on testset/validset\n",
                "num_feature_per_sentence = []\n",
                "for key, value in testset_sentence_to_feature.items():\n",
                "    num_feature_per_sentence.append(len(value))\n",
                "    assert len(value) > 0       # every sentence should have at least 1 feature"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "print(\"Mean number of features per sentence: {}\".format(np.mean(num_feature_per_sentence)))\n",
                "print(\"Max number of features per sentence: {}\".format(np.max(num_feature_per_sentence)))\n",
                "print(\"Min number of features per sentence: {}\".format(np.min(num_feature_per_sentence)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mean number of features per sentence: 2.5262377155572038\n",
                        "Max number of features per sentence: 11\n",
                        "Min number of features per sentence: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load User to SentenceID"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "source": [
                "train_user2sentids_filepath = '../Dataset/{}/train/user/user2sentids.json'.format(dataset_name)\n",
                "with open(train_user2sentids_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_user2sentids_filepath))\n",
                "    trainset_user_to_sent_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/user/user2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Item to SentenceID"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "source": [
                "train_item2sentids_filepath = '../Dataset/{}/train/item/item2sentids.json'.format(dataset_name)\n",
                "with open(train_item2sentids_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_item2sentids_filepath))\n",
                "    trainset_item_to_sent_id = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/item/item2sentids.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load User-Item Pairs on TrainSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "source": [
                "train_useritem_pairs_filepath = '../Dataset/{}/train/useritem_pairs.json'.format(dataset_name)\n",
                "with open(train_useritem_pairs_filepath, 'r') as f:\n",
                "    print(\"Load file: {}\".format(train_useritem_pairs_filepath))\n",
                "    trainset_useritem_pairs = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/tripadvisor/train/useritem_pairs.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "source": [
                "# Get the user set and item set on train-set\n",
                "train_user_set = set()\n",
                "train_item_set = set()\n",
                "for key,value in trainset_useritem_pairs.items():\n",
                "    uid = key\n",
                "    assert uid not in train_user_set\n",
                "    train_user_set.add(uid)\n",
                "    for iid in value:\n",
                "        train_item_set.add(iid)\n",
                "print(\"Number of users on the constructed train set: {}\".format(len(train_user_set)))\n",
                "print(\"Number of items on the constructed train set: {}\".format(len(train_item_set)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on the constructed train set: 4950\n",
                        "Number of items on the constructed train set: 4493\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# For Each Data Instance on TestSet"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## GroupBy User"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "source": [
                "group_by_user_test = df_test_data_feat.groupby('user')\n",
                "print(\"Number of users on test-set: {}\".format(len(group_by_user_test)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Number of users on test-set: 4936\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Construct Valid Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "source": [
                "import random\n",
                "sample_sent_num = 500           # this should be among 30, 200 and 500\n",
                "user_item_candidate_sent_ids_validset = dict()\n",
                "cnt_empty_true_sentence = 0\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "review_with_duplicate_sentences = 0\n",
                "review_with_duplicate_sentences_list = list()\n",
                "user_item_candidate_sentence_num = list()\n",
                "user_item_candidate_sentence_num_sampled = list()\n",
                "cnt_being_cut_useritem = 0\n",
                "testset_user_id_set = set()\n",
                "testset_item_id_set = set()\n",
                "# Loop over all users\n",
                "user_cnt = 0\n",
                "for user_df_chunk in list(group_by_user_test):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    user_df = user_df_chunk[1]\n",
                "    if user_id_str not in train_user_set:\n",
                "        print(\"User: {} not in train but in test!\".format(user_id_str))\n",
                "        continue\n",
                "    assert user_id_str not in testset_user_id_set\n",
                "    testset_user_id_set.add(user_id_str)\n",
                "    # get user sentences, these sentences are on TRAIN set\n",
                "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        if item_id_str not in train_item_set:\n",
                "            print(\"Item: {} not in train but in test!\".format(item_id_str))\n",
                "            continue\n",
                "        testset_item_id_set.add(item_id_str)\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sentences, they are on TRAIN set\n",
                "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
                "        # get review_text's sent ids, they are on TEST set\n",
                "        cur_review_sent_ids = set()\n",
                "        cur_review_sent_ids_list = list()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the testset_sent_to_id dict\n",
                "        review_has_duplicate_sentences = False\n",
                "        for sent in review_sents:\n",
                "            if sent in testset_sent_to_id:\n",
                "                cur_sent_id = testset_sent_to_id[sent]\n",
                "                if cur_sent_id not in cur_review_sent_ids:\n",
                "                    # add this sentence into the set of current review\n",
                "                    cur_review_sent_ids.add(cur_sent_id)\n",
                "                    cur_review_sent_ids_list.append(cur_sent_id)\n",
                "                else:\n",
                "                    review_has_duplicate_sentences = True\n",
                "        if review_has_duplicate_sentences:\n",
                "            review_with_duplicate_sentences += 1\n",
                "            review_with_duplicate_sentences_list.append([item_id_str, user_id_str, review_text])\n",
                "        try:\n",
                "            assert cur_review_sent_ids == set(cur_review_sent_ids_list)\n",
                "        except:\n",
                "            print(cur_review_sent_ids, cur_review_sent_ids_list)\n",
                "        # construct the candidate set which is an union of user sentence and item sentence\n",
                "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "        # sample some sentences (they are on TRAIN set)\n",
                "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            cnt_being_cut_useritem += 1\n",
                "        else:\n",
                "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "        # add this into the dict\n",
                "        if len(cur_review_sent_ids) != 0:\n",
                "            # only add the ones that contain at least 1 true label sentence (on valid/test set)\n",
                "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), cur_review_sent_ids_list]\n",
                "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
                "            user_item_candidate_sentence_num_sampled.append(len(sample_useritem_sent_ids))\n",
                "        else:\n",
                "            cnt_empty_true_sentence += 1\n",
                "\n",
                "    # add this item-level dict into the user-level dict\n",
                "    if len(item_candidate_sent_ids) == 0:\n",
                "        print(\"User: {} has no useful items, skip it.\".format(user_id_str))\n",
                "    else:\n",
                "        user_item_candidate_sent_ids_validset[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 1000 == 0:\n",
                "        print(\"{} user processed.\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {0} users, {1} items.'.format(len(testset_user_id_set), len(testset_item_id_set)))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, cnt_empty_true_sentence))\n",
                "print('During constructing, {} user-item pair are being cut due to their length'.format(\n",
                "    cnt_being_cut_useritem))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed.\n",
                        "2000 user processed.\n",
                        "3000 user processed.\n",
                        "4000 user processed.\n",
                        "Finish.\n",
                        "Totally 4936 users, 4120 items.\n",
                        "Totally 19439 reviews. Among them 0 reviews has empty true label sentence\n",
                        "During constructing, 10075 user-item pair are being cutted due to their length\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "source": [
                "print(\"Totally {} user item pairs in the testset\".format(len(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"mean number of candidate sentence: {}\".format(np.median(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"max number of candidate sentence: {}\".format(np.max(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"min number of candidate sentence: {}\".format(np.min(user_item_candidate_sentence_num_sampled)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 19439 user item pairs in the testset\n",
                        "mean number of candidate sentence: 500.0\n",
                        "max number of candidate sentence: 500\n",
                        "min number of candidate sentence: 62\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "source": [
                "print(sorted(user_item_candidate_sentence_num)[-200:])\n",
                "\n",
                "\"\"\" This shows that if we restrict the candidate sentences to have a maximum number of 2000,\n",
                "we will cut-off about 200 reviews. This will be applied on test set to avoid user-item paris\n",
                "with too many candidate sentences.\n",
                "\"\"\""
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1977, 1979, 1981, 1984, 1985, 1986, 1988, 1990, 1992, 1993, 1995, 1997, 1997, 1998, 1998, 1999, 2001, 2003, 2004, 2009, 2009, 2011, 2014, 2016, 2017, 2018, 2018, 2019, 2019, 2022, 2024, 2024, 2024, 2026, 2029, 2031, 2031, 2039, 2041, 2043, 2043, 2044, 2044, 2046, 2050, 2050, 2054, 2057, 2059, 2060, 2060, 2062, 2062, 2063, 2063, 2067, 2067, 2069, 2070, 2073, 2074, 2077, 2081, 2081, 2084, 2085, 2087, 2088, 2089, 2089, 2089, 2091, 2092, 2100, 2101, 2103, 2111, 2114, 2125, 2128, 2136, 2140, 2140, 2141, 2144, 2148, 2148, 2155, 2163, 2166, 2168, 2171, 2179, 2179, 2181, 2188, 2191, 2192, 2194, 2204, 2205, 2206, 2218, 2225, 2226, 2227, 2237, 2238, 2240, 2242, 2243, 2247, 2251, 2259, 2261, 2263, 2266, 2268, 2269, 2269, 2278, 2281, 2294, 2296, 2296, 2298, 2299, 2318, 2329, 2333, 2344, 2344, 2344, 2346, 2361, 2365, 2379, 2398, 2425, 2445, 2449, 2461, 2518, 2562, 2569, 2599, 2666, 2681, 2757, 2789, 2832, 3055, 12819, 12822, 12825, 12826, 12833, 12835, 12835, 12836, 12839, 12840, 12844, 12847, 12848, 12849, 12849, 12849, 12850, 12852, 12853, 12855, 12855, 12855, 12860, 12860, 12861, 12867, 12867, 12881, 12887, 12894, 12916, 12917, 12919, 12920, 12920, 12928, 12950, 12957, 12975, 12981, 13000, 13011, 13023, 13041, 13055, 13067, 13117, 13121]\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "' This shows that if we restrict the candidate sentences to have a maximum number of 2000,\\nwe will cut-off about 1000 reviews. This will be applied on test set to avoid user-item paris\\nwith too many candidate sentences.\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 66
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "source": [
                "len(user_item_candidate_sent_ids_validset)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4936"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 67
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "source": [
                "# save this into json file\n",
                "valid_useritem2sentids_filepath = '../Dataset/{}/valid/useritem2sentids_test.json'.format(dataset_name)\n",
                "with open(valid_useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(valid_useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids_validset, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/tripadvisor/valid/useritem2sentids_test.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "source": [
                "check_user_id = \"0\"\n",
                "check_item_id = \"1689\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 0 \t item: 1689\n",
                        "number of sentence in candidate set: 500\n",
                        "number of sentence in true review set: 3\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "source": [
                "for sentid in user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1]:\n",
                "    print(testset_id_to_sent[sentid])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "summer ( at the front desk ) was perfect !\n",
                        "she alone is worth the stay at this hotel !\n",
                        "the rest of the staff went out of their way to keep me comfortable for the 5 weeks i was there !\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "source": [
                "# Checking How Many User/Item/Review are in the valid set\n",
                "cnt_user = 0\n",
                "cnt_review = 0\n",
                "cnt_item_set = set()\n",
                "for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
                "    user_id_str = str(trainset_user_chunk[0])\n",
                "    user_id = int(trainset_user_chunk[0])\n",
                "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "    for item_chunk in user_item_chunks:\n",
                "        item_id_str = str(item_chunk[0])\n",
                "        item_id = int(item_chunk[0])\n",
                "        # candidate_true_sent_ids = item_chunk[1]\n",
                "        # cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'sent_id': candidate_true_sent_ids}\n",
                "        # write this into the json file\n",
                "        # json.dump(cur_data_dict, f1)\n",
                "        # f1.write(\"\\n\")\n",
                "        # assert user_id_str in train_user_id_set\n",
                "        # assert item_id_str in train_item_id_set\n",
                "        cnt_item_set.add(item_id_str)\n",
                "        cnt_review += 1\n",
                "    cnt_user += 1\n",
                "\n",
                "print(\"Total number of reviews: {}\".format(cnt_review))\n",
                "print(\"Total number of user: {}\".format(cnt_user))\n",
                "print(\"Total number of item: {}\".format(len(cnt_item_set)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Total number of reviews: 19439\n",
                        "Total number of user: 4936\n",
                        "Total number of item: 4120\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "source": [
                "# Write useritem2sentids_test into a line-by-line format\n",
                "valid_useritem2sentids_multiline_filepath = '../Dataset/{}/valid/useritem2sentids_test_multilines.json'.format(dataset_name)\n",
                "with open(valid_useritem2sentids_multiline_filepath, 'w') as f1:\n",
                "    print(\"Write file: {}\".format(valid_useritem2sentids_multiline_filepath))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
                "        user_id_str = str(trainset_user_chunk[0])\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            item_id_str = str(item_chunk[0])\n",
                "            item_id = int(item_chunk[0])\n",
                "            item_set.add(item_id_str)\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            true_revw_sent_ids = item_chunk[1][1]\n",
                "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert len(user_set) == cnt_user\n",
                "assert len(useritem_set) == cnt_review\n",
                "print(\"Total {} users\".format(cnt_user))\n",
                "print(\"Total {} items\".format(len(item_set)))\n",
                "print(\"Totat {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/tripadvisor/valid/useritem2sentids_test_multilines.json\n",
                        "Total 4936 users\n",
                        "Total 4120 items\n",
                        "Totat 19439 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construct Test Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "source": [
                "sample_sent_num = 2000\n",
                "user_item_candidate_sent_ids_testset = dict()\n",
                "cnt_empty_true_sentence = 0\n",
                "user_cnt = 0\n",
                "review_cnt = 0\n",
                "review_with_duplicate_sentences = 0\n",
                "review_with_duplicate_sentences_list = list()\n",
                "user_item_candidate_sentence_num = list()\n",
                "user_item_candidate_sentence_num_sampled = list()\n",
                "cnt_being_cut_useritem = 0\n",
                "testset_user_id_set = set()\n",
                "testset_item_id_set = set()\n",
                "# Loop over all users\n",
                "user_cnt = 0\n",
                "for user_df_chunk in list(group_by_user_test):\n",
                "    user_id = int(user_df_chunk[0])\n",
                "    user_id_str = str(user_df_chunk[0])\n",
                "    if user_id_str not in train_user_set:\n",
                "        print(\"User: {} not in train but in test!\".format(user_id_str))\n",
                "        continue\n",
                "    assert user_id_str not in testset_user_id_set\n",
                "    testset_user_id_set.add(user_id_str)\n",
                "    user_df = user_df_chunk[1]\n",
                "    # get user sentences, these sentences are on TRAIN set\n",
                "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
                "    # item-level dict\n",
                "    item_candidate_sent_ids = dict()\n",
                "    for idx, row in user_df.iterrows():\n",
                "        item_id = int(row['item'])\n",
                "        item_id_str = str(row['item'])\n",
                "        if item_id_str not in train_item_set:\n",
                "            print(\"Item: {} not in train but in test!\".format(item_id_str))\n",
                "            continue\n",
                "        testset_item_id_set.add(item_id_str)\n",
                "        review_text = row['review']\n",
                "        review_cnt += 1\n",
                "        # get item sentences, they are on TRAIN set\n",
                "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
                "        # get review_text's sent ids, they are on TEST set\n",
                "        cur_review_sent_ids = set()\n",
                "        cur_review_sent_ids_list = list()\n",
                "        ## tokenize this review\n",
                "        review_sents = sent_tokenize(review_text)\n",
                "        ## check whether this sentence is in the testset_sent_to_id dict\n",
                "        review_has_duplicate_sentences = False\n",
                "        for sent in review_sents:\n",
                "            if sent in testset_sent_to_id:\n",
                "                cur_sent_id = testset_sent_to_id[sent]\n",
                "                if cur_sent_id not in cur_review_sent_ids:\n",
                "                    # add this sentence into the set of current review\n",
                "                    cur_review_sent_ids.add(cur_sent_id)\n",
                "                    cur_review_sent_ids_list.append(cur_sent_id)\n",
                "                else:\n",
                "                    review_has_duplicate_sentences = True\n",
                "        if review_has_duplicate_sentences:\n",
                "            review_with_duplicate_sentences += 1\n",
                "            review_with_duplicate_sentences_list.append([item_id_str, user_id_str, review_text])\n",
                "        try:\n",
                "            assert cur_review_sent_ids == set(cur_review_sent_ids_list)\n",
                "        except:\n",
                "            print(cur_review_sent_ids, cur_review_sent_ids_list)\n",
                "        # construct the candidate set which is an union of user sentence and item sentence\n",
                "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
                "        # sample some sentences (they are on TRAIN set)\n",
                "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
                "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
                "            cnt_being_cut_useritem += 1\n",
                "        else:\n",
                "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
                "        # add this into the dict\n",
                "        if len(cur_review_sent_ids) != 0:\n",
                "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), cur_review_sent_ids_list]\n",
                "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
                "            user_item_candidate_sentence_num_sampled.append(len(sample_useritem_sent_ids))\n",
                "        else:\n",
                "            cnt_empty_true_sentence += 1\n",
                "\n",
                "    # add this item-level dict into the user-level dict\n",
                "    if len(item_candidate_sent_ids) == 0:\n",
                "        print(\"User: {} has no useful items, skip it.\".format(user_id_str))\n",
                "    else:\n",
                "        user_item_candidate_sent_ids_testset[user_id_str] = item_candidate_sent_ids\n",
                "    user_cnt += 1\n",
                "    if user_cnt % 1000 == 0:\n",
                "        print(\"{} user processed.\".format(user_cnt))\n",
                "\n",
                "print('Finish.')\n",
                "print('Totally {0} users, {1} items.'.format(len(testset_user_id_set), len(testset_item_id_set)))\n",
                "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
                "    review_cnt, cnt_empty_true_sentence))\n",
                "print(\"{} reviews have duplicate sentences.\".format(review_with_duplicate_sentences))\n",
                "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1000 user processed.\n",
                        "2000 user processed.\n",
                        "3000 user processed.\n",
                        "4000 user processed.\n",
                        "Finish.\n",
                        "Totally 4936 users, 4120 items.\n",
                        "Totally 19439 reviews. Among them 0 reviews has empty true label sentence\n",
                        "9 reviews have duplicate sentences.\n",
                        "During constructing, 184 user-item pair are being cutted due to their length\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "source": [
                "print(\"Totally {} user item pairs in the testset\".format(\n",
                "    len(user_item_candidate_sentence_num)))\n",
                "print(\"mean number of candidate sentence: {}\".format(\n",
                "    np.mean(user_item_candidate_sentence_num)))\n",
                "print(\"max number of candidate sentence: {}\".format(\n",
                "    np.max(user_item_candidate_sentence_num)))\n",
                "print(\"min number of candidate sentence: {}\".format(\n",
                "    np.min(user_item_candidate_sentence_num)))\n",
                "print(\"mean number of sampled candidate sentence: {}\".format(\n",
                "    np.mean(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"max number of sampled candidate sentence: {}\".format(\n",
                "    np.max(user_item_candidate_sentence_num_sampled)))\n",
                "print(\"min number of sampled candidate sentence: {}\".format(\n",
                "    np.min(user_item_candidate_sentence_num_sampled)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 19439 user item pairs in the testset\n",
                        "mean number of candidate sentence: 638.9731982097844\n",
                        "max number of candidate sentence: 13121\n",
                        "min number of candidate sentence: 62\n",
                        "mean number of sampled candidate sentence: 610.6922166778127\n",
                        "max number of sampled candidate sentence: 2000\n",
                        "min number of sampled candidate sentence: 62\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "source": [
                "print(sorted(user_item_candidate_sentence_num)[-40:])\n",
                "print(sorted(user_item_candidate_sentence_num_sampled)[-40:])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[12839, 12840, 12844, 12847, 12848, 12849, 12849, 12849, 12850, 12852, 12853, 12855, 12855, 12855, 12860, 12860, 12861, 12867, 12867, 12881, 12887, 12894, 12916, 12917, 12919, 12920, 12920, 12928, 12950, 12957, 12975, 12981, 13000, 13011, 13023, 13041, 13055, 13067, 13117, 13121]\n",
                        "[2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "source": [
                "len(user_item_candidate_sent_ids_testset)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4936"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 77
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "source": [
                "# save this into json file\n",
                "test_useritem2sentids_filepath = '../Dataset/{}/test/useritem2sentids_test.json'.format(dataset_name)\n",
                "with open(test_useritem2sentids_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(test_useritem2sentids_filepath))\n",
                "    json.dump(user_item_candidate_sent_ids_testset, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/tripadvisor/test/useritem2sentids_test.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "source": [
                "review_test_cnt = 0\n",
                "for user_chunk in user_item_candidate_sent_ids_testset.items():\n",
                "    user_id = user_chunk[0]\n",
                "    user_dict = user_chunk[1]\n",
                "    for user_item_chunk in user_dict.items():\n",
                "        item_id = user_item_chunk[0]\n",
                "        candidate_sents = user_item_chunk[0]\n",
                "        true_label_sents = user_item_chunk[1]\n",
                "        review_test_cnt += 1\n",
                "print(review_test_cnt)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "19439\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "check_user_id = \"0\"\n",
                "check_item_id = \"1689\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 0 \t item: 1689\n",
                        "number of sentence in candidate set: 2000\n",
                        "number of sentence in true review set: 3\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "source": [
                "# Write useritem2sentids_test into a line-by-line format\n",
                "test_useritem2sentids_multiline_filepath = '../Dataset/{}/test/useritem2sentids_test_multilines.json'.format(dataset_name)\n",
                "with open(test_useritem2sentids_multiline_filepath, 'w') as f1:\n",
                "    print(\"Write file: {}\".format(test_useritem2sentids_multiline_filepath))\n",
                "    cnt_user = 0\n",
                "    cnt_review = 0\n",
                "    user_set = set()\n",
                "    item_set = set()\n",
                "    useritem_set = set()\n",
                "    for trainset_user_chunk in list(user_item_candidate_sent_ids_testset.items()):\n",
                "        user_id_str = str(trainset_user_chunk[0])\n",
                "        user_id = int(trainset_user_chunk[0])\n",
                "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
                "        for item_chunk in user_item_chunks:\n",
                "            item_id_str = str(item_chunk[0])\n",
                "            item_id = int(item_chunk[0])\n",
                "            item_set.add(item_id_str)\n",
                "            candidate_sent_ids = item_chunk[1][0]\n",
                "            true_revw_sent_ids = item_chunk[1][1]\n",
                "            cur_data_dict = {\n",
                "                'user_id': user_id,\n",
                "                'item_id': item_id,\n",
                "                'candidate': candidate_sent_ids,\n",
                "                'review': true_revw_sent_ids\n",
                "            }\n",
                "            # write this into the json file\n",
                "            json.dump(cur_data_dict, f1)\n",
                "            f1.write(\"\\n\")\n",
                "            cnt_review += 1\n",
                "            useritem_set.add((user_id_str, item_id_str))\n",
                "        cnt_user += 1\n",
                "        user_set.add(user_id_str)\n",
                "\n",
                "assert len(user_set) == cnt_user\n",
                "assert len(useritem_set) == cnt_review\n",
                "print(\"Total {} users\".format(cnt_user))\n",
                "print(\"Total {} items\".format(len(item_set)))\n",
                "print(\"Totat {} reviews\".format(cnt_review))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/tripadvisor/test/useritem2sentids_test_multilines.json\n",
                        "Total 4936 users\n",
                        "Total 4120 items\n",
                        "Totat 19439 reviews\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "source": [
                "check_user_id = \"999\"\n",
                "check_item_id = \"128\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"[VALID] number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"[VALID] number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))\n",
                "print(\"[TEST]  number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"[TEST]  number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 999 \t item: 128\n",
                        "[VALID] number of sentence in candidate set: 500\n",
                        "[VALID] number of sentence in true review set: 3\n",
                        "[TEST]  number of sentence in candidate set: 845\n",
                        "[TEST]  number of sentence in true review set: 3\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "source": [
                "check_user_id = \"0\"\n",
                "check_item_id = \"1111\"\n",
                "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
                "print(\"[VALID] number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
                "print(\"[VALID] number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))\n",
                "print(\"[TEST]  number of sentence in candidate set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
                "print(\"[TEST]  number of sentence in true review set: {}\".format(\n",
                "    len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "user: 0 \t item: 1111\n",
                        "[VALID] number of sentence in candidate set: 500\n",
                        "[VALID] number of sentence in true review set: 1\n",
                        "[TEST]  number of sentence in candidate set: 2000\n",
                        "[TEST]  number of sentence in true review set: 1\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}