{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!which python"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/u/pw7nc/anaconda3/bin/python\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import csv\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "from nltk.corpus import brown\n",
                "from mittens import GloVe, Mittens\n",
                "from sklearn.feature_extraction import _stop_words\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "import pickle\n",
                "import re\n",
                "import json\n",
                "import os\n",
                "import pandas as pd\n",
                "import spacy\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "def glove2dict(glove_filename):\n",
                "    with open(glove_filename, encoding='utf-8') as f:\n",
                "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
                "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
                "                for line in reader}\n",
                "    return embed\n",
                "\n",
                "def get_rareoov(xdict, val):\n",
                "    return [k for (k,v) in Counter(xdict).items() if v<=val]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Load the orignial dataset to train glove. Larger dataset can give better results for word embedding\n",
                "# NOTE: For current fast development use the whole_filtered dataset.\n",
                "# Later should change this to whole_cleaned dataset.\n",
                "\n",
                "dir_path = '../Dataset/wine/'\n",
                "# Load train dataset\n",
                "train_review = []\n",
                "cnt = 0\n",
                "file_path = os.path.join(dir_path, 'whole_filtered.json')\n",
                "with open(file_path) as f:\n",
                "    for line in f:\n",
                "        line_data = json.loads(line)\n",
                "        user_id = line_data['user']\n",
                "        item_id = line_data['item']\n",
                "        rating = line_data['rating']\n",
                "        review = line_data['review']\n",
                "        train_review.append([item_id, user_id, rating, review])\n",
                "        cnt += 1\n",
                "        if cnt % 100000 == 0:\n",
                "            print('{} lines loaded.'.format(cnt))\n",
                "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "100000 lines loaded.\n",
                        "200000 lines loaded.\n",
                        "300000 lines loaded.\n",
                        "Finish loading train dataset, totally 307746 lines.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "train_review_doc = list(df_train_data['review'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "len(train_review_doc)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "307746"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "with open('./embeddings/wine/train-review.txt', 'w') as f:\n",
                "    cnt = 0\n",
                "    for review in train_review_doc:\n",
                "        # write this into file\n",
                "        f.write(review.strip())\n",
                "        f.write('\\n')\n",
                "        cnt += 1\n",
                "    print('Write {} lines of review into file'.format(cnt))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write 307746 lines of review into file\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "from collections import Counter\n",
                "import simplejson\n",
                "from nltk.tokenize import word_tokenize\n",
                "import string\n",
                "import re\n",
                "import nltk\n",
                "nltk.download('punkt')\n",
                "\n",
                "def tokenize(infname=\"embeddings/wine/train-review.txt\"):\n",
                "    outfname = open(infname + \".tokenized\", \"w\")\n",
                "    sents = open(infname).read().split(\"\\n\")\n",
                "    for sent in sents:\n",
                "        # convert characters into lower cases\n",
                "        text = sent.lower()\n",
                "        # tokenize the raw text using nltk.tokenize\n",
                "        text = word_tokenize(text)\n",
                "        # remove all punctuation\n",
                "        text = list(filter(lambda token: token not in string.punctuation, text))\n",
                "        # # replace all numbers with a special token <num>\n",
                "        # text = [re.sub('[0-9]+','<num>', token) for token in text]\n",
                "        # write this processed text into the output file\n",
                "        simplejson.dump(text, outfname)\n",
                "        outfname.write(\"\\n\")\n",
                "    outfname.close()\n",
                "\n",
                "\n",
                "def token_filter(infname=\"embeddings/wine/train-review.txt.tokenized\", thresh=5):\n",
                "    outfname = open(infname.replace(\".tokenized\", \".filtered\"), 'w')\n",
                "    vocab = []  \n",
                "    vocab_cnt = Counter()\n",
                "    tokenized_sents = []\n",
                "    with open(infname) as f_in:\n",
                "        for line in f_in:\n",
                "            tokenized_text = simplejson.loads(line)\n",
                "            vocab_cnt.update(tokenized_text)\n",
                "            tokenized_sents.append(tokenized_text)\n",
                "    for tokenized_sent in tokenized_sents:\n",
                "        # filter word that has less than 5 TF over the whole dataset\n",
                "        filtered_text = []\n",
                "        for token in tokenized_sent:\n",
                "            if vocab_cnt[token] < thresh:\n",
                "                pass\n",
                "            else:\n",
                "                filtered_text.append(token)\n",
                "        # write this filtered data into file\n",
                "        # ignore the last line of the initial imdb-small dataset since it's an empty line\n",
                "        if len(tokenized_sent) > 0:\n",
                "            new_sentence = \" \".join(filtered_text).strip()+'\\n'\n",
                "            outfname.write(new_sentence)\n",
                "    # bulld the vocabulary\n",
                "    ## filter out token with low TF\n",
                "    vocab_cnt_filter = {token: cnt for token, cnt in vocab_cnt.items() if cnt >= thresh}\n",
                "    vocab_cnt_filter = Counter(vocab_cnt_filter)\n",
                "    ## convert this vocabulary counter to a list\n",
                "    for token,cnt in vocab_cnt_filter.most_common():\n",
                "        vocab.append(token)\n",
                "    outfname.close()\n",
                "    return vocab"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package punkt to /u/pw7nc/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "tokenize()\n",
                "vocab = token_filter()\n",
                "print(\"The vocab size = {}\".format(len(vocab)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "The vocab size = 27729\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load Feature Words"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "dataset_name = 'wine'\n",
                "feature_2_id_file = '../Dataset/{}/train/feature/feature2id.json'.format(dataset_name)\n",
                "with open(feature_2_id_file, 'r') as f:\n",
                "    print(\"Load file: {}\".format(feature_2_id_file))\n",
                "    feature_vocab = json.load(f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Load file: ../Dataset/wine/train/feature/feature2id.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "for fea_word in list(feature_vocab.keys()):\n",
                "    if fea_word in vocab:\n",
                "        pass\n",
                "    else:\n",
                "        print(fea_word)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "gin\n",
                        "greengage\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "print(\"Length of feature vocab: {}\".format(len(feature_vocab)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Length of feature vocab: 215\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "total_vocab = set(vocab) | set(list(feature_vocab.keys()))\n",
                "print(\"The total vocab size = {}\".format(len(total_vocab)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "The total vocab size = 27731\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "total_vocab_list = list(total_vocab)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "cv = CountVectorizer(ngram_range=(1,1), vocabulary=total_vocab_list)\n",
                "X = cv.fit_transform(train_review_doc)\n",
                "Xc = (X.T * X)\n",
                "Xc.setdiag(0)\n",
                "coocc_ar = Xc.toarray()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/u/pw7nc/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py:708: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
                        "  self[i, j] = values\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "coocc_ar.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(27731, 27731)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 17
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "X.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(307746, 27731)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 18
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "len(cv.vocabulary_)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "27731"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 19
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "train_vocab = cv.get_feature_names()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "print(train_vocab[:20])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['warre', 'seduce', 'carnival', '.dried', 'candidates', 'pruning', 'youngish', 'reopened', 'arabic', 'soak', 'calling', 'hermitage', 'lie', 'incroyablement', 'comtes', 'conditioned', 'wore', 'union', 'declassified', 'intercontinental']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "cv.vocabulary_[\"aroma\"]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "10639"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 22
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "train_vocab == total_vocab_list"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 27
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "from mittens import GloVe\n",
                "import numpy as np\n",
                "\n",
                "# mittens_model = Mittens(n=256, max_iter=1000)\n",
                "\n",
                "# new_embeddings = mittens_model.fit(\n",
                "#     coocc_ar,\n",
                "#     vocab=train_vocab)\n",
                "\n",
                "# NOTE: For fast development, set the max_iter to be 100.\n",
                "# Should set it to be 1000 for more accurate word embedding.\n",
                "glove_model = GloVe(n=256, max_iter=100)\n",
                "embeddings = glove_model.fit(coocc_ar)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Iteration 100: error 330686.7032"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "embeddings.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(27731, 256)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 29
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "len(cv.vocabulary_)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "27731"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 30
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "len(cv.get_feature_names())"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "27731"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 31
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "vocabulary_words = cv.get_feature_names()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "print(list(feature_vocab.keys())[:20])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['wine', 'fruit', 'palate', 'dark', 'acidity', 'cherry', 'spice', 'smooth', 'taste', 'vanilla', 'pinot', 'alcohol', 'tart', 'citrus', 'blackberry', 'plum', 'acid', 'ruby', 'lemon', 'apple']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "print(vocabulary_words[:20])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['warre', 'seduce', 'carnival', '.dried', 'candidates', 'pruning', 'youngish', 'reopened', 'arabic', 'soak', 'calling', 'hermitage', 'lie', 'incroyablement', 'comtes', 'conditioned', 'wore', 'union', 'declassified', 'intercontinental']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "# construct an embedding dict which has a format of {'word':'wine', 'embedding': array([...])}\n",
                "word_embedding_dict = dict()\n",
                "for i in range(len(embeddings)):\n",
                "    current_word = vocabulary_words[i]\n",
                "    current_embedding = embeddings[i]\n",
                "    word_embedding_dict[current_word] = current_embedding\n",
                "print(\"Totally {} words begin added in the word embedding dict\".format(len(word_embedding_dict)))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 27731 words begin added in the word embedding dict\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "# Save the Embedding dict into pickle file\n",
                "import pickle\n",
                "with open('./embeddings/wine/glove.pickle', 'wb') as handle:\n",
                "    pickle.dump(word_embedding_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "# extract the embedding vector for each feature word\n",
                "feature_embedding = dict()\n",
                "emb_cnt = 0\n",
                "for feature_word in list(feature_vocab.keys()):\n",
                "    feature_word_id = cv.vocabulary_[feature_word]\n",
                "    feature_vocab_id = feature_vocab[feature_word]\n",
                "    feature_word_emb = embeddings[feature_word_id]\n",
                "    feature_embedding[feature_vocab_id] = feature_word_emb\n",
                "    emb_cnt += 1\n",
                "print(\"Totally {} feature words\".format(emb_cnt))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Totally 215 feature words\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "# extract the embedding vector for each feature word\n",
                "feature_embed_dict = dict()\n",
                "for emb_chunk in list(feature_embedding.items()):\n",
                "    feature_id = emb_chunk[0]   # this is a str\n",
                "    feature_emb_np = emb_chunk[1].tolist()\n",
                "    feature_embed_dict[feature_id] = feature_emb_np"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "len(feature_embed_dict)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "215"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 39
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "feature_emb_filepath = '../Dataset/wine/train/feature/featureid2embedding.json'\n",
                "with open(feature_emb_filepath, 'w') as f:\n",
                "    print(\"Write file: {}\".format(feature_emb_filepath))\n",
                "    json.dump(feature_embed_dict, f)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Write file: ../Dataset/wine/train/feature/featureid2embedding.json\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73d0647c863cb9ce92fb50b3911519dc6558e38bcfd5798aa86981c2dac43fdf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}